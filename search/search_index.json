{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Decentralized Reliability Engineering (DRE)","text":"<p>Welcome to the documentation for DFINITY's Decentralized Reliability Engineering (DRE) tools and processes. This documentation covers everything you need to know about managing, monitoring, and maintaining the Internet Computer network.</p>"},{"location":"index.html#what-is-dre","title":"What is DRE?","text":"<p>DRE (Decentralized Reliability Engineering) is a comprehensive suite of tools and practices designed to manage the Internet Computer network in a decentralized manner. Our tools help maintain network reliability, manage updates, and ensure the smooth operation of the Internet Computer Protocol (ICP).</p>"},{"location":"index.html#key-components","title":"Key Components","text":""},{"location":"index.html#dre-cli-tool","title":"DRE CLI Tool","text":"<p>A command-line interface tool that provides essential functionality for:</p> <ul> <li>Managing subnet configurations</li> <li>Handling node operations</li> <li>Executing network updates</li> <li>Monitoring system health</li> </ul>"},{"location":"index.html#internal-dre-dashboard","title":"Internal DRE Dashboard","text":"<p>A web-based interface offering:</p> <ul> <li>Real-time network monitoring</li> <li>Deployment management</li> <li>System metrics visualization</li> <li>Operational status tracking</li> </ul>"},{"location":"index.html#service-discovery","title":"Service Discovery","text":"<p>Maintains an up-to-date registry of:</p> <ul> <li>IC network targets</li> <li>Log aggregation endpoints</li> <li>Metrics collection points</li> </ul>"},{"location":"index.html#log-management","title":"Log Management","text":"<p>Comprehensive logging solutions for:</p> <ul> <li>Host nodes</li> <li>Guest nodes</li> <li>Boundary nodes</li> <li>Canister operations</li> </ul>"},{"location":"index.html#getting-started","title":"Getting Started","text":"<ol> <li>Installation Guide - Set up the DRE tools and environment</li> <li>Contributing Guide - Learn how to contribute to the project</li> <li>Documentation Guide - Help improve our documentation</li> </ol>"},{"location":"index.html#core-features","title":"Core Features","text":"<ul> <li>Release Management: Coordinate and execute network updates</li> <li>Subnet Management: Configure and maintain network subnets</li> <li>Monitoring: Track network health and performance</li> <li>Decentralization: Support the Internet Computer's decentralized architecture</li> </ul>"},{"location":"index.html#quick-links","title":"Quick Links","text":"<ul> <li>Create a Neuron</li> <li>Set Up Neuron Following</li> <li>Submit NNS Proposals</li> <li>Submit Firewall Change Proposals</li> <li>Run Qualification Tests</li> <li>Manage Releases</li> </ul>"},{"location":"index.html#resources","title":"Resources","text":"<ul> <li>GitHub Repository</li> <li>Latest Releases</li> <li>Issue Tracker</li> </ul>"},{"location":"index.html#license","title":"License","text":"<p>This project is licensed under the Apache 2.0 License.</p>"},{"location":"be-dr-dre.html","title":"Who really is Dr. DRE (<code>@dr-dre</code>)?","text":"<p>To enhance our team's effectiveness, we have implemented a weekly rotation system. Each week, a designated team member takes responsibility for managing routine and unexpected operations. This document provides an overview of the on-call responsibilities and includes important links to resources that will support you in this role.</p>"},{"location":"be-dr-dre.html#where-to-find-the-rotation","title":"Where to find the rotation","text":"<p>The rotation schedules can be found on our Jira Team Operations page (requires DFINITY Jira access which can be obtained through Okta). There are two schedules, both following the same round-robin system:</p> <ol> <li>DRE Alerts: Handles automatic paging related to our infrastructure.</li> <li>DRE Ops Rotation: Determines who will act as <code>@dr-dre</code> (our Slack handle).</li> </ol> Why are there two schedules? <p>The two-schedule system was designed to separate responsibilities and ensure balance.</p> <ul> <li>DRE Alerts focuses on managing infrastructure alerts and operates only during working hours, as we don\u2019t adhere to any strict SLA/SLO requirements.</li> <li>DRE Ops Rotation handles Slack pings and general team operations.</li> </ul> I am not getting paged for alerts? <p>We use the Jira cloud app for on-call and rotations.</p> <p>To set it up follow the document on Notion.</p>"},{"location":"be-dr-dre.html#regular-activities","title":"Regular activities","text":"<p>As Dr. DRE, your role for the week involves taking on several responsibilities. These include, but are not limited to:</p>"},{"location":"be-dr-dre.html#1-follow-through-the-ic-os-release-process","title":"1. Follow through the IC OS release process","text":"<p>The release process is documented in detail here.  In short:</p> <ul> <li>Follow the schedule presented on the rollout dashboard.  If problems arise, diagnose using the low-level statuses from Airflow (the dashboard also links directly to the problem task in Airflow).</li> <li>Cut a new GuestOS &amp; HostOS release on Thursday, and create any additional feature builds as per the spreadsheet as well as security hotfixes.</li> <li>Ensure team engineers review the release notes through Friday.</li> <li>Ensure the release controller submits GuestOS &amp; HostOS version elect proposals on Friday -- not earlier, to allow sufficient time for community and DFINITY voters to review and vote without rush.</li> <li>In-depth explanation of the release process can be found on Notion.</li> </ul> Regular week <p>Usually most of this work boils down to running <pre><code>dre vote\n</code></pre></p>"},{"location":"be-dr-dre.html#2-review-alerts-for-our-clusters","title":"2. Review alerts for our clusters","text":"<ul> <li>All alerts that our clusters send are aggregated in our Jira ops board.</li> <li>Heartbeats are present here.</li> </ul> What should I do if there are alerts? <ul> <li>It's not expected that every alert can be resolved immediately or by a single team member.</li> <li>The key objective is to maintain the stability of our clusters.</li> <li>Evaluate the alert based on its severity and the affected cluster to determine if further action is required.</li> <li>Escalate or address issues as needed to ensure operations continue smoothly.</li> </ul>"},{"location":"be-dr-dre.html#3-handle-all-notifications-and-answer-all-questions-asked-in-the-teams-slack-channels","title":"3. Handle all notifications and answer all questions asked in the team's slack channels","text":"<ul> <li><code>#eng-dre</code>: General channel for activities</li> <li><code>#eng-release</code>: Questions related to release process</li> <li><code>#eng-release-bots</code>: Automations send important notifications to this channel, which you must handle</li> <li><code>#eng-observability</code>: Questions related to our observability</li> </ul> But I don't know the answers to all questions <ul> <li>It\u2019s perfectly fine not to have all the answers.</li> <li>Take the initiative to investigate the issue and see how you can assist.</li> <li>If you\u2019re unable to resolve the question, redirect it to the appropriate team member.</li> <li>The primary goal is to support the organization and relieve pressure on the rest of the team during your on-call week.</li> </ul>"},{"location":"be-dr-dre.html#4-submit-requested-proposals","title":"4. Submit requested proposals","text":"<p>All requested proposals must:</p> <ol> <li>Be registered as a ticket under the DRE Ops Rotation queue</li> <li>Include clear requirements and expected outcomes</li> <li>Be followed through in a timely manner based on priority</li> </ol> <p>Typical types of requested proposals are:</p> <ul> <li>Help in on-boarding or off-boarding of datacenters and node providers</li> <li>Firewall rule modifications</li> <li>Node rewards adjustment proposals (see Handoff operations below)</li> <li>Any other requested proposals</li> </ul> Tooling <p>For all regular ops we have sufficient tooling implemented in our <code>dre</code> tool. For all new proposals and specific scenarios it is your responsibility to add them to the tooling as the new use cases come.</p>"},{"location":"be-dr-dre.html#5-submit-proposals-conventionally-submitted-once-a-week","title":"5. Submit proposals conventionally submitted once a week","text":"<ul> <li>Replace dead nodes</li> <li>Mainnet topology proposals, such as <code>dre network --heal --optimize --ensure-operator-nodes-unassigned --ensure-operator-nodes-assigned --remove-cordoned-nodes</code> or a subset of these operations. The operations are still not polished enough to be run automatically.</li> <li>Provider reward adjustment proposals, if any are needed that week. Please ask in <code>#eng-dre</code> if you don't know if any are needed.</li> </ul> <p>Please register proposals as tickets under the DRE Ops Rotation queue, so adoption and progress can be tracked, and context can be observed by your teammates.</p>"},{"location":"be-dr-dre.html#6-monitor-status-and-health-of-ci","title":"6. Monitor status and health of CI","text":"<ul> <li> <p>Weekly dependency upgrade jobs:</p> </li> <li> <p>A GitHub Action runs weekly to automatically upgrade dependencies.</p> </li> <li>Dependabot also issues PRs regularly.</li> <li>While some weeks result in straightforward updates, others may require manual intervention due to API changes or other breaking updates.</li> <li>Review and address any issues with the generated pull request<ul> <li>Find the automation PRs here.</li> <li>Find the Dependabot PRs here.</li> </ul> </li> <li>Ensure the fixes are implemented and attempt to merge the PR into the repository.</li> <li>Maintaining compatibility between the IC repo and our repo reduces friction and ensures our tooling operates smoothly.</li> </ul>"},{"location":"be-dr-dre.html#7-drive-progress-on-the-dre-ops-rotation-task-queue","title":"7. Drive progress on the DRE Ops Rotation task queue","text":"<p>Our DRE Ops rotation dashboard lets you view the queue.  The queue exists to keep track of work falling under the Dr. DRE umbrella that may span multiple days or weeks.  It contains a list of child tickets that you need to work on.</p> <p>Tend to the queue at least once a day.  Read and heed the guidelines in the umbrella epic.  Here is a brief summary (which is not a substitute for reading the guidelines):</p> <ul> <li>Record (as tickets of type task) multi-day work under the umbrella of the DRE Ops Rotation, with the task queue ticket as the new ticket's epic.</li> <li>Drive progress on tasks that are not blocked.</li> <li>Mark blocked tasks as blocked.</li> <li>Record completion of tasks.</li> <li>Provide enough context there for your teammates to pick up ongoing work the week after.</li> <li>Move tickets that change scope out of the queue and into its own epic or project.</li> </ul>"},{"location":"be-dr-dre.html#8-handoff-operations","title":"8. Handoff operations","text":"<ul> <li>If there are any pending tasks or unresolved operations, it is your responsibility to inform the next on-call team member.</li> <li>Provide clear details on what needs to be addressed and any context that might help them pick up where you left off.</li> <li>Pass on information about node rewards adjustments requested to the next on-call team member.</li> </ul> <p>The DRE Ops Rotation dashboard is an invaluable aid in getting yourself in context as well as providing context to your teammates.</p>"},{"location":"contributing.html","title":"Contributing to DRE","text":"<p>Thank you for your interest in contributing to the Decentralized Reliability Engineering (DRE) project. This guide will help you set up your development environment and understand our contribution process.</p>"},{"location":"contributing.html#table-of-contents","title":"\ud83d\udcda Table of Contents","text":"<ol> <li>Development Environment Setup</li> <li>Project Structure</li> <li>Code Style Guidelines</li> <li>Development Workflow</li> <li>Pull Request Process</li> <li>Running Tests</li> <li>Common Issues</li> <li>Getting Help</li> </ol>"},{"location":"contributing.html#development-environment-setup","title":"\ud83d\udee0 Development Environment Setup","text":""},{"location":"contributing.html#1-python-environment-rye","title":"1. Python Environment (Rye)","text":"<p>Rye is our preferred Python environment manager. It provides a unified experience for managing Python installations, dependencies, and virtual environments.</p>"},{"location":"contributing.html#installation","title":"Installation","text":"<pre><code>curl -sSf https://rye.astral.sh/get | bash\nsource \"$HOME/.rye/env\"  # Add to your shell's RC file\n</code></pre>"},{"location":"contributing.html#project-setup","title":"Project Setup","text":"<pre><code>rye sync  # Install all dependencies\n</code></pre>"},{"location":"contributing.html#common-rye-commands","title":"Common Rye Commands","text":"<pre><code>rye run &lt;command&gt;  # Run a command with project dependencies\nrye show           # Show current environment info\nrye toolchain list --include-downloadable  # List available Python versions\n</code></pre>"},{"location":"contributing.html#2-ide-configuration","title":"2. IDE Configuration","text":"<p>Configure your IDE to use the Python interpreter from <code>.venv/bin/python</code>. This ensures consistent development settings across the team.</p>"},{"location":"contributing.html#troubleshooting-rye","title":"Troubleshooting Rye","text":"<p>If you encounter issues: 1. Update Rye: <code>rye self update</code> 2. Verify Python path: <code>which python3</code> 3. Check environment: <code>rye show</code> 4. List toolchains: <code>rye toolchain list --include-downloadable</code></p>"},{"location":"contributing.html#3-pre-commit-hooks","title":"3. Pre-commit Hooks","text":"<p>We use pre-commit hooks to ensure code quality and consistency.</p> <pre><code>rye run pre-commit install\n</code></pre> <p>For more information, visit the pre-commit documentation.</p>"},{"location":"contributing.html#4-rust-development-setup-optional","title":"4. Rust Development Setup (Optional)","text":"<p>If you plan to work on Rust components:</p>"},{"location":"contributing.html#install-rust-toolchain","title":"Install Rust Toolchain","text":"<pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n</code></pre>"},{"location":"contributing.html#system-dependencies","title":"System Dependencies","text":"<p>For Linux: <pre><code>sudo apt install -y clang mold protobuf-compiler\n</code></pre></p> <p>For macOS: <pre><code>brew install mold protobuf\n</code></pre></p> <p>Add Cargo to your PATH: <pre><code>export PATH=\"$HOME/.cargo/bin:$PATH\"  # Add to your shell's RC file\n</code></pre></p>"},{"location":"contributing.html#verify-rust-setup","title":"Verify Rust Setup","text":"<pre><code>cd rs\ncargo check\n</code></pre>"},{"location":"contributing.html#5-nodejs-and-yarn","title":"5. Node.js and Yarn","text":"<p>Required for frontend development:</p> <ol> <li> <p>Install NVM: <pre><code>curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash\n</code></pre></p> </li> <li> <p>Install Node.js: <pre><code>nvm install 14\nnvm use 14\n</code></pre></p> </li> <li> <p>Install Yarn: <pre><code>npm install --global yarn\n</code></pre></p> </li> </ol>"},{"location":"contributing.html#project-structure","title":"\ud83d\udcc2 Project Structure","text":"<p>The DRE repository is organized into several key components:</p> <ul> <li><code>/dashboard</code> - Internal DRE dashboard (frontend and backend)</li> <li><code>/rs</code> - Rust implementations</li> <li><code>/pylib</code> - Python libraries</li> <li><code>/docs</code> - Project documentation</li> <li><code>/k8s</code> - Kubernetes configurations</li> <li><code>/scripts</code> - Utility scripts</li> </ul>"},{"location":"contributing.html#code-style-guidelines","title":"\ud83d\udcdd Code Style Guidelines","text":""},{"location":"contributing.html#python","title":"Python","text":"<ul> <li>Follow PEP 8 style guide</li> <li>Use type hints for function arguments and return values</li> <li>Document functions and classes using docstrings</li> <li>Maximum line length: 100 characters</li> </ul>"},{"location":"contributing.html#rust","title":"Rust","text":"<ul> <li>Follow the official Rust Style Guide</li> <li>Use <code>rustfmt</code> for code formatting</li> <li>Run <code>clippy</code> for linting</li> </ul>"},{"location":"contributing.html#javascripttypescript","title":"JavaScript/TypeScript","text":"<ul> <li>Follow the project's ESLint configuration</li> <li>Use TypeScript for new code</li> <li>Follow the Angular commit message format</li> </ul>"},{"location":"contributing.html#development-workflow","title":"\ud83d\udd04 Development Workflow","text":"<ol> <li> <p>Fork the repository and create your branch from <code>main</code>:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Set up development environment:    <pre><code>rye sync\nrye run pre-commit install\n</code></pre></p> </li> <li> <p>Make your changes:</p> </li> <li>Write tests for new functionality</li> <li>Update documentation as needed</li> <li> <p>Follow code style guidelines</p> </li> <li> <p>Commit your changes:    <pre><code>git commit -m \"feat: add new feature\"\n</code></pre>    Follow the conventional commits specification</p> </li> <li> <p>Push to your fork and create a pull request</p> </li> </ol>"},{"location":"contributing.html#pull-request-process","title":"\ud83d\udd0d Pull Request Process","text":"<ol> <li>Ensure all tests pass locally</li> <li>Update documentation if needed</li> <li>Add a clear description of the changes</li> <li>Link any related issues</li> <li>Request review from maintainers</li> <li>Address review feedback</li> <li>Ensure CI checks pass</li> </ol>"},{"location":"contributing.html#pr-title-format","title":"PR Title Format","text":"<ul> <li>feat: Add new feature</li> <li>fix: Fix bug</li> <li>docs: Update documentation</li> <li>test: Add tests</li> <li>refactor: Code refactoring</li> <li>chore: Maintenance tasks</li> </ul>"},{"location":"contributing.html#running-tests","title":"\u26a1 Running Tests","text":""},{"location":"contributing.html#backend-tests","title":"Backend Tests","text":"<pre><code>rye run pytest\n</code></pre>"},{"location":"contributing.html#frontend-tests","title":"Frontend Tests","text":"<pre><code>cd dashboard\nyarn test\n</code></pre>"},{"location":"contributing.html#ic-network-internal-dashboard","title":"\ud83d\udda5 IC Network Internal Dashboard","text":""},{"location":"contributing.html#setup","title":"Setup","text":"<pre><code>cd dashboard\nyarn install\n</code></pre>"},{"location":"contributing.html#development","title":"Development","text":"<pre><code>yarn dev  # Starts development server\n</code></pre>"},{"location":"contributing.html#using-dre-cli-with-local-dashboard","title":"Using DRE CLI with Local Dashboard","text":"<pre><code>dre --dev subnet replace --id &lt;subnet-id&gt; -o1\n</code></pre>"},{"location":"contributing.html#common-issues","title":"\u2757 Common Issues","text":""},{"location":"contributing.html#linux-no-disk-space-left-with-bazel","title":"Linux: \"No disk space left\" with Bazel","text":"<p>If you encounter inotify issues: <pre><code>sudo sysctl -w fs.inotify.max_user_watches=1048576\n</code></pre></p>"},{"location":"contributing.html#other-common-issues","title":"Other Common Issues","text":"<ol> <li> <p>Permission Denied Errors <pre><code>sudo chown -R $(whoami) .\n</code></pre></p> </li> <li> <p>Node Version Mismatch <pre><code>nvm use 14  # Ensure correct Node version\n</code></pre></p> </li> <li> <p>Bazel Cache Issues <pre><code>bazel clean --expunge\n</code></pre></p> </li> </ol>"},{"location":"contributing.html#getting-help","title":"\ud83e\udd1d Getting Help","text":"<ul> <li>Check existing GitHub Issues</li> <li>Join our developer community</li> <li>Review our documentation</li> <li>Reach out to maintainers on Discord</li> </ul>"},{"location":"contributing.html#before-asking-for-help","title":"Before Asking for Help","text":"<ol> <li>Search existing issues</li> <li>Check the documentation</li> <li>Try troubleshooting steps</li> <li>Provide relevant details when asking</li> </ol> <p>Remember: Good code is not just about functionality\u2014it's about maintainability, readability, and collaboration. Thank you for contributing to DRE! \ud83d\ude80</p>"},{"location":"create-a-neuron.html","title":"Create a New Neuron for Proposal Submissions","text":"<p>Follow these steps to create a new neuron for proposal submission.</p>"},{"location":"create-a-neuron.html#requirements","title":"Requirements","text":"<ul> <li>26 ICP (25 of which are to be staked for the NNS proposal deposit)</li> <li>Basic understanding of neurons, staking, and governance proposals.</li> <li>Optional Hardware wallet</li> </ul>"},{"location":"create-a-neuron.html#install-the-required-tools","title":"Install the Required Tools","text":"<p>Install dfx <pre><code>sh -ci \"$(curl -fsSL https://smartcontracts.org/install.sh)\"\n</code></pre></p> <pre><code>export PATH=$HOME/bin:$PATH\n</code></pre> <p>If you already had <code>dfx</code> installed, make sure it's updated to the latest version <pre><code>dfx upgrade\ndfx --version\n</code></pre></p>"},{"location":"create-a-neuron.html#create-and-manage-neuron-via-nns-frontend-dapp-and-internet-identity","title":"Create and Manage Neuron via NNS Frontend Dapp and Internet Identity","text":"<ol> <li>Send at least 11 ICPs (10 of which are required for proposals submisison, and 1 is aiming to cover all potential fees) to your NNS UI address.</li> <li>Navigate to the Neurons tab in the NNS UI and create a Neuron by staking at least 10 ICP.</li> <li>Set the dissolve delay to at least 6 months and confirm the choice.</li> </ol>"},{"location":"create-a-neuron.html#create-a-neuron-hotkey","title":"Create a neuron Hotkey","text":"<p>The neuron in the NNS UI (https://nns.ic0.app) needs to be managed with a locally generated private key, in order to be able to submit proposals from the command line. This is done through a so called Hotkey. In the NNS UI you need to add a locally generated identity as a hotkey, which means that you will be able to perform actions with the NNS UI identity, by using the locally generated identity.</p> <p><pre><code>dfx identity new --storage-mode=plaintext neuron-hotkey\ndfx --identity neuron-hotkey identity get-principal\n</code></pre> Example output: <code>wuyst-x5tpn-g5wri-mp3ps-vjtba-de3xs-w5xgb-crvek-tucbe-o5rqi-mae</code></p> <p>That's it, you now have a local identity that can submit proposals.</p>"},{"location":"create-a-neuron.html#topping-up-a-neuron","title":"Topping up a neuron","text":"<p>You may need to top up a neuron to send more proposals, either more proposals at the same time, or add balance in case some proposal gets rejected. You need 10 ICP per proposal submission.</p> <p>The recommended way to add more ICPs to the neuron (increase the stake) is through the NNS UI. This should be straightforward and you can just follow the instructions in the NNS UI.</p> <p>Topping up can also be done from the command line but is slightly more involved. We have built a helper utility for this:</p> <p><pre><code>dre neuron top-up\n</code></pre> Will print you the neuron account address. Then you add funds to the printed account address, and after that you need to run:</p> <pre><code>dre neuron refresh\n</code></pre> <p>So that the staked balance on the neuron is refreshed by the governance canister. Refreshing is typically done automatically when topping up the neuron from the NNS UI.</p>"},{"location":"create-a-neuron.html#optional-and-advanced-hardware-wallet-identity","title":"(Optional and advanced) Hardware wallet identity","text":"<p>In the NNS UI you can also add a hardware wallet as a controlling entity. You will not be able to submit proposals with the hardware wallet entity though. But you can keep all your funds on the hardware wallet.</p> <p>Steps:</p> <ol> <li>After the basic NNS neuron is created, add the hardware wallet in the NNS UI.</li> <li>Confirm adding the IC NNS Dapp in your hardware wallet by following the instructions from the hardware wallet.</li> <li>You can send funds to the hardware wallet address instead of sending them to the regular address in the NNS UI.</li> <li>When staking ICPs, confirm the transaction on your hardware wallet.</li> <li>For voting and proposal submission from the command line you still need to add a regular dfx identity (plaintext)</li> </ol>"},{"location":"decentralization.html","title":"Decentralization in the DRE Codebase","text":"<p>The decentralization in this code is measured using the Nakamoto Coefficient. Nakamoto Coefficient is a key metric used to evaluate the decentralization of decentralized networks by determining the minimum number of independent entities required to disrupt the network's consensus. The coefficient is calculated by analyzing the distribution of nodes across various dimensions or features.</p>"},{"location":"decentralization.html#dimensions-features-used","title":"Dimensions / Features Used:","text":"<ol> <li>Node Providers: Entities that own or manage nodes.</li> <li>Geographic Distribution: Locations such as country, continent, or city.</li> <li>Data Center Ownership: Ensures that no single data center or its owner dominates.</li> <li>Other Custom Features: Any additional features deemed critical for decentralization.</li> </ol> <p>Each of these features is independently evaluated to understand the distribution of control. The Nakamoto Coefficient is calculated for each feature by determining how many of the most dominant entities are needed to control a critical portion of the network.</p>"},{"location":"decentralization.html#calculation-of-nakamoto-coefficients","title":"Calculation of Nakamoto Coefficients:","text":"<ol> <li>Feature Analysis: The code first counts how many nodes are controlled by each entity for each feature.</li> <li>Sorting and Accumulation: These counts for a particular feature are sorted in descending order, and the code accumulates the counts until a threshold (typically one-third of the total nodes) is crossed.</li> <li>Coefficient Determination: The number of entities required to cross this threshold determines the Nakamoto Coefficient for that feature.</li> </ol> <p>Example:</p> <p>Let's consider calculation of the Nakamoto Coefficient for the Node Provider feature in a hypothetical 13-node subnet:</p> <ul> <li>Provider A controls 6 nodes.</li> <li>Provider B controls 3 nodes.</li> <li>Provider C controls 2 nodes.</li> <li>Provider D controls 1 node.</li> <li>Provider E controls 1 node.</li> </ul> <p>Next, sort the Node Providers by the number of nodes they control, in descending order:</p> <pre><code>A: 6, B: 3, C: 2, D: 1, E: 1\n</code></pre> <p>Calculate Total Nodes: summing up, we get 13.</p> <p>Determine the Threshold: For Nakamoto Coefficient, consider the number of nodes required to control more than \u2153rd of the total nodes:</p> \\[\\text{Threshold} = \\lceil \\frac{13}{3} \\rceil = 5\\] <p>Begin accumulating the node counts from the top of the sorted list until the threshold is met.</p> <p>Accumulate Nodes: <code>A: 6</code> -- Threshold is already exceeded with the Provider A: therefore the Nakamoto Coefficient is 1</p> <p>Result: The Nakamoto Coefficient for the Node Provider feature is 1, meaning that a single Node Provider (Provider A) can disrupt the network by controlling more than \u2153rd of the nodes.</p> <p>Example with a Higher Coefficient:</p> <p>Suppose the distribution was: <pre><code>A: 4, B: 4, C: 3, D: 2\n</code></pre> Accumulate nodes: <code>A + B = 4 + 4 = 8 (exceeds the threshold, Nakamoto Coefficient is 2)</code></p> <p>Result: Here, the Nakamoto Coefficient is 2, as at least two providers are needed to exceed the threshold.</p>"},{"location":"decentralization.html#comparison-of-subnet-configurations","title":"Comparison of Subnet Configurations:","text":"<p>When comparing two subnet configurations, the code evaluates the Nakamoto Coefficient across all features before and after a change. The aim is to maximize the lowest Nakamoto Coefficient (minimizing the risk of centralization) while considering other factors like geographic diversity and feature balance.</p>"},{"location":"decentralization.html#exhaustive-search-vs-heuristic-approach","title":"Exhaustive Search vs. Heuristic Approach","text":""},{"location":"decentralization.html#exhaustive-search","title":"Exhaustive Search:","text":"<p>When considering the addition or removal of nodes in a subnet, an exhaustive search would evaluate every possible combination of available nodes. The goal is to identify the combination that maximizes the Nakamoto Coefficient. However, the computational complexity grows exponentially as the number of nodes to be replaced increases.</p> <p>Example Calculations:</p> <ol> <li> <p>Single Node Replacement:</p> <ul> <li>For 500 available nodes, the number of possible combinations is 500.</li> <li>Nakamoto Coefficient Calculations: 500</li> <li>Comparisons: 499</li> </ul> </li> <li> <p>Two Nodes Replacement:</p> <ul> <li>Possible combinations: $$ \\binom{500}{2} = \\frac{500 \\times 499}{2} = 124,750$$</li> <li>Nakamoto Coefficient Calculations: 124,750</li> <li>Comparisons: 124,749</li> </ul> </li> <li> <p>Three Nodes Replacement:</p> <ul> <li>Possible combinations: $$ \\binom{500}{3} = \\frac{500 \\times 499 \\times 498}{6} = 20,708,500$$</li> <li>Nakamoto Coefficient Calculations: 20,708,500</li> <li>Comparisons: 20,708,499</li> </ul> </li> <li> <p>Four Nodes Replacement:</p> <ul> <li>Possible combinations: $$ \\binom{500}{4} = \\frac{500 \\times 499 \\times 498 \\times 497}{24} \\approx 2.57 \\times 10^9$$</li> <li>Nakamoto Coefficient Calculations: 2.57 billion</li> <li>Comparisons: 2.57 billion - 1</li> </ul> </li> <li> <p>Five Nodes Replacement:</p> <ul> <li>Possible combinations: $$ \\binom{500}{5} = \\frac{500 \\times 499 \\times 498 \\times 497 \\times 496}{120} \\approx 2.57 \\times 10^{11}$$</li> <li>Nakamoto Coefficient Calculations: 257 billion</li> <li>Comparisons: 257 billion - 1</li> </ul> </li> </ol> <p>As you can see, we very quickly get into a combinatorial explosion, which makes it computationally extremely expensive to perform an exhaustive search. We therefore have to approach a heuristic in general case.</p>"},{"location":"decentralization.html#heuristic-approach","title":"Heuristic Approach:","text":"<p>Rather than evaluating all possible combinations, the heuristic approach:</p> <ol> <li> <p>Calculate Nakamoto Coefficients for all Candidates:</p> <ul> <li>For each available node calculate the Nakamoto coefficients (across all dimensions/features) that we would get if we added the particular node into the subnet.</li> <li> <p>Penalty System:</p> </li> <li> <p>Penalize candidate nodes that do not satisfy essential business rules, such as avoiding centralization in specific regions or data centers.</p> </li> <li>Narrow down the candidates with the best Nakamoto coefficients:</li> <li>From the list of all candidate nodes, compose a list of the candidates that all have the same and maximum Nakamoto coefficients across all dimensions</li> </ul> </li> <li> <p>Deterministic Random Selection:</p> <ul> <li>Among the top candidates (those that pass the initial selection), a deterministic random process is used to choose the best node(s). If some of the initial nodes are in the top candidates, prefer to keep the initial nodes.</li> </ul> </li> </ol> <p>Example Calculations:</p> <ol> <li> <p>Single Node Replacement:</p> <ul> <li>Nakamoto Coefficient Calculations: 500 (same as exhaustive)</li> <li>Comparisons: 500 (same as exhaustive)</li> </ul> </li> <li> <p>Multiple Nodes Replacement:</p> <ul> <li>Instead of calculating every combination of candidate nodes, the heuristic picks a replacement node in each iteration, dramatically reducing calculations. For instance:<ul> <li>Two Nodes: 500 + 499 = 999</li> <li>Three Nodes: 500 + 499 + 498 = 1497</li> </ul> </li> <li>The cost of calculation grows linearly with the number of replaced nodes (total cost being approximately the number of available nodes * number of replaced nodes)</li> <li>This is significantly fewer than exhaustive combinations and makes it possible to very quickly replace 10s of nodes.</li> </ul> </li> </ol> <p>In summary, the heuristic approach enables the code to efficiently find optimal or near-optimal solutions without the prohibitive cost of exhaustive search. It balances the need for thorough evaluation with the practical limits of computation, making it feasible to manage large, decentralized networks effectively.</p> <p>If needed in the future, the node selection process can be improved by keeping the best potential replacement candidates (e.g. 10-20 nodes) and only perform exhaustive combinatorial search among those. However, even in that case the number of computations would very quickly grow and would be necessary to approach such a change very carefully.</p>"},{"location":"decentralization.html#other-areas-of-interest","title":"Other Areas of Interest","text":""},{"location":"decentralization.html#optimization-limits","title":"Optimization Limits:","text":"<p>The system limits the number of node replacements in a single operation to prevent excessive state-sync operations and disruption to the network's state, especially in critical subnets.</p>"},{"location":"decentralization.html#health-considerations","title":"Health Considerations:","text":"<p>Unhealthy or degraded nodes are prioritized for removal, and by default are automatically included into the replacement proposals, to maintain the network's overall health.</p>"},{"location":"decentralization.html#business-rule-enforcement","title":"Business Rule Enforcement:","text":"<p>The code enforces strict business rules to prevent any single entity or feature from becoming overly dominant. This includes maintaining a minimum number of DFINITY-owned nodes in each subnet, ensuring geographic diversity, and adhering to the Target Topology.</p> <p>How does DRE tooling handle these business rule requirements? If any business rule is not satisfied, the code adds a penalty to the candidate node. Subsequently, when comparing all candidate nodes, the code prioritizes those with the lowest penalties and selects candidates that also improve on other dimensions.</p> <p>Thus, if any candidate nodes meet the business rule requirements, they will be selected. If no candidates improve the business rules, the code opts for solutions with the lowest penalties, meaning the code picks those that are closest to the Target IC Topology and other business requirements.</p>"},{"location":"decentralization.html#network-healing-and-resilience","title":"Network Healing and Resilience:","text":"<p>The network healing process is designed to identify unhealthy subnets and optimize them by replacing underperforming nodes with those that improve decentralization and resilience. This process is particularly critical for maintaining the security and efficiency of critical subnets like the NNS (Network Nervous System).</p>"},{"location":"decentralization.html#quick-introduction-to-the-source-code","title":"Quick Introduction to the Source Code","text":"<p>The lib.rs file in the codebase serves as the main entry point for the module that handles decentralization in the network. Here's a breakdown of its contents:</p>"},{"location":"decentralization.html#modules","title":"Modules","text":"<ul> <li>nakamoto/mod.rs: Likely contains logic for calculating the Nakamoto Coefficient, a metric to measure decentralization.</li> <li>network.rs: Handles operations related to the network, including nodes and subnets.</li> <li>subnets.rs: Manages subnet-related operations.</li> </ul>"},{"location":"decentralization.html#structs","title":"Structs","text":"<ul> <li><code>SubnetChangeResponse</code>: Captures the details of changes made to a subnet, including nodes added or removed, changes in the Nakamoto score, and additional metadata like motivation and comments.</li> </ul>"},{"location":"decentralization.html#key-features","title":"Key Features:","text":"<ul> <li>Nakamoto Score: The struct utilizes the Nakamoto score to compare the decentralization state before and after changes.</li> <li>Feature Diff: Provides a detailed comparison of network features before and after a change, stored in a <code>BTreeMap</code>.</li> </ul>"},{"location":"decentralization.html#implementation-details","title":"Implementation Details:","text":"<ul> <li>The struct provides methods to generate responses based on subnet changes and can display these changes, including the impact on decentralization, in a user-friendly format.</li> </ul>"},{"location":"decentralization.html#display-implementation","title":"Display Implementation:","text":"<ul> <li>The <code>Display</code> trait is implemented to print detailed information about the changes, including the impact on the Nakamoto coefficient and lists of added or removed nodes. It also provides a tabular view of feature differences, making it easy to see how changes affect network decentralization.</li> </ul>"},{"location":"decentralization.html#nakamoto-coefficient-calculation","title":"Nakamoto Coefficient Calculation","text":"<p>Nakamoto coefficient calculation source code: nakamoto/mod.rs.</p>"},{"location":"decentralization.html#node-replacement-and-selection","title":"Node Replacement and Selection","text":"<p>The code optimizes for decentralization when replacing nodes by carefully selecting which nodes to add or remove from a subnet.</p>"},{"location":"decentralization.html#node-removal","title":"Node Removal","text":"<p>The node removal process prioritizes removing nodes belonging to over-represented actors, reducing centralization risk. It calculates how each candidate's removal affects the Nakamoto Coefficient, choosing the node whose removal least negatively impacts the network's decentralization.</p>"},{"location":"decentralization.html#node-addition","title":"Node Addition","text":"<p>Adding a node follows similar logic. The system seeks to increase decentralization by selecting a node that strengthens the network's distribution across key features. The chosen node typically adds diversity, either by being from an under-represented actor or by being located in a different geographical area.</p>"},{"location":"decentralization.html#calculation-of-best-candidate-nodes","title":"Calculation of Best Candidate Nodes","text":"<p>The code calculates the \"best candidate\" for node removal and addition by evaluating several criteria:</p> <ul> <li>Impact on Nakamoto Coefficient: The primary criterion for node replacement is how a node's addition or removal affects the Nakamoto Coefficient. The goal is to maintain or increase the coefficient, ensuring the network remains decentralized.</li> <li>Controlled Nodes: The number of nodes controlled by a single actor is assessed, and nodes that would increase this control are deprioritized.</li> <li>Critical Features: For critical features (e.g., Node Providers, Countries), the system checks the balance and tries to distribute control evenly across actors.</li> </ul> <p>These checks are implemented in functions like <code>describe_difference_from</code> and <code>nakamoto</code>, which evaluate and compare different scenarios to ensure optimal decentralization.</p>"},{"location":"decentralization.html#network-optimization","title":"Network Optimization","text":"<p>The <code>src/network.rs</code> file is a comprehensive module that manages decentralized subnets in a distributed network. It involves various structures, traits, and implementations that work together to ensure optimal decentralization and resilience against centralization risks. Here's a more detailed analysis of its contents:</p>"},{"location":"decentralization.html#key-structures-and-their-roles","title":"Key Structures and Their Roles:","text":"<ol> <li> <p><code>Node</code> Structure:</p> <ul> <li>Represents individual nodes in the network, each identified by a <code>PrincipalId</code>.</li> <li>Stores node features, such as whether a node is owned by DFINITY.</li> <li>Implements utility methods for feature comparison, display, and conversion from external types (<code>ic_management_types::Node</code>).</li> </ul> </li> <li> <p><code>DecentralizedSubnet</code> Structure:</p> <ul> <li>Manages subnet configurations, maintaining a list of nodes and tracking changes (additions/removals).</li> <li>Includes methods for subnet modifications while ensuring compliance with decentralization business rules, such as maintaining a minimal number of DFINITY-owned nodes and geographic diversity.</li> <li>Calculates the Nakamoto Coefficient to measure decentralization and uses this metric to evaluate the impact of node changes.</li> </ul> </li> <li> <p>Node Replacement and Subnet Optimization:</p> <ul> <li>Implements sophisticated logic for selecting nodes to add or remove from subnets, guided by the goal of maximizing decentralization.</li> <li><code>ReplacementCandidate</code>: Evaluates potential nodes for inclusion or removal, considering their impact on the Nakamoto Coefficient and any penalties (e.g., non-decentralization).</li> <li>Methods like <code>subnet_with_more_nodes</code> and <code>subnet_with_fewer_nodes</code> handle adding or removing nodes while preserving or enhancing decentralization.</li> </ul> </li> <li> <p>SubnetChangeRequest Structure:</p> <ul> <li>Facilitates requests to modify subnets, including adding or removing nodes.</li> <li>Includes methods to optimize the subnet by balancing the Nakamoto Coefficient, ensuring the network remains decentralized.</li> <li>Can be used to \"rescue\" a subnet by reconfiguring its nodes to restore or improve its decentralization metrics.</li> </ul> </li> <li> <p>Network Healing:</p> <ul> <li><code>NetworkHealRequest</code>: Orchestrates the process of identifying unhealthy nodes within subnets and optimizes the network by replacing these nodes with healthier, more decentralized alternatives.</li> <li>Prioritizes critical subnets like the NNS (Network Nervous System) during the healing process.</li> <li>Implements a deterministic yet randomized selection process to avoid biases in node replacement while ensuring consistency in subnet composition.</li> </ul> </li> <li> <p>Business Rule Enforcement:</p> <ul> <li>Rigorously enforces business rules for subnets, such as ensuring that no single actor controls too many nodes and that geographic diversity is maintained.</li> <li>The <code>check_business_rules</code> method validates subnet configurations against these rules, issuing penalties for non-compliance that affect node selection during optimizations.</li> </ul> </li> </ol>"},{"location":"decentralization.html#detailed-logic-and-features","title":"Detailed Logic and Features:","text":"<ol> <li> <p>Deterministic Node Selection:</p> <ul> <li>Ensures that node selection, even when randomized, is deterministic based on the current subnet state. This guarantees that identical inputs yield identical results, crucial for maintaining consistency in a distributed network.</li> </ul> </li> <li> <p>Penalties and Score Comparisons:</p> <ul> <li>Nodes are evaluated based on penalties associated with violating business rules, and these penalties influence the selection of nodes during optimization. The code carefully balances penalties with the need to maximize the Nakamoto Coefficient.</li> </ul> </li> <li> <p>Handling of Critical Subnets:</p> <ul> <li>Critical subnets, such as those managing key infrastructure like the NNS, receive special attention during the healing process. The code ensures that these subnets are kept robust and decentralized, minimizing risks of centralization.</li> </ul> </li> <li> <p>Feature Mapping and Evaluation:</p> <ul> <li>Nodes are categorized by features (e.g., geographic location, data center ownership), and the code evaluates how changes to these features affect the overall decentralization of the network.</li> </ul> </li> </ol>"},{"location":"decentralization.html#subnet-health-and-node-removal","title":"Subnet Health and Node Removal","text":"<p>The <code>src/subnets.rs</code> file focuses on managing the health and removal of nodes within subnets. Here's a detailed breakdown:</p>"},{"location":"decentralization.html#functions-and-structures","title":"Functions and Structures:","text":"<ol> <li> <p><code>unhealthy_with_nodes</code> Function:</p> <ul> <li>Identifies subnets that contain unhealthy nodes. It takes a map of subnets and node health statuses, returning a map where each entry is a subnet ID and a list of unhealthy nodes within that subnet.</li> </ul> </li> <li> <p><code>NodesRemover</code> Struct:</p> <ul> <li>Manages the removal of nodes from the network based on various criteria.</li> </ul> <p>Key Fields:</p> <ul> <li><code>no_auto</code>: Disables automatic node removal.</li> <li><code>remove_degraded</code>: Determines whether to remove nodes marked as degraded, in addition to dead nodes.</li> <li><code>extra_nodes_filter</code>: A list of criteria to filter additional nodes for removal.</li> <li><code>exclude</code>: A list of features to exclude certain nodes from removal.</li> <li><code>motivation</code>: The reason or motivation for node removal.</li> </ul> <p><code>remove_nodes</code> Method:</p> <ul> <li>Processes the removal of nodes from the network based on the criteria set in the struct.</li> <li>Filters nodes based on their health status, subnet association, and whether they match specific filters or exclusion lists.</li> <li>Nodes are removed for reasons such as being unhealthy, duplicates, or matching specific filters.</li> </ul> </li> </ol>"},{"location":"decentralization.html#detailed-analysis","title":"Detailed Analysis:","text":"<ol> <li> <p>Node Health Management:</p> <ul> <li>The <code>unhealthy_with_nodes</code> function is essential for maintaining the health of the network by identifying and isolating unhealthy nodes within subnets. This function is likely used in broader network management routines to ensure subnets are composed of healthy nodes.</li> </ul> </li> <li> <p>Node Removal Process:</p> <ul> <li>The <code>NodesRemover</code> struct is designed to handle the complex process of node removal, allowing flexibility in what nodes are removed and why. By incorporating filters, exclusions, and health checks, this struct ensures node removal is performed in a controlled and justified manner.</li> <li>The <code>remove_nodes</code> method is particularly robust, handling various scenarios where nodes might need removal, including automatic removal based on health status, exclusion of specific nodes, and custom filtering.</li> </ul> </li> <li> <p>Flexibility and Customization:</p> <ul> <li>The file provides a high degree of customization in managing node health and removal, crucial for maintaining the overall health and decentralization of the network. The ability to filter, exclude, and motivate node removal allows network administrators to tailor the node management process to specific needs and conditions.</li> </ul> </li> </ol>"},{"location":"firewall-proposals.html","title":"Submitting firewall change proposals","text":"<p>This guide shows how to propose firewall rule changes using the DRE CLI. It reflects the exact behavior of the <code>dre firewall</code> command as implemented in the source code, so the steps and flags below are guaranteed to work.</p>"},{"location":"firewall-proposals.html#prerequisites","title":"Prerequisites","text":"<ul> <li>DRE CLI installed and on your PATH.</li> <li>A proposing neuron configured (HSM or private key).</li> <li>Access to the target network (e.g., mainnet).</li> </ul> <p>Helpful globals you can pass to any command: - <code>--network &lt;mainnet|staging|testnet&gt;</code> - <code>--private-key-pem &lt;path&gt;</code> or HSM flags - <code>--dry-run</code> to simulate first - <code>-y, --yes</code> to auto-confirm - <code>--forum-post-link &lt;URL|discourse|ask|omit&gt;</code> to control forum posting</p> <p>See <code>dre firewall --help</code> and the Authentication Options in the main README for details.</p>"},{"location":"firewall-proposals.html#scopes","title":"Scopes","text":"<p>Rules are stored per scope. The command accepts exactly these scope strings: - <code>global</code> - <code>replica_nodes</code> - <code>api_boundary_nodes</code> - <code>subnet(&lt;SUBNET_ID&gt;)</code>  e.g., <code>subnet(pjljw-...)</code> - <code>node(&lt;NODE_ID&gt;)</code>      e.g., <code>node(z6jp6-...)</code></p>"},{"location":"firewall-proposals.html#inspect-current-rules-recommended","title":"Inspect current rules (recommended)","text":"<pre><code># Example: view current global rules\ndre get firewall-rules global | jq\n\n# Or for replica nodes\ndre get firewall-rules replica_nodes | jq\n</code></pre>"},{"location":"firewall-proposals.html#make-a-change-via-the-interactive-editor","title":"Make a change via the interactive editor","text":"<p>Run: <pre><code># Minimal required flags\ndre firewall \\\n  --rules-scope global \\\n  --summary \"&lt;what and why; include forum link if you have it&gt;\"\n</code></pre> Optional but recommended: - <code>--title \"Proposal to modify firewall rules\"</code> (defaults to that value if omitted) - <code>--forum-post-link &lt;URL|discourse|ask|omit&gt;</code> - <code>--dry-run</code> to simulate; rerun without it to submit - <code>--yes</code> to skip the confirmation prompt, if desired</p> <p>What happens: 1. The tool fetches the current rules for the given scope and opens your <code>$EDITOR</code> with a JSON document. 2. The JSON is a map of <code>position -&gt; rule</code>, where positions are integers starting at 0 (order matters). Example structure: <pre><code>{\n  \"0\": { \"...\": \"existing rule\" },\n  \"1\": { \"...\": \"existing rule\" }\n}\n</code></pre> 3. To add a rule, copy an existing rule object as a template and paste it under a new key equal to the next available integer (append to the end). To update a rule, modify the value at its existing key. To remove a rule, delete its key.</p> <p>Important details based on the implementation: - The tool determines your intent by comparing the edited JSON to the original:   - New key -&gt; addition   - Same key with different value -&gt; update   - Removed key -&gt; removal - Only one type of change is submitted per run (add OR update OR remove). If you mix types, the tool will only submit one batch (the last-by-type after sorting). For multiple types, run the command multiple times. - Keep the rule object shape consistent with existing rules (field names and types). The safest approach is to copy an existing rule and adjust ports, prefixes, and comment. - To append to the end, use the next integer position. Do not renumber existing keys unless you intend to update or remove those rules.</p>"},{"location":"firewall-proposals.html#example-allow-dfinity-observability-to-attest-sev-protected-nodes-global","title":"Example: allow DFINITY observability to attest SEV-protected nodes (global)","text":"<p>Goal: open TCP port 19523 to a small set of IPv6 prefixes, at global scope.</p> <p>1) Preview current rules to find a similar ALLOW rule to copy its field names and valid <code>action</code> value: <pre><code>dre get firewall-rules global | jq\n</code></pre> 2) Run the interactive editor in dry-run first: <pre><code>dre firewall \\\n  --rules-scope global \\\n  --title \"Allow the DFINITY observability stack to remotely attest SEV-protected nodes\" \\\n  --summary \"We propose to allow the DFINITY observability stack to access the remote attestation endpoint on TCP port 19523. This access is essential for monitoring and attesting SEV-protected nodes. It allows the DFINITY observability stack to periodically fetch an attestation report and verify it. For details, see: &lt;FORUM POST URL&gt;\" \\\n  --forum-post-link &lt;FORUM POST URL&gt; \\\n  --dry-run\n</code></pre></p> <p>Example using a local summary file and the <code>--forum</code> alias: <pre><code>dre firewall \\\n  --summary \"$(cat 2025-10-07-observability-remote-attestation-summary.md)\" \\\n  --rules-scope global \\\n  --forum https://forum.dfinity.org/t/nns-proposal-enabling-remote-attestation-for-dfinity-observability/58611\n</code></pre> 3) In the JSON that opens, paste a full object. If the ruleset is empty, a minimal valid example is: <pre><code>{\n  \"0\": {\n    \"ipv4_prefixes\": [],\n    \"ipv6_prefixes\": [\n      \"2602:fb2b:100:12::/64\",\n      \"2602:fb2b:120:12::/64\",\n      \"2602:fb2b:110:12::/64\"\n    ],\n    \"ports\": [\n      19523\n    ],\n    \"action\": 1,\n    \"comment\": \"remote attestation for DFINITY obs stack\"\n  }\n}\n</code></pre> If there are existing entries, keep them as-is and add a new key at the next available integer (e.g., \"123\"). Use the same <code>action</code> value as existing ALLOW rules in your environment. Notes: - Copy the <code>action</code> value from an existing ALLOW rule in your current ruleset to ensure it matches the expected enum/representation. - If your environment stores other required fields (e.g., protocol), keep them as in the copied template and adjust as needed. - Some environments include extra fields such as <code>user</code> and <code>direction</code>. Preserve them as seen in your existing rules unless you intentionally need to change them.</p> <p>4) Save and exit the editor. The tool prints a pretty diff and simulates the proposal, computing a content hash. 5) If the simulation looks correct, rerun without <code>--dry-run</code>. Add <code>--yes</code> to skip confirmation if desired: <pre><code>dre firewall \\\n  --rules-scope global \\\n  --title \"Allow the DFINITY observability stack to remotely attest SEV-protected nodes\" \\\n  --summary \"We propose to allow the DFINITY observability stack to access the remote attestation endpoint on TCP port 19523. This access is essential for monitoring and attesting SEV-protected nodes. It allows the DFINITY observability stack to periodically fetch an attestation report and verify it. For details, see: &lt;FORUM POST URL&gt;\" \\\n  --forum &lt;FORUM POST URL&gt;\n</code></pre></p>"},{"location":"firewall-proposals.html#tips","title":"Tips","text":"<ul> <li>Always dry-run first to review the diff and the generated <code>ic-admin propose-to-...</code> command.</li> <li>Submit additions, updates, and removals in separate runs.</li> <li>Prefer appending new rules to the end unless you have a specific ordering requirement.</li> <li>Keep comments clear; include a forum link in the summary to aid reviewers.</li> </ul>"},{"location":"firewall-proposals.html#replica-nodes-rules-common-workflow","title":"Replica nodes rules: common workflow","text":"<p>Replica nodes (<code>replica_nodes</code>) scope is the most commonly changed set of rules. The workflow is identical to the global example above, but you pass <code>--rules-scope replica_nodes</code> and copy field shapes from the current replica ruleset.</p> <p>1) Inspect current replica rules and note the <code>action</code> value used for ALLOW and any extra fields: <pre><code>dre get firewall-rules replica_nodes | jq\n</code></pre></p> <p>2) Run the interactive editor in dry-run first: <pre><code>dre firewall \\\n  --rules-scope replica_nodes \\\n  --title \"Open TCP 19523 for SEV attestation to DFINITY observability\" \\\n  --summary \"Allow DFINITY observability prefixes to access TCP 19523 for remote attestation on all replica nodes. See: &lt;FORUM POST URL&gt;\" \\\n  --forum-post-link &lt;FORUM POST URL&gt; \\\n  --dry-run\n</code></pre></p> <p>3) Append a new entry at the next available integer key. Use the same <code>action</code> value as other ALLOW rules in the replica ruleset, and preserve optional fields if present: <pre><code>\"&lt;NEXT_INDEX&gt;\": {\n  \"ipv4_prefixes\": [],\n  \"ipv6_prefixes\": [\n    \"2602:fb2b:100:12::/64\",\n    \"2602:fb2b:120:12::/64\",\n    \"2602:fb2b:110:12::/64\"\n  ],\n  \"ports\": [19523],\n  \"action\": &lt;copy from an existing ALLOW rule&gt;,\n  \"comment\": \"remote attestation for DFINITY obs stack\",\n  \"user\": null,\n  \"direction\": null\n}\n</code></pre></p> <p>4) Save and exit to simulate. If correct, rerun without <code>--dry-run</code> (optionally with <code>--yes</code>). Submit additions, updates, and removals as separate runs if you need multiple kinds of changes.</p>"},{"location":"getting-started.html","title":"Getting Started","text":""},{"location":"getting-started.html#downloading-the-pre-built-binary","title":"Downloading the pre-built binary","text":"<p>We build the DRE tool for Linux and for MacOS (Darwin). We tested on Ubuntu 22.04 and 24.04.</p> <p>On Linux, you can download the tool with:</p> <pre><code>mkdir -p $HOME/bin\ncurl -L https://github.com/dfinity/dre/releases/latest/download/dre-x86_64-unknown-linux -o $HOME/bin/dre\nchmod +x $HOME/bin/dre\n</code></pre> <p>On MacOS you can use the following:</p> <pre><code>mkdir -p $HOME/bin\ncurl -L https://github.com/dfinity/dre/releases/latest/download/dre-x86_64-apple-darwin -o $HOME/bin/dre\nchmod +x $HOME/bin/dre\n</code></pre> <p>Make sure that <code>$HOME/bin</code> is added to your path. If it's not, you might get an error such as: <pre><code>\u276f dre\nCommand 'dre' not found, did you mean:\n[...]\n</code></pre></p> <p>To fix this issue, you can run <pre><code>export PATH=$HOME/bin:$PATH\n</code></pre></p> <p>And you can also add this to your $HOME/.bashrc file (or the one appropriate for your shell).</p>"},{"location":"how-to-update-docs.html","title":"How to Update Documentation","text":"<p>We use MkDocs to generate, serve, and search the team documentation. For full documentation visit mkdocs.org.</p>"},{"location":"how-to-update-docs.html#commands","title":"Commands","text":"<ul> <li><code>bazel run \"//:mkdocs\" new [dir-name]</code> - Create a new project.</li> <li><code>bazel run \"//:mkdocs\" serve</code> - Start the live-reloading docs server.</li> <li><code>bazel run \"//:mkdocs\" build</code> - Build the documentation site.</li> <li><code>bazel run \"//:mkdocs\" -h</code> - Print help message and exit.</li> </ul> <p>To generate documentation as HTML, you can use convenience script <code>./bin/mkdocs-build.sh</code>. The generated documentation will be in the <code>site</code> subdirectory of the repo root.</p>"},{"location":"how-to-update-docs.html#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"make-release.html","title":"How to make a new release","text":"<p>Go to the repo root, and check that you don't have any dirty changes and that you are on the main branch.</p> <pre><code>git checkout main\ngit pull\ngit status\n</code></pre> <p>If all looks okay, you can create a tag and push it.</p> <p>Example:</p> <pre><code>TAG=v1.2.3\ngit tag $TAG\ngit push origin $TAG \n</code></pre> <p>Next, go to the github repo and find the workflow Release DRE binary. You should find the tag you pushed at the top. After it finishes two things will happen:</p> <ol> <li>A new PR will be created (like this one) that contains updated files. </li> <li>A new Draft release will be created with the name you set as the tag.</li> </ol> <p>If there are any issues with the PR, resolve that first, request approvals, and merge the changes. </p> <p>Only after merging the PR should you find your draft release in GH releases page. You will also have a link on the PR that contains the link to the exact release. Review the release notes and make changes if needed.</p> <p>Set the <code>latest</code> to point to this new release by unselecting <code>Set as a pre-release</code> and selecting <code>Set as the latest release</code>. Create a new discussion for the release, and finally click <code>Publish release</code>.</p> <p></p> <p>Celebrate!</p>"},{"location":"neuron-following-setup.html","title":"Setting Up Neuron Following for Subnet Upgrades","text":"<p>This guide describes how to set up neuron following for automated subnet upgrades on the Internet Computer. This enables a secure approval process for subnet upgrade proposals where multiple parties need to be involved.</p>"},{"location":"neuron-following-setup.html#overview","title":"Overview","text":"<p>The neuron following setup ensures that automated subnet upgrade proposals require approval from both: 1. The automation system (primary proposer) 2. At least one authorized release team member</p> <p>This provides a secure two-party approval system for subnet upgrades.</p>"},{"location":"neuron-following-setup.html#technical-details","title":"Technical Details","text":"<ul> <li>Topic ID: <code>12</code> (IC-OS Version Deployment)</li> <li>Topic Reference: Defined in the IC governance protobuf as <code>TOPIC_IC_OS_VERSION_DEPLOYMENT</code></li> </ul>"},{"location":"neuron-following-setup.html#setting-up-neuron-following","title":"Setting Up Neuron Following","text":"<p>To set up neuron following, you'll need: - Access to dfx CLI - Your neuron ID - The ID of the neuron you want to follow - HSM identity configured</p>"},{"location":"neuron-following-setup.html#command-to-set-up-following","title":"Command to Set Up Following","text":"<pre><code>cd &lt;path&gt;/ic/rs/nns\n# Export required variables\nexport NEURON_ID=&lt;Your neuron ID&gt;\nexport NEURON_TO_FOLLOW=&lt;Neuron ID to follow&gt;\nexport DFX_HSM_PIN=\"$(cat ~/.hsm-pin)\"\n\n# Set up following for your neuron\ndfx --identity hsm canister --network mainnet call governance manage_neuron \\\n  \"(record{ id = opt record{ id = ${NEURON_ID} : nat64 }; command = opt variant{Follow = record{ topic = 12 : int32; followees = vec{ record{ id = ${NEURON_TO_FOLLOW} : nat64 } } }}})\"\n</code></pre>"},{"location":"neuron-following-setup.html#verify-following-setup","title":"Verify Following Setup","text":"<p>You can verify your neuron following configuration with:</p> <pre><code>dfx --identity hsm canister --network mainnet call governance get_full_neuron \"(${NEURON_ID}:nat64)\"\n</code></pre> <p>The output will include a <code>followees</code> section showing all topics and neurons being followed.</p>"},{"location":"neuron-following-setup.html#notes","title":"Notes","text":"<ul> <li>Only configure following for Topic 12 (subnet replica version management)</li> <li>Ensure you're following the correct neuron based on your role</li> <li>Following relationships should be configured according to your organization's security policies</li> </ul>"},{"location":"nns-motion-proposals.html","title":"Submitting NNS Motion Proposals","text":"<p>This guide explains how to submit motion proposals to the Network Nervous System (NNS) using the DRE CLI tool, based on the actual implementation.</p>"},{"location":"nns-motion-proposals.html#prerequisites","title":"Prerequisites","text":"<ul> <li>DRE CLI tool installed</li> <li>A neuron with sufficient voting power</li> <li>Required proposal submission fee</li> <li>Forum discussion with community feedback (at least 2 weeks old)</li> <li>Authentication credentials (private key or HSM)</li> </ul>"},{"location":"nns-motion-proposals.html#forum-discussion-requirements","title":"Forum Discussion Requirements","text":"<p>Before submitting a motion proposal, it is crucial to:</p> <ol> <li> <p>Create a Forum Discussion</p> <ul> <li>Post your proposal on the official Internet Computer forum</li> <li>Include all relevant details and documentation</li> <li>Allow sufficient time for community feedback</li> </ul> </li> <li> <p>Discussion Period</p> <ul> <li>A minimum of 2 weeks of discussion is expected</li> <li>This period allows the community to:<ul> <li>Review the proposal</li> <li>Provide feedback</li> <li>Suggest improvements</li> <li>Raise concerns</li> </ul> </li> </ul> </li> <li> <p>Forum Link Requirement</p> <ul> <li>A link to the forum discussion is mandatory</li> <li>Proposals without forum discussion links will likely be rejected</li> <li>The discussion should show active engagement with community feedback</li> </ul> </li> </ol> <p>Important</p> <p>Proposals submitted without adequate forum discussion (less than 2 weeks) or without a forum link are likely to be rejected. Ensure you have met these requirements before submission.</p>"},{"location":"nns-motion-proposals.html#authentication-options","title":"Authentication Options","text":""},{"location":"nns-motion-proposals.html#private-key-authentication","title":"Private Key Authentication","text":"<pre><code>--private-key-pem &lt;PATH&gt;  # Path to private key file (PEM format)\n</code></pre>"},{"location":"nns-motion-proposals.html#hsm-authentication","title":"HSM Authentication","text":"<pre><code>--hsm-pin &lt;PIN&gt;          # Pin for the HSM key\n--hsm-slot &lt;SLOT&gt;        # Slot that HSM key uses\n--hsm-key-id &lt;KEY_ID&gt;    # HSM Key ID\n</code></pre>"},{"location":"nns-motion-proposals.html#proposal-components","title":"Proposal Components","text":"<p>A motion proposal consists of:</p> <ol> <li> <p>Summary (required, max 30 KiB)</p> <ul> <li>Written in Markdown format</li> <li>Can include a title as first-line H1 heading</li> <li>Main content describing the proposal</li> </ul> </li> <li> <p>Title (optional)</p> <ul> <li>Can be specified explicitly or extracted from summary</li> <li>If not specified, extracted from first H1 heading in summary</li> </ul> </li> <li> <p>Motion Text (optional, max 100 KiB)</p> <ul> <li>Detailed description of the motion</li> <li>If not provided, defaults to referencing the summary</li> </ul> </li> </ol>"},{"location":"nns-motion-proposals.html#command-line-usage","title":"Command Line Usage","text":""},{"location":"nns-motion-proposals.html#basic-command-structure","title":"Basic Command Structure","text":"<pre><code>dre governance propose motion [OPTIONS] &lt;SUMMARY_FILE&gt;\n</code></pre>"},{"location":"nns-motion-proposals.html#required-arguments","title":"Required Arguments","text":"<ul> <li><code>&lt;SUMMARY_FILE&gt;</code>: Path to the proposal summary file (max 30 KiB)<ul> <li>Use \"-\" to read from standard input</li> <li>First line can be H1 heading for title (it can start with <code>#</code>, for example: <code># Title</code>)</li> </ul> </li> </ul>"},{"location":"nns-motion-proposals.html#essential-options","title":"Essential Options","text":"<pre><code>--neuron-id &lt;ID&gt;         # Neuron ID for proposal submission, should not be necessary to be set explicitly\n--title &lt;TITLE&gt;          # Optional custom title\n--motion-text-file &lt;FILE&gt;# Motion text file (max 100 KiB)\n--motion-text &lt;TEXT&gt;     # Direct motion text input\n</code></pre>"},{"location":"nns-motion-proposals.html#forum-integration-options","title":"Forum Integration Options","text":"<pre><code>--forum-post-link &lt;OPTION&gt;  # Forum link handling:\n                           # - 'discourse': Auto-create post/topic\n                           # - URL: Direct forum post link\n                           # - 'ask': Prompt for link\n                           # - 'omit': Skip link (discouraged)\n</code></pre>"},{"location":"nns-motion-proposals.html#discourse-forum-integration-for-automatic-post-creation","title":"Discourse Forum Integration, for automatic post creation","text":"<p>Note that this requires a special API key, which may not be available to you.</p> <pre><code>--discourse-api-key &lt;KEY&gt;   # API key for forum interaction\n--discourse-api-user &lt;USER&gt; # API user (default: DRE-Team)\n--discourse-api-url &lt;URL&gt;   # Forum URL (default: https://forum.dfinity.org)\n</code></pre> <p>Also, due to the requirement that forum posts have long-lasting discussion before proposal submission, this type of discourse integration is not very useful for this type of proposals.</p>"},{"location":"nns-motion-proposals.html#additional-options","title":"Additional Options","text":"<pre><code>--dry-run              # Simulate proposal submission\n--verbose              # Print detailed information\n-y, --yes              # Skip confirmation prompt\n</code></pre>"},{"location":"nns-motion-proposals.html#motion-proposal-workflow-for-dre-team","title":"Motion Proposal Workflow for DRE Team","text":"<ol> <li>Prepare your motion proposal content in Markdown, similar to other files in <code>/docs/motion-proposals</code>.</li> <li>Create a new file under the repository path /docs/motion-proposals/.md (the filename can initially be descriptive, for example \"proposal-summary.md\"). <li>Open a pull request (PR) with your changes and get feedback from others.</li> <li>Address any feedback by making changes in the PR until the proposal content is approved.</li> <li>Submit the proposal using the DRE CLI tool, following the process described below.</li> <li>After submission, update the filename to include the submitted proposal ID (for example, change <code>proposal-summary.md</code> to <code>&lt;proposalID&gt; proposal-summary.md</code>).</li> <li>Merge the PR to finalize the changes.</li>"},{"location":"nns-motion-proposals.html#creating-the-proposal","title":"Creating the Proposal","text":""},{"location":"nns-motion-proposals.html#1-prepare-the-summary-file","title":"1. Prepare the Summary File","text":"<p>Create a Markdown file (e.g., <code>proposal-summary.md</code>) with your proposal:</p> <pre><code># Proposal Title\n\n## Overview\nBrief description of what this proposal aims to achieve.\n\n## Details\nDetailed explanation of the proposal...\n\n## Impact\nExpected outcomes and benefits...\n\n## Community Discussion\nLink to forum discussion: [Forum Thread Title](https://forum.dfinity.org/t/...)\nDiscussion period: DD/MM/YYYY - DD/MM/YYYY\n</code></pre>"},{"location":"nns-motion-proposals.html#2-optional-prepare-motion-text","title":"2. Optional: Prepare Motion Text","text":"<p>Create a separate file for detailed motion text (e.g., <code>motion-text.md</code>) if needed:</p> <pre><code>This motion proposes to...\n\nTechnical details:\n1. ...\n2. ...\n\nImplementation timeline:\n- Phase 1: ...\n- Phase 2: ...\n</code></pre>"},{"location":"nns-motion-proposals.html#step-3-adjusting-the-filename","title":"Step 3: Adjusting the Filename","text":"<p>After a successful submission, the governance canister will return a proposal ID. Rename the motion proposal markdown file to include this ID in its filename (for example, updating the file from <code>proposal-summary.md</code> to <code>&lt;proposalID&gt; proposal-summary.md</code>). Commit and push this change to the PR.</p>"},{"location":"nns-motion-proposals.html#step-4-merging-the-pr","title":"Step 4: Merging the PR","text":"<p>Finally, merge the PR that includes the updated filename to have an official record of the submitted proposal in the repository, for future reference.</p>"},{"location":"nns-motion-proposals.html#submission-examples","title":"Submission Examples","text":""},{"location":"nns-motion-proposals.html#basic-submission","title":"Basic Submission","text":"<pre><code>dre governance propose motion \\\n    --private-key-pem key.pem \\\n    --forum-post-link https://forum.dfinity.org/... \\\n    proposal-summary.md\n</code></pre>"},{"location":"nns-motion-proposals.html#with-custom-title-and-motion-text","title":"With Custom Title and Motion Text","text":"<pre><code>dre governance propose motion \\\n    --private-key-pem key.pem \\\n    --forum-post-link https://forum.dfinity.org/... \\\n    --title \"Custom Proposal Title\" \\\n    --motion-text-file motion-text.md \\\n    proposal-summary.md\n</code></pre>"},{"location":"nns-motion-proposals.html#dry-run-for-testing","title":"Dry Run for Testing","text":"<pre><code>dre governance propose motion \\\n    --forum-post-link https://forum.dfinity.org/... \\\n    --dry-run \\\n    proposal-summary.md\n</code></pre>"},{"location":"nns-motion-proposals.html#best-practices","title":"Best Practices","text":"<ol> <li> <p>Community Engagement</p> <ul> <li>Start forum discussion early (at least 2 weeks before submission)</li> <li>Actively engage with community feedback</li> <li>Document changes made based on community input</li> <li>Include forum discussion link in proposal</li> </ul> </li> <li> <p>Summary Format</p> <ul> <li>Start with a clear H1 heading for the title</li> <li>Keep content under 30 KiB</li> <li>Use Markdown formatting for readability</li> <li>Include forum discussion link and timeline</li> </ul> </li> <li> <p>Motion Text</p> <ul> <li>Keep content under 100 KiB</li> <li>Use clear, structured formatting</li> <li>Include implementation details</li> </ul> </li> <li> <p>Testing</p> <ul> <li>Use <code>--dry-run</code> to verify proposal before submission</li> <li>Verify forum integration with test post if using Discourse API</li> </ul> </li> </ol>"},{"location":"nns-motion-proposals.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"nns-motion-proposals.html#common-issues","title":"Common Issues","text":"<ol> <li> <p>File Size Limits</p> <ul> <li>Summary must be under 30 KiB</li> <li>Motion text must be under 100 KiB</li> </ul> </li> <li> <p>Authentication</p> <ul> <li>Verify private key or HSM credentials</li> <li>Check neuron ID and permissions</li> </ul> </li> <li> <p>Discourse Forum Integration</p> <ul> <li>Verify API key and permissions</li> <li>Check forum URL format</li> <li>Ensure post meets forum guidelines</li> </ul> </li> <li> <p>Forum Requirements</p> <ul> <li>Insufficient discussion period (less than 2 weeks)</li> <li>Missing forum discussion link</li> <li>Lack of community engagement</li> </ul> </li> </ol>"},{"location":"nns-motion-proposals.html#error-messages","title":"Error Messages","text":"<ul> <li>\"Summary must be valid UTF-8\": Check file encoding</li> <li>\"Proposal submission failed\": Verify neuron status and network connectivity</li> <li>No proposal ID returned: Check governance canister response</li> </ul>"},{"location":"nns-motion-proposals.html#additional-resources","title":"Additional Resources","text":"<ul> <li>NNS Documentation</li> <li>Internet Computer Forum</li> <li>Markdown Guide</li> <li>DRE CLI Documentation</li> </ul>"},{"location":"nns-proposals.html","title":"Submitting NNS proposals","text":"<p>Most of the commands here can be run in multiple ways. Currently we are putting in the effort to make <code>dre</code> as useful as possible. As such it provides support for <code>dry_run</code> as default and that can be highly beneficial in most scenarios (for eg. if someone is asking you to submit a proposal for them the best practice way is to run a <code>dry_run</code> and ask them to double check the command and the payload that would be submitted) and that is why we recommend using <code>dre</code> whenever possible. In some use-cases <code>dre</code> cannot help you, and that is when you should use whatever tool/script is at hand.</p>"},{"location":"nns-proposals.html#get-the-principal-from-your-hsm","title":"Get the principal from your HSM","text":"<pre><code>\u276f dfx identity use hsm\nUsing identity: \"hsm\".\n\u276f export DFX_HSM_PIN=$(cat ~/.hsm-pin)\n\u276f dfx identity get-principal\nas4rt-t4nqh-64j36-ubyaa-2c6uz-f2qbm-67llh-ftd3y-epn2e-wzaut-wae\n</code></pre>"},{"location":"nns-proposals.html#get-the-neuron-id-associated-with-your-hsm","title":"Get the neuron id associated with your HSM","text":"<pre><code>\u276f export DFX_HSM_PIN=$(cat ~/.hsm-pin)\n\u276f dfx canister --identity=hsm --network=ic call rrkah-fqaaa-aaaaa-aaaaq-cai get_neuron_ids '()'\n(vec { 40 : nat64 })\n</code></pre>"},{"location":"nns-proposals.html#getting-the-mainnet-firewall-rules","title":"Getting the Mainnet firewall rules","text":"<pre><code>dre get firewall-rules replica_nodes | jq\n</code></pre>"},{"location":"nns-proposals.html#get-the-node-rewards-table-used-for-the-node-provider-compensation","title":"Get the Node Rewards Table, used for the Node Provider compensation","text":"<pre><code>dre get node-rewards-table\n{\n  \"table\": {\n    \"Asia\": {\n[...]\n}\n</code></pre>"},{"location":"nns-proposals.html#update-the-node-rewards-table","title":"Update the Node Rewards Table","text":"<pre><code>dre propose update-node-rewards-table --summary-file 2022-12-type3.md --updated-node-rewards \"$(cat 2022-12-type3-rewards.json | jq -c)\"\n</code></pre>"},{"location":"nns-proposals.html#enable-the-https-outcalls-on-a-subnet","title":"Enable the HTTPs outcalls on a subnet","text":"<pre><code>dre propose update-subnet \\\n    --features \"http_requests\" \\\n    --subnet uzr34-akd3s-xrdag-3ql62-ocgoh-ld2ao-tamcv-54e7j-krwgb-2gm4z-oqe \\\n    --summary \"Enable the HTTPS outcalls feature on the non-whitelisted uzr34 subnet so that the exchange rate canister can query exchange rate data.\"\n</code></pre>"},{"location":"nns-proposals.html#removing-node-operator-principal-id","title":"Removing node operator principal id","text":"<pre><code>dre propose remove-node-operators kdqam-hauon-sdvym-42eyg-5wyff-4ywbw-v6iij-2sw2z-bu4rj-ejusn-jae \\\n    --summary \"&lt;An appropriate summary for the proposal, and a link to the forum post for further discussion, if possible&gt;\"\n</code></pre>"},{"location":"nns-proposals.html#replacing-or-removing-nodes-in-a-subnet","title":"Replacing (or removing) nodes in a subnet","text":"<p>There are situations where nodes need to be removed from subnets, such as for maintenance. Subnets are expected to maintain a certain size (number of nodes). Therefore, nodes cannot simply be removed from a subnet; new nodes must be added back into the subnet to maintain the required number.</p> <p>The <code>ic-admin</code> command for this is <code>propose-to-change-subnet-membership</code> but this command should NOT be invoked directly from <code>ic-admin</code> because it's necessary to consider the effect of node replacement on the subnet decentralization. The DRE tool will 1) find the optimal replacement for a node the given subnet, and 2) help you to review and submit the required NNS proposal.</p> <p>Typical usage would be like this:</p> <pre><code>dre subnet replace &lt;NODE_ID...&gt;\n</code></pre> <p>Note: It is possible to provide multiple node IDs in the same command line, but they MUST be in the same subnet.</p> <p>Example usage to remove an unhealthy node from the subnet <code>tdb26</code>:</p> <pre><code>dre subnet replace --id tdb26-jop6k-aogll-7ltgs-eruif-6kk7m-qpktf-gdiqx-mxtrf-vb5e6-eqe\n</code></pre> <p>Example usage to remove healthy nodes from their current subnet:</p> <pre><code>dre subnet replace --nodes tm3pc-2bjsx-hhv3v-fsrt7-wotdj-nbu3t-ewloq-uporp-tacou-lupdn-oae 5iihd-fkroy-ow5zp-hlvwz-bsgbl-mecta-kxubm-6adxr-ckcu6-prsus-fqe --motivation \"Removing nodes for redeployment. Link to the forum post: https://forum.dfinity.org/....\"\n</code></pre>"},{"location":"nns-proposals.html#replacing-and-optimizing-nodes-in-all-subnet-with-unhealthy-nodes","title":"Replacing and optimizing nodes in all subnet with unhealthy nodes","text":"<p>It is possible to run a single dre command to a) find all subnets with unhealthy nodes, b) subnet proposals for all subnets with unhealthy nodes, and attempt to replace a few additional nodes to improve optimization.</p> <p>The invocation is straighforward:</p> <pre><code>dre heal\n</code></pre> <p>Please pay close attention to the output of the command and if possible get someone else to double check the effect on decentralization before confirming the proposal submission.</p>"},{"location":"nns-proposals.html#freeing-up-removing-from-their-subnets-nodes-from-a-specific-dc","title":"Freeing up (removing from their subnets) nodes from a specific DC","text":"<p>To free up nodes from a specific data center (e.g., bo1) within subnets, follow these steps:</p> <p>First, open the public dashboard for the DC in question to find a list of subnets that include nodes from the data center. Or, alternatively, you can bypass the public dashboard by querying the registry directly:</p> <p><pre><code>dre registry | jq '.nodes[] | select((.dc_id == \"bo1\") and (.subnet_id != null))'\n</code></pre> Explanation: This command retrieves a dump of the registry, extracts the list of nodes, and selects only those that belong to the <code>bo1</code> data center and have a <code>subnet_id</code> set (i.e., not null).</p>"},{"location":"nns-proposals.html#dumping-raw-registry-versions-for-precise-diffs","title":"Dumping raw registry versions (for precise diffs)","text":"<p>Use the DRE CLI to dump raw registry versions, either a single version or a range, for exact inspection of changes:</p> <pre><code># One version\ndre registry --dump-version 12345 | jq\n\n# Range: Python-style indexing (end-exclusive). Indexing semantics:\n# - Positive indices are 1-based positions (registry is 1-based)\n# - 0 means start (same as omitting FROM)\n# - Negative indices count from end (-1 is last)\n# - Reversed ranges yield empty results\ndre registry --dump-versions -5 &gt; last5.json\n\n# All versions (large output)\ndre registry --dump-versions &gt; all.json\n</code></pre> <p>Each output element is a single registry record at a registry version with best-effort decoded value. Unknown byte blobs are shown compactly using base64 as <code>{ \"bytes_base64\": \"...\" }</code>. For short byte arrays the tool may also add a <code>principal</code> string.</p> Click here to see the output of the above command <pre><code>{\n  \"node_id\": \"4jtgm-ywxcc-xh3o3-x2omx-tgmdm-gobca-agb3a-alvw4-dhmyn-khis6-xae\",\n  \"xnet\": {\n    \"ip_addr\": \"2600:c0d:3002:4:6801:dfff:fee7:5ae8\",\n    \"port\": 2497\n  },\n  \"http\": {\n    \"ip_addr\": \"2600:c0d:3002:4:6801:dfff:fee7:5ae8\",\n    \"port\": 8080\n  },\n  \"node_operator_id\": \"ut325-qbq5v-fli2f-e2a5h-qapdd-fsuyv-xej2j-ogvux-i3fc2-5nj3a-2ae\",\n  \"chip_id\": null,\n  \"hostos_version_id\": \"2e269c77aa2f6b2353ddad6a4ac3d5ddcac196b1\",\n  \"public_ipv4_config\": null,\n  \"subnet_id\": \"nl6hn-ja4yw-wvmpy-3z2jx-ymc34-pisx3-3cp5z-3oj4a-qzzny-jbsv3-4qe\",\n  \"dc_id\": \"bo1\",\n  \"node_provider_id\": \"lq5ra-f4ibl-t7wpy-hennc-m4eb7-tnfxe-eorgd-onpsl-wervo-7chjj-6qe\",\n  \"status\": \"Healthy\"\n}\n{\n  \"node_id\": \"af7ti-auyik-jfsne-tljmz-6purg-2msmy-jw34z-b4ie3-abk5f-h23xt-zae\",\n  \"xnet\": {\n    \"ip_addr\": \"2600:c0d:3002:4:6801:19ff:fe8c:de47\",\n    \"port\": 2497\n  },\n  \"http\": {\n    \"ip_addr\": \"2600:c0d:3002:4:6801:19ff:fe8c:de47\",\n    \"port\": 8080\n  },\n  \"node_operator_id\": \"ut325-qbq5v-fli2f-e2a5h-qapdd-fsuyv-xej2j-ogvux-i3fc2-5nj3a-2ae\",\n  \"chip_id\": null,\n  \"hostos_version_id\": \"2e269c77aa2f6b2353ddad6a4ac3d5ddcac196b1\",\n  \"public_ipv4_config\": null,\n  \"subnet_id\": \"w4rem-dv5e3-widiz-wbpea-kbttk-mnzfm-tzrc7-svcj3-kbxyb-zamch-hqe\",\n  \"dc_id\": \"bo1\",\n  \"node_provider_id\": \"lq5ra-f4ibl-t7wpy-hennc-m4eb7-tnfxe-eorgd-onpsl-wervo-7chjj-6qe\",\n  \"status\": \"Healthy\"\n}\n{\n  \"node_id\": \"fd5e4-a2xzl-lxu7m-kjvn6-2arnt-jghro-rdrgx-zvvkd-j2hza-pbwl4-5qe\",\n  \"xnet\": {\n    \"ip_addr\": \"2600:c0d:3002:4:6801:94ff:fec9:6b\",\n    \"port\": 2497\n  },\n  \"http\": {\n    \"ip_addr\": \"2600:c0d:3002:4:6801:94ff:fec9:6b\",\n    \"port\": 8080\n  },\n  \"node_operator_id\": \"ut325-qbq5v-fli2f-e2a5h-qapdd-fsuyv-xej2j-ogvux-i3fc2-5nj3a-2ae\",\n  \"chip_id\": null,\n  \"hostos_version_id\": \"2e269c77aa2f6b2353ddad6a4ac3d5ddcac196b1\",\n  \"public_ipv4_config\": null,\n  \"subnet_id\": \"tdb26-jop6k-aogll-7ltgs-eruif-6kk7m-qpktf-gdiqx-mxtrf-vb5e6-eqe\",\n  \"dc_id\": \"bo1\",\n  \"node_provider_id\": \"lq5ra-f4ibl-t7wpy-hennc-m4eb7-tnfxe-eorgd-onpsl-wervo-7chjj-6qe\",\n  \"status\": \"Healthy\"\n}\n{\n  \"node_id\": \"q2ucv-x7dv5-hheao-ocsye-jbg4z-enm75-ss62d-ehqhj-zwwm3-cap5q-tqe\",\n  \"xnet\": {\n    \"ip_addr\": \"2600:c0d:3002:4:6801:bcff:fee7:4008\",\n    \"port\": 2497\n  },\n  \"http\": {\n    \"ip_addr\": \"2600:c0d:3002:4:6801:bcff:fee7:4008\",\n    \"port\": 8080\n  },\n  \"node_operator_id\": \"ut325-qbq5v-fli2f-e2a5h-qapdd-fsuyv-xej2j-ogvux-i3fc2-5nj3a-2ae\",\n  \"chip_id\": null,\n  \"hostos_version_id\": \"2e269c77aa2f6b2353ddad6a4ac3d5ddcac196b1\",\n  \"public_ipv4_config\": null,\n  \"subnet_id\": \"io67a-2jmkw-zup3h-snbwi-g6a5n-rm5dn-b6png-lvdpl-nqnto-yih6l-gqe\",\n  \"dc_id\": \"bo1\",\n  \"node_provider_id\": \"lq5ra-f4ibl-t7wpy-hennc-m4eb7-tnfxe-eorgd-onpsl-wervo-7chjj-6qe\",\n  \"status\": \"Healthy\"\n}\n</code></pre> <p>To further refine the command and group nodes by subnets: <pre><code>DC=bo1\ndre registry | jq -r '.nodes | map(select((.dc_id == \"'$DC'\") and (.subnet_id != null))) | group_by(.subnet_id) | map(\"dre subnet replace --exclude '$DC' --nodes \\([.[].node_id] | join(\" \"))\") | .[]'\n</code></pre></p> Click here to see the explanation of the jq command  1.  **`-r` (raw output):**      -   This option tells `jq` to output raw strings instead of JSON-formatted strings. This is particularly useful when you want to generate command-line strings like the one you're creating. 2.  **`.nodes`:**      -   This accesses the `nodes` array in the JSON structure. 3.  **`map(select((.dc_id == \"bo1\") and (.subnet_id != null)))`:**      -   This filters the nodes array. It keeps only the nodes where `dc_id` is `\"bo1\"` and `subnet_id` is not `null`. `map` applies this filter to each element of the array. 4.  **`group_by(.subnet_id)`:**      -   This groups the filtered nodes by their `subnet_id`. The result is an array of arrays, where each inner array contains nodes that share the same `subnet_id`. 5.  **`map(\"dre subnet replace --exclude '$DC' --nodes \\([.[].node_id] | join(\" \"))\")`:**      -   This `map` applies a transformation to each group (each inner array).     -   The transformation constructs a string that starts with `\"dre subnet replace --exclude '$DC' --nodes \"`.     -   `\\([.[].node_id] | join(\" \"))`:         -   `.[].node_id` accesses the `node_id` of each node in the current group.         -   `join(\" \")` joins all the `node_id`s into a single string separated by spaces.     -   The result is a string for each subnet that lists all its nodes. 6.  **`.[]`:**      -   This unwraps the array of strings created by the previous `map` step into individual strings. This means the output will be a list of commands, one per line.  Click here to see the example output <pre><code>dre subnet replace --exclude bo1 --nodes q2ucv-x7dv5-hheao-ocsye-jbg4z-enm75-ss62d-ehqhj-zwwm3-cap5q-tqe\ndre subnet replace --exclude bo1 --nodes 4jtgm-ywxcc-xh3o3-x2omx-tgmdm-gobca-agb3a-alvw4-dhmyn-khis6-xae\ndre subnet replace --exclude bo1 --nodes fd5e4-a2xzl-lxu7m-kjvn6-2arnt-jghro-rdrgx-zvvkd-j2hza-pbwl4-5qe\ndre subnet replace --exclude bo1 --nodes af7ti-auyik-jfsne-tljmz-6purg-2msmy-jw34z-b4ie3-abk5f-h23xt-zae\n</code></pre> <p>You would now go through each of these output lines (subnets) and submit proposals. Remember to include a motivation for each proposal. For example:</p> <pre><code>dre subnet replace --exclude bo1 --nodes q2ucv-x7dv5-hheao-ocsye-jbg4z-enm75-ss62d-ehqhj-zwwm3-cap5q-tqe --motivation \"Removing BO1 nodes for maintenance\"\n</code></pre>"},{"location":"nns-proposals.html#replacing-a-specific-node-in-a-subnet","title":"Replacing a Specific Node in a Subnet","text":"<p>The following step-by-step instructions describe how a node in a subnet can be replaced using the DRE tool while maintaining the same number of nodes in the subnet.</p>"},{"location":"nns-proposals.html#prerequisites","title":"Prerequisites","text":"<ul> <li>DRE tool installed and configured.</li> <li>Knowledge of the node IDs to be replaced.</li> </ul>"},{"location":"nns-proposals.html#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"nns-proposals.html#1-understand-the-command-structure","title":"1. Understand the Command Structure","text":"<p>The command to replace a node looks like this: <pre><code>\u276f dre subnet replace --nodes &lt;NODES_TO_REMOVE&gt; --motivation \"&lt;REPLACEMENT_REASON&gt;\"\n</code></pre></p>"},{"location":"nns-proposals.html#2-example-command","title":"2. Example Command","text":"<p>Below is an example command to replace a node in subnet <code>pjljw</code>: <pre><code>\u276f dre subnet replace \\\n  --nodes z6jp6-245uu-gh3cs-sblcy-f3jmj-s4ngl-v3z4u-lafz2-qudjr-6mbqx-vqe \\\n  --motivation \"Requested by the node operator in order to redeploy all nodes in the DC after 48 months, and switch to a new node operator ID.\"\n</code></pre></p> <p>Note that it's possible to provide multiple node ids in the command above, separated by spaces, as long as they are all in the same subnet.</p>"},{"location":"nns-proposals.html#3-execute-the-command","title":"3. Execute the Command","text":"<p>Run the command to propose the node replacement. The tool will perform a series of checks and outputs similar to the following:</p> <pre><code>2024-12-27T17:33:31.470Z INFO  dre &gt; Running version 0.5.7-9c513fc1\n2024-12-27T17:33:31.541Z INFO  dre::store &gt; Using local registry path for network mainnet: /home/user/.cache/dre-store/local_registry/mainnet\n...\n2024-12-27T17:33:54.327Z INFO  dre::ic_admin &gt; running ic-admin:\n...\n</code></pre>"},{"location":"nns-proposals.html#4-verify-the-proposal-details","title":"4. Verify the Proposal Details","text":"<p>Review the details provided by the tool for: - Nodes to be removed - Nodes to be added - Impact on decentralization metrics</p>"},{"location":"nns-proposals.html#5-confirm-the-replacement","title":"5. Confirm the Replacement","text":"<p>If the proposal details look correct, confirm the action when prompted: <pre><code>Do you want to continue? yes\n</code></pre></p>"},{"location":"nns-proposals.html#6-post-execution-verification","title":"6. Post-Execution Verification","text":"<p>To get the proposal adopted and executed: - Please answer questions in the forum if there are any for the proposal.</p>"},{"location":"nns-proposals.html#7-post-execution-verification","title":"7. Post-Execution Verification","text":"<p>After the replacement process is complete: - Verify that the new node is active and healthy in the subnet.</p>"},{"location":"nns-proposals.html#example-output","title":"Example Output","text":"<p>The tool provides a detailed summary of the decentralization impact, as shown below:</p> <pre><code>Decentralization Nakamoto coefficient changes for subnet `pjljw`:\n\n    node_provider: 5.00 -&gt; 5.00    (+0%)\n      data_center: 5.00 -&gt; 5.00    (+0%)\ndata_center_owner: 5.00 -&gt; 5.00    (+0%)\n             area: 5.00 -&gt; 5.00    (+0%)\n          country: 4.00 -&gt; 4.00    (+0%)\n</code></pre>"},{"location":"nns-proposals.html#notes","title":"Notes","text":"<ul> <li>Always verify the decentralization impact to ensure the subnet remains balanced.</li> <li>If any issues arise, consult the DRE documentation or reach out to the appropriate support channels.</li> </ul>"},{"location":"nns-proposals.html#additional-resources","title":"Additional Resources","text":"<ul> <li>DRE GitHub Repository</li> <li>Dfinity Forum</li> </ul>"},{"location":"nns-proposals.html#removing-nodes-from-the-registry","title":"Removing nodes from the registry","text":"<p>Here is an example where we remove all AW1 nodes from the registry, for redeployment. Note that the nodes should already be removed from the subnet(s), so they should in unassigned state (awaiting subnet). You can get the node status by checking the public dashboard or with <code>dre registry</code> and then analyzing the generated JSON from the command.</p> <p>After all relevant nodes are removed from their subnets, you can also remove them from the registry with:</p> <pre><code>dre nodes remove aw1 --motivation \"Removing AW1 nodes for redeployment\"\n</code></pre>"},{"location":"node-rewards.html","title":"Node Rewards Command","text":"<p>The <code>node-rewards</code> command allows node providers to check and analyze their rewards by querying the Node Rewards Canister (NRC) and comparing them with governance rewards from the NNS. This tool provides comprehensive insights into daily rewards, performance metrics, and helps identify discrepancies between calculated rewards and actual governance payouts.</p>"},{"location":"node-rewards.html#overview","title":"Overview","text":"<p>The <code>node-rewards</code> command fetches reward data from two sources:</p> <ol> <li>Node Rewards Canister (NRC): Calculates rewards based on daily node performance metrics</li> <li>Governance (NNS): Actual rewards paid out through governance snapshots</li> </ol> <p>By comparing these two sources, node providers can: - Verify that their rewards match expectations - Identify any discrepancies between calculated and actual rewards - Monitor daily performance and identify underperforming nodes - Export detailed CSV reports for further analysis</p>"},{"location":"node-rewards.html#command-structure","title":"Command Structure","text":"<p>The command has two modes:</p> <pre><code>dre node-rewards &lt;mode&gt; [options]\n</code></pre>"},{"location":"node-rewards.html#modes","title":"Modes","text":"<ol> <li><code>ongoing</code>: Shows rewards from the latest governance snapshot timestamp to yesterday</li> <li><code>past-rewards &lt;month&gt;</code>: Shows past rewards for a specific month (format: <code>YYYY-MM</code>) and compares with governance</li> </ol>"},{"location":"node-rewards.html#options","title":"Options","text":"<p>Both modes support the following options:</p> <ul> <li><code>--csv-detailed-output-path &lt;path&gt;</code>: If set, writes detailed CSV files to the specified directory</li> <li><code>--provider-id &lt;id&gt;</code>: Filter to a single provider (full principal ID or provider prefix)</li> </ul>"},{"location":"node-rewards.html#usage-examples","title":"Usage Examples","text":""},{"location":"node-rewards.html#check-ongoing-rewards","title":"Check Ongoing Rewards","text":"<p>View rewards from the latest governance snapshot to yesterday:</p> <pre><code>dre node-rewards ongoing\n</code></pre> <p>This will display a daily rewards summary table showing: - Daily base and adjusted rewards totals - Number of nodes and assigned nodes - Underperforming nodes (nodes with performance multiplier &lt; 1.0) - Adjusted rewards percentage</p>"},{"location":"node-rewards.html#check-past-rewards-for-a-specific-month","title":"Check Past Rewards for a Specific Month","text":"<p>View and compare rewards for a past month (e.g., October 2024):</p> <pre><code>dre node-rewards past-rewards 2024-10\n</code></pre> <p>This mode provides: - Daily rewards summary for the specified month - Comparison table showing NRC vs Governance rewards - Difference and percentage difference calculations - List of underperforming nodes</p>"},{"location":"node-rewards.html#filter-by-provider-id","title":"Filter by Provider ID","text":"<p>Check rewards for a specific provider using their full principal ID or prefix:</p> <pre><code>dre node-rewards ongoing --provider-id tm3pc-2bjsx-hhv3v-fsrt7-wotdj-nbu3t-ewloq-uporp-tacou-lupdn-oae\n</code></pre> <p>Or using just the prefix:</p> <pre><code>dre node-rewards ongoing --provider-id tm3pc\n</code></pre>"},{"location":"node-rewards.html#export-detailed-csv-reports","title":"Export Detailed CSV Reports","text":"<p>Generate comprehensive CSV files for further analysis:</p> <pre><code>dre node-rewards ongoing --csv-detailed-output-path ./rewards_export\n</code></pre> <p>This creates a directory structure like:</p> <pre><code>rewards_export/\n\u2514\u2500\u2500 rewards_2024-10-01_to_2024-10-31/\n    \u251c\u2500\u2500 &lt;provider_id&gt;/\n    \u2502   \u251c\u2500\u2500 base_rewards.csv\n    \u2502   \u251c\u2500\u2500 base_rewards_type3.csv\n    \u2502   \u251c\u2500\u2500 rewards_summary.csv\n    \u2502   \u251c\u2500\u2500 node_metrics_by_day.csv\n    \u2502   \u2514\u2500\u2500 node_metrics_by_node.csv\n    \u2514\u2500\u2500 subnets_failure_rates.csv\n</code></pre>"},{"location":"node-rewards.html#complete-example-export-past-month-with-provider-filter","title":"Complete Example: Export Past Month with Provider Filter","text":"<pre><code>dre node-rewards past-rewards 2024-10 \\\n    --provider-id tm3pc \\\n    --csv-detailed-output-path ./october_rewards\n</code></pre>"},{"location":"node-rewards.html#understanding-the-output","title":"Understanding the Output","text":""},{"location":"node-rewards.html#daily-rewards-summary","title":"Daily Rewards Summary","text":"<p>When viewing rewards in console mode (without CSV export), you'll see a table for each provider showing:</p> Column Description Day UTC day (YYYY-MM-DD) Base Rewards Total Sum of <code>base_rewards_xdr_permyriad</code> across all nodes (XDRPermyriad) Adjusted Rewards Total Sum of <code>adjusted_rewards_xdr_permyriad</code> across all nodes (XDRPermyriad) Adjusted Rewards % (Adjusted Rewards Total / Base Rewards Total) \u00d7 100% Nodes Total nodes found in registry on that day Assigned Number of nodes assigned to a subnet on that day Underperf Number of nodes with performance multiplier &lt; 1.0 Underperf Nodes Comma-separated list of underperforming node IDs (prefixes)"},{"location":"node-rewards.html#comparison-table-past-rewards-mode","title":"Comparison Table (Past Rewards Mode)","text":"<p>When using <code>past-rewards</code> mode, an additional comparison table is displayed:</p> Column Description Provider Provider ID prefix NRC Total rewards from Node Rewards Canister (XDRPermyriad) Governance Total rewards from NNS governance (XDRPermyriad) Difference NRC - Governance (XDRPermyriad) % Diff (Difference / NRC) \u00d7 100% Underperforming Nodes List of node prefixes with performance issues <p>Understanding Rewards Units</p> <p>All rewards are displayed in XDRPermyriad (XDR per ten-thousand). This is a standardized unit used for reward calculations on the IC.</p> <p>Performance Multiplier</p> <p>Nodes with a performance multiplier &lt; 1.0 are considered underperforming. This multiplier affects the adjusted rewards based on the node's actual performance metrics.</p>"},{"location":"node-rewards.html#csv-export-files","title":"CSV Export Files","text":"<p>When using <code>--csv-detailed-output-path</code>, the following CSV files are generated:</p>"},{"location":"node-rewards.html#per-provider-files","title":"Per-Provider Files","text":"<p>Each provider gets a directory with these files:</p>"},{"location":"node-rewards.html#base_rewardscsv","title":"<code>base_rewards.csv</code>","text":"<p>Contains base reward calculations by reward type and region: - <code>day_utc</code>: Date in UTC - <code>monthly_xdr_permyriad</code>: Monthly base reward - <code>daily_xdr_permyriad</code>: Daily base reward - <code>node_reward_type</code>: Type of reward (e.g., Type1, Type2, Type3) - <code>region</code>: Geographic region</p>"},{"location":"node-rewards.html#base_rewards_type3csv","title":"<code>base_rewards_type3.csv</code>","text":"<p>Contains Type3 reward calculations (region-based rewards): - <code>day_utc</code>: Date in UTC - <code>value_xdr_permyriad</code>: Daily reward value - <code>region</code>: Geographic region - <code>nodes_count</code>: Number of nodes in this region - <code>avg_rewards_xdr_permyriad</code>: Average reward per node - <code>avg_coefficient</code>: Average coefficient applied</p>"},{"location":"node-rewards.html#rewards_summarycsv","title":"<code>rewards_summary.csv</code>","text":"<p>Daily summary of rewards per provider: - <code>day_utc</code>: Date in UTC - <code>base_rewards_total</code>: Total base rewards for the day - <code>adjusted_rewards_total</code>: Total adjusted rewards after performance multipliers - <code>adjusted_rewards_percent</code>: Percentage of base rewards received - <code>rewards_total_xdr_permyriad</code>: Total rewards for the day - <code>nodes_in_registry</code>: Total nodes in registry - <code>assigned_nodes</code>: Number of nodes assigned to subnets - <code>underperforming_nodes_count</code>: Count of underperforming nodes - <code>underperforming_nodes</code>: Comma-separated list of node IDs</p>"},{"location":"node-rewards.html#node_metrics_by_daycsv","title":"<code>node_metrics_by_day.csv</code>","text":"<p>Node-level metrics organized by day: - <code>day_utc</code>: Date in UTC - <code>node_id</code>: Full node principal ID - <code>node_reward_type</code>: Reward type for the node - <code>region</code>: Geographic region - <code>dc_id</code>: Datacenter ID - <code>node_status</code>: Assigned, Unassigned, or Unknown - <code>performance_multiplier</code>: Performance multiplier (1.0 = full performance) - <code>rewards_reduction</code>: Reduction applied to rewards - <code>base_rewards_xdr_permyriad</code>: Base rewards for this node - <code>adjusted_rewards_xdr_permyriad</code>: Adjusted rewards after multiplier - <code>subnet_assigned</code>: Subnet ID if assigned - <code>subnet_assigned_failure_rate</code>: Failure rate of the subnet - <code>num_blocks_proposed</code>: Number of blocks proposed - <code>num_blocks_failed</code>: Number of blocks that failed - <code>original_failure_rate</code>: Original failure rate - <code>relative_failure_rate</code>: Relative failure rate - <code>extrapolated_failure_rate</code>: Extrapolated failure rate (for unassigned nodes)</p>"},{"location":"node-rewards.html#node_metrics_by_nodecsv","title":"<code>node_metrics_by_node.csv</code>","text":"<p>Same data as <code>node_metrics_by_day.csv</code>, but organized by node ID (all days for a node are grouped together).</p>"},{"location":"node-rewards.html#global-files","title":"Global Files","text":""},{"location":"node-rewards.html#subnets_failure_ratescsv","title":"<code>subnets_failure_rates.csv</code>","text":"<p>Subnet-level failure rate data: - <code>subnet_id</code>: Subnet principal ID - <code>day_utc</code>: Date in UTC - <code>failure_rate</code>: Failure rate for the subnet on that day</p>"},{"location":"node-rewards.html#common-use-cases","title":"Common Use Cases","text":""},{"location":"node-rewards.html#1-monthly-reward-verification","title":"1. Monthly Reward Verification","text":"<p>At the end of each month, verify that your rewards match expectations:</p> <pre><code>dre node-rewards past-rewards 2024-10 --provider-id &lt;your-provider-id&gt;\n</code></pre> <p>Review the comparison table to ensure NRC and Governance rewards match. If there's a significant difference, investigate the causes (e.g., underperforming nodes, timing differences).</p>"},{"location":"node-rewards.html#2-daily-performance-monitoring","title":"2. Daily Performance Monitoring","text":"<p>Check ongoing rewards to monitor daily performance:</p> <pre><code>dre node-rewards ongoing --provider-id &lt;your-provider-id&gt;\n</code></pre> <p>Look for: - Sudden drops in adjusted rewards percentage - New underperforming nodes - Changes in assigned node count</p>"},{"location":"node-rewards.html#3-export-for-analysis","title":"3. Export for Analysis","text":"<p>Generate CSV files for detailed analysis in spreadsheet tools:</p> <pre><code>dre node-rewards past-rewards 2024-10 \\\n    --provider-id &lt;your-provider-id&gt; \\\n    --csv-detailed-output-path ./reports\n</code></pre> <p>Then analyze: - Trends in <code>node_metrics_by_node.csv</code> to identify problematic nodes - Performance patterns in <code>rewards_summary.csv</code> - Subnet failure rates in <code>subnets_failure_rates.csv</code></p>"},{"location":"node-rewards.html#4-identifying-underperforming-nodes","title":"4. Identifying Underperforming Nodes","text":"<p>The command automatically identifies nodes with performance multiplier &lt; 1.0. Review the \"Underperf Nodes\" column to see which nodes need attention:</p> <pre><code>dre node-rewards ongoing --provider-id &lt;your-provider-id&gt;\n</code></pre> <p>Common causes for underperformance: - High failure rates - Network issues - Subnet-specific problems</p>"},{"location":"registry-versions.html","title":"Registry Versions Dump (dre registry)","text":"<p>This document describes how to use the DRE CLI to inspect Internet Computer Protocol (ICP) registry versions as raw records, in JSON, suitable for precise diffs and troubleshooting.</p>"},{"location":"registry-versions.html#commands","title":"Commands","text":"<ul> <li>Dump a single version 50000 (flat list of records):</li> </ul> <pre><code>dre registry --dump-versions 50000 50001 | jq\n</code></pre> <ul> <li>Dump a range of versions using Python-style indexing (end-exclusive), where -1 is the last index and omitted end means \"to the end\":</li> </ul> <pre><code>dre registry --dump-versions -5 &gt; last5.json\n# Indexing semantics (Python slicing, end-exclusive):\n#   - positive indices are 1-based positions (registry is 1-based)\n#   - 0 means start (same as omitting FROM)\n#   - negative indices count from end (-1 is last)\n#   - reversed ranges yield empty results\n# Examples:\n#   -5      -&gt; last 5\n#   -5 -1   -&gt; last 4 (excludes the very last)\n#   -1      -&gt; last 1\n#   0       -&gt; all (from record 0 to the end)\n</code></pre> <ul> <li>Dump ALL versions (warning: large):</li> </ul> <pre><code>dre registry --dump-versions &gt; all.json\n</code></pre>"},{"location":"registry-versions.html#output-shape","title":"Output Shape","text":"<p>Output is a flat JSON array of objects. Each object corresponds to a single registry record at a specific version:</p> <pre><code>  {\n    \"version\": 50000,\n    \"key\": \"node_record_cekdc-hmzri-3u3or-ei7ip-su7ck-3xt6e-zsse2-tgakq-rolmv-6crkh-hqe\",\n    \"value\": {\n      \"xnet\": {\n        \"ip_addr\": \"2401:7500:ff1:20:6801:8fff:fe0c:cff4\",\n        \"port\": 2497\n      },\n      \"http\": {\n        \"ip_addr\": \"2401:7500:ff1:20:6801:8fff:fe0c:cff4\",\n        \"port\": 8080\n      },\n      \"node_operator_id\": {\n        \"bytes_base64\": \"K0aH3L0TlIkJORV0I4GwVU2GMLDZAv5U8Av1kAI=\",\n        \"principal\": \"ri4lg-drli2-d5zpi-tsseq-soivo-qrydm-cvjwd-dbmgz-al7fj-4al6w-iae\"\n      },\n      \"chip_id\": null,\n      \"hostos_version_id\": \"68fc31a141b25f842f078c600168d8211339f422\",\n      \"public_ipv4_config\": null,\n      \"domain\": null,\n      \"node_reward_type\": 5\n    }\n  },\n[...]\n</code></pre>"},{"location":"release.html","title":"Release controller","text":"<p>Automates the process of proposing new releases for IC HostOS and GuestOS.</p>"},{"location":"release.html#usage","title":"Usage","text":"<ol> <li>Register new release / version in release-index.yaml <pre><code>releases:\n  - rc_name: rc--2024-02-21_23-01\n    versions:\n      # It is customary but not mandatory to add a link to the\n      # Qualification pipeline:\n      # https://github.com/dfinity/ic/actions/runs/14491317106\n      - name: base\n        version: 2e921c9adfc71f3edc96a9eb5d85fc742e7d8a9f\n</code></pre></li> <li>Relevant teams are notified with a link to a Google document for them to review the release notes.  In parallel, placeholder post is created in the forum to prepare for publication of the release notes.</li> <li>Once the Google document is reviewed (all teams crossed out), PR will be created with release notes.</li> <li>Once that PR is merged, the proposal will be placed and the placeholder forum post is updated with the final release notes.</li> <li>Once trusted neurons have voted to adopt the proposal, the adopted release can be rolled out (beyond the scope of release controller).</li> </ol>"},{"location":"release.html#release-index-reference","title":"Release index reference","text":"<p>Releases are composed of a list of dictionaries, each having (1) an <code>rc_name</code> corresponding to the RC branch to be released, and (2) a list of versions each containing a <code>name</code> (at least one of which is typically named <code>base</code> and corresponds to the first listed version) and a <code>version</code> containing the commit ID desired to be tagged and released; two additional version fields <code>changelog_base</code> and <code>security_fix</code> are documented below.</p> <p>Out of each version within a release, a release branch named <code>{rc_name}-{version.name}</code> will be constructed to create a specific release for GuestOS and (in the case of the <code>base</code> or first version of all releases) HostOS.  There is currently no way to force a feature / non-base version of a release to turn into a proposed HostOS release.</p> <p>Only the two most recent releases will be paid attention to by the release controller.</p> <p>The release notes (changelog) for each release version is generated automatically, starting from a prior version which is typically determined automatically.  In the case of any base version of a release, the prior base release is considered the baseline for the release notes; in the case of a non-base / feature version, the base version the same release is considered the baseline.</p> <p>You can override this behavior; a version can have an additional <code>changelog_base</code> dictionary with (optional) keys <code>GuestOS</code> and/or <code>HostOS</code>, whose values must be the name of another release (<code>rc_name</code>) listed in the index, as well as the name of one of its versions (typically <code>base</code>).  This dictionary allows you to override which release/version combo is used as the baseline for (the start of) the release notes that will be generated for this OS and version combination.  Here is an example:</p> <pre><code>releases:\n  - rc_name: rc--2025-05-23_03-21\n    versions:\n      - name: base\n        version: 16825c5cbff83a51983d849b60c9d26b3268bbb6\n        changelog_base:\n          # Base the changelog for GuestOS at this version onto the May 1st base release.\n          # Due to absence of HostOS key, use the normal HostOS baseline detection mechanism\n          # for its changelog.\n          GuestOS:\n            rc_name: rc--2025-05-01_03-23\n            name: base\n  - rc_name: rc--2025-05-15_03-20\n    versions:\n      - name: base\n        version: 59ad18a77fbeaf3ebbba863972ff20f7ab588d7a\n  - rc_name: rc--2025-05-01_03-23\n    versions:\n      - name: base\n        version: f195ba756bc3bf170a2888699e5e74101fdac6ba\n</code></pre> <p>Finally, changelog generation can be suppressed entirely by adding <code>security_fix: true</code> to a version.  This creates an abridged release notes containing no changes at all, and indicating to the users that the code plus the changes will be available at a later date.  Use this flag when a release must be performed from the private security-fixes-only repository, as otherwise the changelog code will not work.</p> <pre><code>releases:\n  - rc_name: rc--2025-05-23_03-21\n    versions:\n      - name: base\n        version: 16825c5cbff83a51983d849b60c9d26b3268bbb6\n        security_fix: true\n</code></pre> <p>You are allowed to periodically clean up old releases from the release index, so long as the commit IDs they list are not in the blessed versions for HostOS or GuestOS, and none of the remaining releases refer to them via the <code>changelog_base</code> field.  As a rule of thumb, it will not cause problems to delete releases six months or older.</p>"},{"location":"release.html#recreating-notes","title":"Recreating notes","text":"<p>Sometimes you'd want to recreate notes, either because a bug occured on the first generation, or you just want to have updated version of the notes submitted.</p>"},{"location":"release.html#recreate-google-doc","title":"Recreate Google Doc","text":"<p>To recreate Google Doc, remove the document from Google Drive directory or rename it such that it doesn't include any release details.</p>"},{"location":"release.html#recreate-github-pr-with-release-notes","title":"Recreate GitHub PR with release notes","text":"<p>To recreate GitHub PR, close the outstanding PR and make sure to delete the branch of the PR.</p>"},{"location":"release.html#in-production","title":"In production","text":"<p>Several credentials are necessary.  For the reconciler:</p> <ol> <li>The Google Drive credentials.  These are stored in the DRE Team vault and named FIT on-call schedule sync and release controller Google Drive credential.  Saved to a file, the path to this file must be set in environment variable <code>GDOCS_CREDENTIALS_PATH</code>.</li> <li><code>PROPOSER_NEURON_ID</code> should be set to the neuron ID of the proposer, and the proposer key material should be saved to a file (asn the DRE team for information), whose path should be added to environment variable <code>PROPOSER_KEY_FILE</code>.</li> <li>The <code>GITHUB_TOKEN</code> environment variable must be set to the token that has access to the IC and DRE repositories.  This secret resides in the DRE Team vault under the name Release Controller GitHub API Key.  In the reconciler, this token is used to push tags.</li> </ol> <p>The commit annotator only needs the <code>GITHUB_TOKEN</code> credentials.  This token is used to push notes.</p>"},{"location":"release.html#contributing","title":"Contributing","text":"<p>The project is split into two parts - commit annotator and reconciler.</p> <p>If you want fix a bug, or add a feature, please consider writing a test.</p>"},{"location":"release.html#commit-annotator","title":"commit annotator","text":"<p>This simple service checks out each commit on <code>master</code> and <code>rc-*</code> branches of IC repo and runs target-determinator to identify whether GuestOS build changed as a result of changes in that commit. This information is then pushed to git notes and later used by reconciler.</p> <p>The annotator has a bonus mode to manually annotate failing commits.  See below for more info.</p>"},{"location":"release.html#reconciler","title":"reconciler","text":"<p>Reconciler is responsible for:</p> <ol> <li> <p>generating a draft of the release notes   Done by release_notes.py. See Generate release notes locally in this document for manual release notes generation.</p> </li> <li> <p>making a forum post with the release notes draft   The draft release notes generated in step (1) are published as a post to a Discourse thread (which will be created if necessary).</p> </li> <li> <p>Google Docs publish   Done by google_docs.py. The draft of the release notes is published to Google Docs for internal engineering review.</p> </li> <li> <p>creating a GitHub PR to publish notes   Done by publish_notes.py once the notes are ready for review according to the content of the respective Google Doc. It's not recommended to run this manually. Instead, if you have an issue, try to create a unit test to resolve the issue. You can download the Google Doc you're having problems with to use it in your test. See tests that use <code>release-controller/test_data/b0ade55f7e8999e2842fe3f49df163ba224b71a2.docx</code>.</p> </li> <li> <p>placing the proposal for electing a version   Done by dre_cli.py / reconciler.py. There should be a logs for the command that was run if you want to debug any issues with it.</p> </li> <li> <p>forum post update   Done by forum.py. You can run the program manually to debug issues. Change the <code>main()</code> function to your needs.</p> </li> </ol> <p>It's important to note that forum logic depends on finding alredy created blog posts by querying posts from authenticated user (@DRETeam). For those reasons, it won't be able to find manually created posts by other users.</p>"},{"location":"release.html#resolving-issues","title":"Resolving issues","text":""},{"location":"release.html#diagnostics","title":"Diagnostics","text":"<p>The release controller has its own dashboard. Use the dashboard to supervise the progress of the components that comprise the release controller.</p>"},{"location":"release.html#google-docs-generation-was-wrong-for-particular-commit","title":"Google Docs generation was wrong for particular commit","text":"<p>This could happen if release-index.yaml was wrongly configured or if there's a major bug that needs to be adressed before regenerating the notes again.</p>"},{"location":"release.html#resolution","title":"Resolution","text":"<ol> <li>To cause the reconciler to regenerate release notes: move the document outside of the Google Drive folder. Renaming it to something meaningless (e.g. to-delete-12-09-24) should also do the trick.</li> <li>To fix the code in the commit annotator: manually generate the release notes (see below) on your computer and make changes to the queries or the code until the notes look as expected.</li> </ol>"},{"location":"release.html#release-notes-are-not-yet-ready-for-a-long-time","title":"Release notes are not yet ready for a long time","text":"<p>This is caused by one or more missing GuestOS / HostOS annotations.  Release controller is stuck generating release notes because it's missing a note for some commit.</p>"},{"location":"release.html#diagnostics_1","title":"Diagnostics","text":"<p>Verify that the annotator has completed and is not stuck on any branch (use the dashboard listed above).</p> <p>If <code>target-determinator</code> is crashing as the annotator executes it, here is how you find the failing commit causing the crash:</p> <ol> <li>Click on the Custom query square on the title of the Combined annotater and reconciler logs pane on the dashboard.</li> <li>Search for <code>annotate_object</code> -- the most recent occurrence will have the failing commit ID.</li> </ol>"},{"location":"release.html#resolution_1","title":"Resolution","text":"<p>If the problem is that the annotator keeps crashing because a commit is not buildable, you can manually annotate that commit and that will cause the annotator to skip it.  There is an example below on how to manually annotate a failing commit.</p> <p>You may have to annotate all commits not annotated prior to the failing commit as well (although that should not be necessary because the annotator generally annotates from oldest to newest commit, so all older commits should already be annotated).</p> <p>[!TIP] If someone messed up and labeled commits in between, commit annotator might report that it labeled everything when it did not, and reconciler may never be ready with the release notes. Run the below commands on IC repo to find gaps where there are commits without labels.</p> <pre><code>git fetch origin 'refs/notes/*:refs/notes/*' -f --prune\ngit log --format='%H' --no-merges $BASE_COMMIT..$RELEASE_COMMIT | xargs -L1 -I_commit bash -c \"echo -n '_commit '; git notes --ref guestos-changed show _commit | cat\"\n# substitute guestos-changed with hostos-changed to detect gaps in HostOS annotations.\n# substitute guestos-changed with guestos-targets to see targets and target-determinator output for that commit's annotation work..\n</code></pre>"},{"location":"release.html#missing-proposal","title":"Missing proposal","text":"<p>Proposal placement most likely failed.</p>"},{"location":"release.html#evaluation","title":"Evaluation","text":"<p>release-controller should have a warning message something like this:</p> <pre><code>\"version 99ab7f03700ba6cf832eb18ffd55228f56ae927a: earlier proposal submission attempted but most likely failed\"\n</code></pre> <p>Make sure also that few minutes have passed and that public dashboard still doesn't list the proposal.  Sometimes it takes a minute or two.</p> <p>If the proposal was indeed submitted, you don't have to do anything -- the reconciler will notice and continue normally.</p>"},{"location":"release.html#resolution_2","title":"Resolution","text":"<p>[!WARNING] Should resolve by itself in newer versions</p> <ol> <li>Top up the release-controller neuron if needed</li> <li>Execute into the pod   <pre><code>kubectl -n release-controller exec -it deployment/release-controller -- bash\n</code></pre></li> <li>Delete the state   <pre><code>rm /state/&lt;full_commit_hash&gt;\n</code></pre></li> </ol>"},{"location":"release.html#development","title":"Development","text":"<p>Please see the parent folder's <code>README.md</code> for virtual environment setup. Follow the whole Contributing section to the letter.</p>"},{"location":"release.html#running-the-reconciler-in-dry-run-mode","title":"Running the reconciler in dry-run mode","text":"<pre><code>bazel run //release-controller:release-controller -- --dry-run --verbose\n</code></pre> <p>No credentials of any kind are required by this mode.  By default everything the reconciler does in this mode has no outward effect.</p> <p>All the operations it executes are volatile as well.</p> <p>If you want the release notes this mock mode stores to be persisted in a folder so they are not regenerated on every run:</p> <pre><code>export RECONCILER_DRY_RUN_RELEASE_NOTES_STORAGE=/tmp/dryrun/relnotes\nbazel run //release-controller:release-controller \\\n  --action_env=RECONCILER_DRY_RUN_RELEASE_NOTES_STORAGE \\\n  -- --dry-run --verbose\n</code></pre> <p>If you want the mock forum interactions to be remembered between runs:</p> <pre><code>export RECONCILER_DRY_RUN_FORUM_STORAGE=/tmp/dryrun/forum\nbazel run //release-controller:release-controller \\\n  --action_env=RECONCILER_DRY_RUN_FORUM_STORAGE \\\n  -- --dry-run --verbose\n</code></pre> <p>Typing errors preventing you from running it, because you are editing code and testing your changes?  Add <code>--output_groups=-mypy</code> right after <code>bazel run</code>.</p> <p>The optional argument <code>--skip-preloading-state</code> makes it so that the reconciler will not preload its list of known proposals by version from the governance canister.  It is useful (in conjunction with an empty reconciler state folder) to make the reconciler do all the work of submitting proposals again.  It should only be used alongside <code>--dry-run</code>, to avoid submitting proposals twice.</p>"},{"location":"release.html#running-the-reconciler-in-the-container-it-ships","title":"Running the reconciler in the container it ships","text":"<p>You can load the reconciler into your local podman or docker system:</p> <pre><code>bazel run //release-controller:oci_image_load\n</code></pre> <p>This will spit out a SHA256 sum, which is the name of the container image just built and imported into your containerization system.  Run it as follows:</p> <pre><code>SHASUM=...\npodman run --rm -it --entrypoint=/release-controller/release-controller $SHASUM\n</code></pre> <p>Or, in short:</p> <pre><code>mkdir -p -m 0777 /tmp/git\npodman run --rm -it \\\n  -v /tmp/git:/root/.cache \\\n  --entrypoint /release-controller/release-controller \\\n  $(bazel run --verbose_failures //release-controller:oci_image_load | tail -1 | cut -d : -f 3)\n</code></pre>"},{"location":"release.html#running-the-annotator-locally-in-dry-run-mode","title":"Running the annotator locally in \"dry-run mode\"","text":"<p>The annotator can be run in a mostly stateless mode, for one single loop, with the following options:</p> <pre><code>bazel run //release-controller:commit-annotator \\\n  -- \\\n  --no-push-annotations \\\n  --loop-every=0 \\\n  --no-fetch-annotations \\ # don't clobber locally created annotations \n  --verbose\n</code></pre> <p>The annotator can also be run as a podman container, with a similar technique as above.  However, the annotator requires <code>--user $UID</code> because Bazel will not run as root (UID 0).</p> <p>Please consult <code>--help</code> for additional options.</p>"},{"location":"release.html#manually-annotate-a-troublesome-commit","title":"Manually annotate a troublesome commit","text":"<pre><code>export GITHUB_TOKEN=&lt;any Github token with push access to the IC repo&gt;\nCOMMIT_TO_ANNOTATE=9da8cc52d3d576410174bb28d629862f05a635e0\nAFFECTS_OS=yes\nWHICH_OS=HostOS # or GuestOS, or leave out --os-kind for all OSes\nbazel run //release-controller:commit-annotator \\\n  -- \\\n  manually-annotate \\\n  $COMMIT_TO_ANNOTATE $AFFECTS_OS --os-kind $WHICH_OS\n</code></pre>"},{"location":"release.html#generate-release-notes-locally","title":"Generate release notes locally","text":"<p>Release notes can be generated locally using several approaches:</p>"},{"location":"release.html#method-1-using-bazel-with-specific-rc-names","title":"Method 1: Using Bazel with specific RC names","text":"<pre><code>PREV_RC=rc--2025-03-27_03-14-base\nPREV_COMMIT=3ae3649a2366aaca83404b692fc58e4c6e604a25\nCURR_RC=rc--2025-04-03_03-15\nCURR_COMMIT=68fc31a141b25f842f078c600168d8211339f422\nbazel run //release-controller:release-notes -- $PREV_RC $PREV_COMMIT $CURR_RC $CURR_COMMIT --verbose --commit-annotator=local\n</code></pre>"},{"location":"release.html#method-2-using-bazel-with-generic-names-and-local-commit-annotator","title":"Method 2: Using Bazel with generic names and local commit annotator","text":"<pre><code>bazel run //release-controller:release-notes -- prev bf0d4d1b8cb6c0c19a5afa1454ada014847aa5c6 curr 07c01746ee3fa7700eb0eb781a7c26a53f989b1a --commit-annotator=local\n</code></pre>"},{"location":"release.html#method-3-using-rye-python-environment","title":"Method 3: Using Rye (Python environment)","text":"<p>First, ensure your Python environment is set up:</p> <pre><code>rye sync\n</code></pre> <p>Then run the release notes script directly:</p> <pre><code>rye run python3 release-controller/release_notes.py prev bf0d4d1b8cb6c0c19a5afa1454ada014847aa5c6 curr 07c01746ee3fa7700eb0eb781a7c26a53f989b1a --commit-annotator=local\n</code></pre>"},{"location":"release.html#commit-annotator-options","title":"Commit Annotator Options","text":"<p>The form of the command above requires you to run a commit annotator in parallel.  If you want to use the internal commit annotator that does not need a commit annotator running in parallel, add option <code>--commit-annotator-url local</code> instead.  If you want to recalculate the commit annotations instead of using cached ones, you can use option <code>--commit-annotator-url recreate</code>.  This last option is useful when testing the effects of changes made to the commit annotator code or Bazel query formulas the annotator uses.</p> <p>A great tip / trick to diagnose exactly what the release notes and commit annotation processes would do is to pick a commit from the IC repo, figure out which its parent commit is, then run:</p> <pre><code>PREV_RC=prev\nPREV_COMMIT=1354f31c9cd4fb6b4a65ab64eb9ac4a0a4d16839 # parent commit\nCURR_RC=curr\nCURR_COMMIT=f8131bfbc2d339716a9cff06e04de49a68e5a80b # commit\nbazel run //release-controller:release-notes -- \\\n   $PREV_RC $PREV_COMMIT $CURR_RC $CURR_COMMIT \\\n   --commit-annotator-url recreate \\\n  --os-kind=GuestOS \\\n  --verbose\nbazel run //release-controller:release-notes -- \\\n   $PREV_RC $PREV_COMMIT $CURR_RC $CURR_COMMIT \\\n   --commit-annotator-url recreate \\\n  --os-kind=HostOS \\\n  --verbose\n</code></pre> <p>That run tells you what the annotation process would do for that single commit in question.</p> <p>Please consult <code>--help</code> for additional options.</p>"},{"location":"release.html#tests","title":"Tests","text":""},{"location":"release.html#unit-tests","title":"Unit tests","text":"<pre><code>bazel test //release-controller/...\n</code></pre> <p>The above runs all tests and typechecks tested files.</p> <p>With a <code>.venv</code> setup by <code>rye</code>, you can also run (with varying levels of success):</p> <pre><code>export PYTHONPATH=$PWD/release-controller/\n.venv/bin/python3 release-controller/tests/runner.py\n</code></pre> <p>If you want to run a specific test file, specify its path as an argument to the above command line.</p>"},{"location":"release.html#typing-correctness","title":"Typing correctness","text":"<p>Building it all tests MyPy types:</p> <pre><code>bazel build //release-controller/...\n</code></pre>"},{"location":"release.html#maintenance","title":"Maintenance","text":"<p>The container image currently used by release controller components is an Ubuntu 24.04 image built by Bazel.  Refer to BUILD.bazel and images/BUILD.bazel for instructions on how to maintain and update the images.</p>"},{"location":"subnet-decentralization-whatif.html","title":"What-If Analysis of Subnet Decentralization","text":"<p>The <code>WhatifDecentralization</code> subcommand in the DRE tool allows users to perform \"what-if\" scenarios on the membership of a subnet and see the effect that the membership change would have on the subnet decentralization, without actually applying them, enabling better decision-making and risk assessment.</p>"},{"location":"subnet-decentralization-whatif.html#command-structure","title":"Command Structure","text":"<p>The <code>whatif-decentralization</code> command is a subcommand of the <code>subnet</code> command. The command accepts various parameters that specify the nodes to be added or removed, as well as specifying the subnet on which the analysis is performed.</p>"},{"location":"subnet-decentralization-whatif.html#usage","title":"Usage","text":"<pre><code>subnet whatif-decentralization &lt;SUBNET_ID&gt; [--add-nodes &lt;node-id...&gt;] [--remove-nodes &lt;node-id...&gt;] [--subnet-nodes-initial &lt;node-id...&gt;]\n</code></pre>"},{"location":"subnet-decentralization-whatif.html#parameters","title":"Parameters:","text":"<ul> <li> <p><code>&lt;SUBNET_ID&gt;</code>: The ID of the subnet where the analysis is performed. This parameter is required.</p> <p>Example: <code>tdb26-jop6k-aogll-7ltgs-eruif-6kk7m-qpktf-gdiqx-mxtrf-vb5e6-eqe</code></p> </li> <li> <p><code>--add-nodes</code>: A list of node IDs that you want to simulate adding to the subnet.</p> <p>Example:</p> <p><code>--add-nodes yahq2-6rnmm-n7ubm-q76zd-256dl-5f7k6-jxx5l-njyo2-hl7tk-sqcet-6ae tn2ne-tskw6-dfk3n-urdmd-krtq6-tcebq-dx2xr-kokq7-eood7-fadkg-5qe</code></p> <p>would show the impact on subnet decentralization if we add nodes <code>yahq2</code> and <code>tn2ne</code> to the subnet.</p> </li> <li> <p><code>--remove-nodes</code>: A list of node IDs that you want to simulate removing from the subnet.</p> <p><code>--remove-nodes gvm7l-ds4n6-vkyjn-gwalp-3vdo6-qfmq7-pxhu4-zvqcu-ozvb7-qz3gr-vqe  w2sev-mtuls-aa7m5-cdjgi-5vipg-2jn7i-4awvf-o6suu-73ebs-sw5db-kqe</code></p> <p>would show the impact on subnet decentralization if we remove nodes <code>gvm7l</code> and <code>w2sev</code> from the subnet.</p> </li> <li> <p><code>--subnet-nodes-initial</code>: A list of node IDs representing the initial state of the subnet. This can be used to override the current list of nodes in the subnet for the purpose of analysis.</p> <p>Example:</p> <p><code>--subnet-nodes-initial ncr4b-rasb7-tueb3-n4uos-5nxou-3wbxv-xmyt3-wfdsd-vu4b6-5x3cp-aqe ouffe-miylc-6zcwl-afv2d-lai62-qwzns-xtlji-p7pu2-qkx2e-x72y2-sqe tm3pc-2bjsx-hhv3v-fsrt7-wotdj-nbu3t-ewloq-uporp-tacou-lupdn-oae</code></p> <p>Note that it is necessary to provide the complete list of nodes in the subnet, so most likely you will need to provide 13 or more nodes in the list.</p> </li> </ul> <p>Please note that the number of nodes in the subnet should typically stay unchanged.</p>"},{"location":"subnet-decentralization-whatif.html#example-usage","title":"Example Usage","text":"<ol> <li> <p>Querying decentralization of an existing subnet:</p> <pre><code>dre subnet whatif tdb26-jop6k-aogll-7ltgs-eruif-6kk7m-qpktf-gdiqx-mxtrf-vb5e6-eqe\n</code></pre> </li> <li> <p>Adding and Removing Nodes from a Subnet: To simulate the effect of adding nodes <code>node4</code> and <code>node5</code> and removing nodes <code>node6</code> and <code>node7</code> from subnet <code>subnet123</code>, use the following command:</p> <pre><code>dre subnet whatif-decentralization subnet123 --add-nodes node4 node5 --remove-nodes node6 node7\n</code></pre> </li> <li> <p>Specifying Initial Nodes: If you want to override the current nodes in the subnet with a custom set for the analysis:</p> <pre><code>dre subnet whatif-decentralization subnet123 --subnet-nodes-initial node1 node2 node3 node4 node5 --add-nodes node6 --remove-nodes node2\n</code></pre> </li> </ol>"},{"location":"subnet-decentralization-whatif.html#how-it-works","title":"How It Works","text":"<ul> <li>The command creates a <code>ChangeSubnetMembershipPayload</code> that represents the proposed changes.</li> <li>The specified <code>SUBNET_ID</code> is used as the target subnet for the analysis.</li> <li>If the <code>subnet-nodes-initial</code> is specified, the analysis uses this custom list of nodes as the starting point; otherwise, it uses the current nodes in the subnet.</li> <li>The <code>decentralization_change</code> function then performs the analysis based on the information from the NNS registry, simulating the removal and addition of nodes, and prints the results.</li> </ul>"},{"location":"bazel/tips-and-tricks.html","title":"Tips and Tricks","text":"Local development and troubleshooting with OCI images <p>Steps: <pre><code># find available bazel build targets\nbazel query ... | grep image\n\n# build the image (target) of interest\nbazel build //rs/slack-notifications:slack-notifications-image\n\n# import the docker image generated by bazel into podman\nIMAGE=$(find bazel-out/ -name slack-notifications-image)\npodman load --input $IMAGE\n\n# run and test:\npodman run [&lt;other-args&gt;] localhost/bazel-out/k8-opt/bin/rs/slack-notifications/slack-notifications-image\n</code></pre></p> Add a deb package to an Ubuntu OCI image <p>Example code to be added to WORKSPACE.bazel (adjustments are necessary for your package!): <pre><code>oci_pull(\n    # tag = 22.04\n    # https://hub.docker.com/layers/library/ubuntu/22.04/images/sha256-cb2af41f42b9c9bc9bcdc7cf1735e3c4b3d95b2137be86fd940373471a34c8b0\n    name = \"ubuntu_22_04\",\n    digest = \"sha256:cb2af41f42b9c9bc9bcdc7cf1735e3c4b3d95b2137be86fd940373471a34c8b0\",\n    image = \"index.docker.io/library/ubuntu\",\n)\n\n_DEB_TO_LAYER = \"\"\"\\\ngenrule(\n    name = \"layer_tar\",\n    srcs = [\"@ubuntu22_ca_certificates//:data.tar.zst\"],\n    outs = [\"ca_certificates.tar\"],\n    cmd = \"cat $&lt; | zstd -d - -c &gt;| $@\",\n    visibility = [\"//visibility:public\"],\n)\n\nalias(\n    name = \"layer\",\n    actual = \":data.tar.zst\",\n    visibility = [\"//visibility:public\"],\n)\n\"\"\"\n\nhttp_archive(\n    name = \"ubuntu22_ca_certificates\",\n    build_file_content = _DEB_TO_LAYER,\n    sha256 = \"8ddd3b5d72fa144e53974d6a5782d25a0a9e1eec006118ecf2b76d53a7530f6a\",\n    urls = [\n        \"http://mirrors.kernel.org/ubuntu/pool/main/c/ca-certificates/ca-certificates_20230311ubuntu0.22.04.1_all.deb\",\n        \"http://de.archive.ubuntu.com/ubuntu/pool/main/c/ca-certificates/ca-certificates_20230311ubuntu0.22.04.1_all.deb\",\n        \"http://ftp.osuosl.org/pub/ubuntu/pool/main/c/ca-certificates/ca-certificates_20230311ubuntu0.22.04.1_all.deb\",\n    ],\n)\n</code></pre></p> <p>After that, once could add the additional layer to an image with something like:</p> <pre><code>rust_binary_oci_image_rules(\n    name = \"oci_image\",\n    src = \":slack-notifications\",\n    base_image = \"@distroless_cc_debian12\",\n    other_layers = [\"@ubuntu22_ca_certificates//:layer_tar\"],\n)\n</code></pre>"},{"location":"k8s/elastic-commands.html","title":"Elastic configuration","text":"<p>Some parts of elasticsearch configuration still has to be done manually since there is a limited scripting funcionallity.</p> <p>One off scripts are worth being done through kibana development console</p> <p>This document contains some snippets that can be useful while configuring the cluster or while debugging certain issues with elastic. Furthermore, there are some grafana dashboards which can help the developers investigate certain issues.</p>"},{"location":"k8s/elastic-commands.html#general-cluster-health","title":"General cluster health","text":"<p>Via kibana development console: <pre><code>GET _cluster/health\n</code></pre> or there is a panel on grafana indicating the same information.</p>"},{"location":"k8s/elastic-commands.html#cluster-stats","title":"Cluster stats","text":"<p>Via kibana dev console: <pre><code>GET _cluster/stats\n\nGET _cat/indices?s=index\n\nGET _cat/nodes?v&amp;h=id,v,rp,dt,du,dup\n\nGET _cat/nodes\n</code></pre> a lot of these are also visible on this grafana dashboard</p>"},{"location":"k8s/elastic-commands.html#cluster-settings","title":"Cluster settings","text":"<p>Via kibana dev console: <pre><code># Listing settings\nGET _cluster/settings\n\n# Updating settings\nPUT _cluster/settings\n{\n  \"persistent\": {\n    \"cluster\": {\n      \"routing\": {\n        \"allocation\": {\n          \"node_concurrent_incoming_recoveries\": \"3\",\n          \"enable\": \"all\",\n          \"node_concurrent_outgoing_recoveries\": \"3\"\n        }\n      },\n      \"max_shards_per_node\": \"6000\"\n    },\n    \"indices\": {\n      \"recovery\": {\n        \"max_bytes_per_sec\": \"100mb\"\n      }\n    }\n  },\n  \"transient\": {}\n}\n</code></pre></p>"},{"location":"k8s/elastic-commands.html#ingest-pipelines","title":"Ingest pipelines","text":"<p>Since there are a lot of logs that are not valuable we've introduced a couple of ingest pipelines which drop certain logs.</p> <p>To configure them use kibana dev console: <pre><code># Getting the current pipelines\nGET _ingest/pipeline\n\n# Our current pipeline settings:\nPUT _ingest/pipeline/false-warn-consensus\n{\n  \"processors\": [\n    {\n      \"drop\": {\n        \"description\": \"Drop documents with unneeded warn from Consensus\",\n        \"if\": \"\"\"\nString msg = ctx['MESSAGE'];\nif (msg != null) {\n  if (msg.contains(\"Error removing artifact ConsensusMessageId\")) {\n    return true;\n  }\n}\nreturn false;\n\"\"\"\n      }\n    }\n  ]\n}\n\nPUT _ingest/pipeline/useless-log-entries\n{\n  \"processors\": [\n    {\n      \"drop\": {\n        \"description\": \"Drop documents that bring no value\",\n        \"if\": \"\"\"\nString msg = ctx['MESSAGE'];\nif (msg == null) {\n  msg = ctx['message'];\n}\nif (msg == null) {\n  return true;\n}\nString syslog_identifier = ctx['SYSLOG_IDENTIFIER'];\nif (syslog_identifier != null) {\n  if (syslog_identifier == \"filebeat\") {\n    return true;\n  }\n  if (syslog_identifier == \"systemd\") {\n    if (msg != null) {\n      if (msg.startsWith(\"Starting \") || msg.startsWith(\"Finished \")) {\n        return true;\n      }\n      if (msg.endsWith(\".service: Succeeded.\")) {\n        return true;\n      }\n    }\n  }\n}\nif (msg != null) {\n  if (msg.startsWith(\"Drop - default policy: IN=enp1s0 OUT=\")) {\n    return true;\n  }\n  if (msg.startsWith(\"OpenTelemetry trace error occurred.\")) {\n    return true;\n  }\n  if (msg.startsWith(\"[DTS] Finished response callback \")) {\n    return true;\n  }\n  if (msg.endsWith(\"Done fetching new response.\")) {\n    return true;\n  }\n  if (msg.trim().length() &lt;= 5) {\n    return true;\n  }\n}\nreturn false;\n\"\"\"\n      }\n    }\n  ]\n}\n\nPUT _ingest/pipeline/nftables_log_entries\n{\n  \"description\": \"Parses nftable log entries into ECS format, dynamically capturing the drop/reject reason\",\n  \"processors\": [\n    {\n      \"grok\": {\n        \"field\": \"MESSAGE\",\n        \"patterns\": [\n          \"^%{DATA:nftable_log_prefix}: IN=%{DATA:observer.ingress.interface.name} OUT=%{DATA:observer.egress.interface.name} MAC=%{DATA:source.mac} SRC=%{IPV6:source.ip} DST=%{IPV6:destination.ip} LEN=%{NUMBER:network.bytes} TC=%{NUMBER:network.traffic_class} HOPLIMIT=%{NUMBER:network.ttl} FLOWLBL=%{NUMBER:network.flow_label} PROTO=%{WORD:network.transport} SPT=%{NUMBER:source.port} DPT=%{NUMBER:destination.port} WINDOW=%{NUMBER:network.window} RES=%{DATA:network.res} SYN URGP=%{NUMBER:network.urgp} $\"\n        ],\n        \"ignore_missing\": true\n      }\n    }\n  ]\n}\n\nPUT _ingest/pipeline/selinux-audit-log\n{\n  \"description\": \"Pipeline to process AVC log messages\",\n  \"processors\": [\n    {\n      \"grok\": {\n        \"field\": \"MESSAGE\",\n        \"patterns\": [\n          \"^AVC avc:  %{GREEDYDATA:action}  %{GREEDYDATA:auditd_rule} for  %{GREEDYDATA:kv_pairs}$\"\n        ],\n        \"ignore_failure\": true\n      }\n    },\n    {\n      \"script\": {\n        \"lang\": \"painless\",\n        \"source\": \"ctx.grok_success = ctx.containsKey('kv_pairs');\",\n        \"ignore_failure\": true\n      }\n    },\n    {\n      \"gsub\": {\n        \"if\": \"ctx.grok_success\",\n        \"field\": \"kv_pairs\",\n        \"pattern\": \"\\\"\",\n        \"replacement\": \"\",\n        \"ignore_failure\": true\n      }\n    },\n    {\n      \"kv\": {\n        \"if\": \"ctx.grok_success\",\n        \"field\": \"kv_pairs\",\n        \"field_split\": \" \",\n        \"value_split\": \"=\",\n        \"ignore_failure\": true\n      }\n    },\n    {\n      \"grok\": {\n        \"if\": \"ctx.grok_success\",\n        \"field\": \"scontext\",\n        \"patterns\": [\n          \"^%{DATA:source_context.user}:%{DATA:source_context.role}:%{DATA:source_context.type}:%{DATA:source_context.level}$\"\n        ],\n        \"ignore_failure\": true\n      }\n    },\n    {\n      \"grok\": {\n        \"if\": \"ctx.grok_success\",\n        \"field\": \"tcontext\",\n        \"patterns\": [\n          \"^%{DATA:target_context.user}:%{DATA:target_context.role}:%{DATA:target_context.type}:%{DATA:target_context.level}$\"\n        ],\n        \"ignore_failure\": true\n      }\n    },\n    {\n      \"remove\": {\n        \"if\": \"ctx.grok_success\",\n        \"field\": \"kv_pairs\",\n        \"ignore_failure\": true,\n        \"ignore_missing\": true\n      }\n    }\n  ]\n}\n\n\nPUT _ingest/pipeline/rm-noisy-log-entries\n{\n  \"processors\": [\n    {\n      \"drop\": {\n        \"if\": \"\"\"String msg = ctx['MESSAGE'];\nif (msg == null) {\n  msg = ctx['message'];\n}\nif (msg == null) {\n  return true;\n}\nString syslog_identifier = ctx['SYSLOG_IDENTIFIER'];\nif (syslog_identifier != null) {\n  if (syslog_identifier == \"filebeat\") {\n    return true;\n  }\n  if (syslog_identifier == \"systemd\") {\n    if (msg != null) {\n      if (msg.startsWith(\"Starting \") || msg.startsWith(\"Finished \")) {\n        return true;\n      }\n      if (msg.endsWith(\".service: Succeeded.\")) {\n        return true;\n      }\n      if (msg.endsWith(\".service: Deactivated successfully.\")) {\n        return true;\n      }\n    }\n  }\n}\nif (msg != null) {\n  if (msg.startsWith(\"Drop - default policy: IN=enp1s0 OUT=\")) {\n    return true;\n  }\n  if (msg.startsWith(\"[DTS] Finished response callback\")) {\n    return true;\n  }\n  if (msg.startsWith(\"Received the response for HttpRequest with callback id\")) {\n    return true;\n  }\n  if (msg.endsWith(\"[Canister renrk-eyaaa-aaaaa-aaada-cai] [GTC] get_account\")) {\n    return true;\n  }\n  if (msg.contains(\"[Canister ryjl3-tyaaa-aaaaa-aaaba-cai] [ledger] Checking the ledger for block\")) {\n    return true;\n  }\n  if (msg.endsWith(\"Done fetching new response.\")) {\n    return true;\n  }\n  if (msg.trim().length() &lt;= 5) {\n    return true;\n  }\n}\nreturn false;\n\"\"\",\n        \"ignore_failure\": true,\n        \"description\": \"Drop documents that bring no value\"\n      }\n    }\n  ]\n}\n\nPUT _ingest/pipeline/One-pipeline-to-rule-them-all\n{\n  \"processors\": [\n    {\n      \"pipeline\": {\n        \"name\": \"rm-noisy-log-entries\",\n        \"ignore_failure\": false\n      }\n    },\n    {\n      \"pipeline\": {\n        \"name\": \"nftables_log_entries\",\n        \"ignore_failure\": true\n      }\n    },\n    {\n      \"pipeline\": {\n        \"name\": \"selinux-audit-log\"\n      }\n    },\n    {\n      \"pipeline\": {\n        \"name\": \"false-warn-consensus\",\n        \"ignore_failure\": true\n      }\n    }\n  ]\n}\n\n# After creation of these pipelines devs should update the index template\nPUT _index_template/ic_logs_index_template\n{\n  \"priority\": 2,\n  \"template\": {\n    \"settings\": {\n      \"index\": {\n        \"routing\": {\n          \"allocation\": {\n            \"include\": {\n              \"_tier_preference\": \"data_content\"\n            }\n          }\n        },\n        \"default_pipeline\": \"One-pipeline-to-rule-them-all\",\n        \"refresh_interval\": \"30s\",\n        \"number_of_shards\": \"6\",\n        \"number_of_replicas\": \"1\"\n      }\n    },\n    \"mappings\": {\n      \"dynamic\": true,\n      \"numeric_detection\": false,\n      \"date_detection\": true,\n      \"dynamic_date_formats\": [\n        \"strict_date_optional_time\",\n        \"yyyy/MM/dd HH:mm:ss Z||yyyy/MM/dd Z\"\n      ],\n      \"_source\": {\n        \"enabled\": true,\n        \"includes\": [],\n        \"excludes\": []\n      },\n      \"_routing\": {\n        \"required\": false\n      },\n      \"dynamic_templates\": []\n    }\n  },\n  \"index_patterns\": [\n    \"ic-logs-*\"\n  ]\n}\n</code></pre></p>"},{"location":"k8s/elastic-commands.html#some-more-tips-and-tricks","title":"Some more tips and tricks","text":"<p>In kibana dev console: <pre><code># Update the default pipeline for one index\nPUT ic-logs-2024-11-28/_settings\n{\n  \"index\": {\n    \"default_pipeline\": \"One-pipeline-to-rule-them-all\"\n  }\n}\n\n# Get the settings for one index\nGET ic-logs-2024-11-28/_settings\n\n# Reroute specific shards\nPOST _cluster/reroute\n{\n  \"commands\" : [\n    {\"move\" : {\n      \"index\" : \"indexname\",\n      \"shard\" : 1,\n      \"from_node\" : \"nodename\",\n      \"to_node\" : \"nodename\"\n      }\n    }\n  ]\n}\n\n# Reindex certain indices\nPOST _reindex\n{\n  \"source\": {\n    \"index\": \"ic-logs-2024-01-14\"\n  },\n  \"dest\": {\n    \"index\": \"reindex-ic-logs-2024-01-14\"\n  }\n}\n\n# Delete indices\nDELETE ic-logs-2024-08-01,ic-logs-2024-08-02\n\n# Delete with wildcards\nPUT _cluster/settings\n{\n    \"transient\": {\n        \"action.destructive_requires_name\": false // allow wildcards\n    }\n}\n\nDELETE ic-logs-2024-*\n\nPUT _cluster/settings\n{\n    \"transient\": {\n        \"action.destructive_requires_name\": true // disallow wildcards\n    }\n}\n</code></pre></p>"},{"location":"k8s/tips-and-tricks.html","title":"Tips and Tricks","text":"Scale down and up Flux kustomize deployment <p>This allows you to make changes in a cluster without having Flux revert your changes all the time. Use it rarely, just during testing, or during troubleshooting. Communicate this change with the rest of the team. <pre><code>kubectl -n flux-system scale deployment kustomize-controller --replicas 0 --as root\n</code></pre></p> <p>You can scale back up the deployment with: <pre><code>kubectl -n flux-system scale deployment kustomize-controller --replicas 1 --as root\n</code></pre></p>"},{"location":"motion-proposals/124816%20New%20Wasm%20Instrumentation.html","title":"124816 New Wasm Instrumentation","text":""},{"location":"motion-proposals/124816%20New%20Wasm%20Instrumentation.html#what-is-instrumentation","title":"What is instrumentation?","text":"<p>The IC instruments Wasm canister binaries by injecting tiny snippets of code in order to count the number of executed instructions. This is needed to ensure that canister execution terminates and is fairly charged for.</p> <p>More information about this and benchmarking results can be found in the\u00a0associated forum post.</p>"},{"location":"motion-proposals/124816%20New%20Wasm%20Instrumentation.html#problem-statement","title":"Problem statement","text":"<p>The current instrumentation algorithm works well, but is inefficient for certain kinds of applications such as language interpreters. Also, the former instrumentation treats all instructions as equal, which may result in unfairness. Moreover, the current instrumentation is less than optimal when it comes to bounding execution round duration. This may result in lower block rate and latency increase for the end users. The new Wasm instrumentation addresses these inefficiencies and achieves an order of magnitude better performance for language interpreters, allowing developers to run their canisters more efficiently and possibly cheaper, as well as improved fairness and block rate.</p>"},{"location":"motion-proposals/124816%20New%20Wasm%20Instrumentation.html#proposed-solution-and-changes","title":"Proposed solution and changes","text":"<p>We propose a redesign of the way the IC performs instrumentation, modifying its core algorithm as well as important algorithm parameters, such as weights to be taken into account for Wasm instructions and system calls. The end result will be more efficient execution of user code, more stable block rate, and fair resource sharing between IC users.</p> <p>Whereas the old instrumentation treats all instructions as equal in terms of cost, the actual cost of an instruction depends on its type. For example, division is more expensive than addition. The standard practice in the blockchain world is to have different costs for different instructions, for example varying gas costs per opcode in the EVM, as specified in the\u00a0Yellow Paper. Therefore, in the reworking of the instrumentation component we propose to take instruction weights into account when doing instruction counting. This leads to a non-uniform instruction cost model that might affect the total cycle consumption of certain workloads.</p> <p>The proposed new instrumentation is already implemented and has been tested thoroughly. There are more details about this in the linked forum post. The new instrumentation is not enabled on mainnet. This proposal aims to align the community on whether the new instrumentation could be enabled. Specifically, the proposed changes for the new instrumentation and its parameters are:</p> <ol> <li>The new instrumentation algorithm \u2013 found in the proposed\u00a0code.</li> <li>Instruction weights are re-calibrated in several changes:\u00a0code,\u00a0code,\u00a0code,\u00a0code.</li> </ol>"},{"location":"motion-proposals/124816%20New%20Wasm%20Instrumentation.html#what-are-we-asking-the-community","title":"What are we asking the community","text":"<ul> <li>Follow the\u00a0instructions\u00a0to try locally the new instrumentation and check if and how your application is impacted.</li> <li>Participate in technical\u00a0discussions\u00a0as the motion moves forward.</li> <li>Vote accept or reject on NNS Motion.</li> </ul>"},{"location":"motion-proposals/124822%20Neuron%20Fund%20enhancements.html","title":"124822 Neuron Fund enhancements","text":""},{"location":"motion-proposals/124822%20Neuron%20Fund%20enhancements.html#proposal-overview","title":"Proposal Overview","text":"<p>We propose several enhancements to the Neurons\u2019 Fund based on collected experience and community feedback.</p> <ul> <li>Matched Funding: Shift from a fixed ICP amount to a model where the fund\u2019s contribution to SNS swaps scales with direct participation.</li> <li>10% Participation Cap: Introduce a cap ensuring the fund\u2019s contribution to an SNS does not exceed 10% of total available funds at the time of proposal execution. Consequently, this automatically adjusts the fund\u2019s participation if neurons opt out.</li> <li>Distinct Swap Contributions: Clearly separate contributions from direct participants and the Neurons\u2019 Fund in both the SNS swap proposal structure and the SNS launch pad in the NNS frontend dapp.</li> <li>Reduced Swap Duration: Decrease the maximum swap duration from 90 days to 14 days to prevent fund blockage by potentially unsuccessful swaps.</li> </ul>"},{"location":"motion-proposals/124822%20Neuron%20Fund%20enhancements.html#how-does-matched-funding-work","title":"How Does Matched Funding Work?","text":""},{"location":"motion-proposals/124822%20Neuron%20Fund%20enhancements.html#matching-function-f","title":"Matching function, f","text":"<p>The Matched Funding model employs an S-shaped matching function f, where x signifies the direct participation, and f(x) represents the Neurons\u2019 Fund (NF) contribution. The function is characterized by:</p> <ul> <li>Bounding Condition: Bounding Condition: To ensure the NF\u2019s contribution never surpasses the direct participation, f(x) is bounded by the function g(x)=x. Additionally, it remains below a cap, defined as the minimum of the ICP equivalent of 1M USD or 10% of the NF\u2019s maturity. This ensures no single SNS drains the NF excessively.</li> <li>Initial Lag Phase (I): Initially, f(x) stays at 0 until direct participation reaches an ICP equivalent of 100k USD, denoted as threshold t1. It then steadily rises until it hits an ICP equivalent of 300k USD (threshold t2), at which point the SNS receives a 2:1 contribution from the NF. This phase encourages projects to attract more direct participation.</li> <li>Growth Phase (II): The NF\u2019s contribution rises faster, providing more support to viable projects. When direct participation reaches the ICP equivalent of 500k USD (threshold t3), the SNS receives a 1:1 NF contribution.</li> <li>Saturation Phase (III): Beyond the threshold t3, the growth rate of f(x) diminishes. Once direct participation exceeds threshold t4, which is twice the cap, f(x) levels off at the cap.</li> </ul> <p>For a visual representation of the suggested matching function (which cannot be included directly in the proposal), please refer to this\u00a0link.</p> <p>The above-mentioned thresholds should be configurable as NNS parameters. Initially, these thresholds might be denominated in units of ICP, but eventually they should be denominated in terms of XDR.</p>"},{"location":"motion-proposals/124822%20Neuron%20Fund%20enhancements.html#benefits-of-matched-funding","title":"Benefits of Matched Funding","text":"<ul> <li>Better Reflection of Market Signals: The matched funding system is designed to closely align with market sentiment. Specifically, a project that successfully raises more direct contributions will correspondingly receive a greater contribution from the NF, up to a predetermined threshold.</li> <li>Streamlined Decision-making for NF NNS Neurons: The automated adjustment feature in the NF\u2019s contributions lessens the decision-making burden on NF neurons. As a result, these neurons have fewer instances where they need to opt out, making the process more efficient.</li> <li>Improved Incentives for Projects: The matching system provides a more compelling incentive structure for projects. Knowing that increased direct funding will be matched (up to a point) by the NF, encourages projects to be more proactive in their fundraising efforts.</li> </ul>"},{"location":"motion-proposals/124822%20Neuron%20Fund%20enhancements.html#community-engagement","title":"Community Engagement","text":"<p>The suggested enhancements have been syndicated in the forum. Find more details in\u00a0this thread.</p>"},{"location":"motion-proposals/125367%20Internet%20Computer%20topology%20-%20Optimization%20model.html","title":"125367 Internet Computer topology   Optimization model","text":""},{"location":"motion-proposals/125367%20Internet%20Computer%20topology%20-%20Optimization%20model.html#problem-statement","title":"Problem statement","text":"<p>The Internet Computer (IC) operates on physical node machines and achieves decentralization by partnering with multiple independent node providers. This decentralization reduces risks of central control, failures, and censorship. However, there is a trade-off: a high degree of decentralization comes at a cost in the form of node provider rewards to account for hardware, maintenance, and operations.</p> <p>In order to strike the right balance between network size &amp; cost and the degree of decentralization, the IC needs to tackle two aspects</p> <ul> <li>Establishing a Target Topology: This involves defining the number of subnets and their respective sizes, aligning with anticipated future demand. It also involves setting decentralization targets. A proposal for this target topology will be submitted in a separate motion proposal.</li> <li>Optimization: Given a target topology, use a model to optimize between node rewards (onboarding of additional new nodes and rewards for existing nodes) and decentralization.This is the primary focus of this motion proposal.</li> </ul>"},{"location":"motion-proposals/125367%20Internet%20Computer%20topology%20-%20Optimization%20model.html#modeling-approach","title":"Modeling approach","text":"<p>To address this problem, it is suggested to use the previously syndicated linear\u00a0model\u00a0for optimizing between node rewards (including onboarding new nodes and compensating existing ones) and network decentralization.</p> <p>Inputs to the model include:</p> <ul> <li>Available nodes and their characteristics (node provider, data center, data center provider, country).</li> <li>The target topology that describes the desired network structure, detailing the count and size of subnets, and decentralization goals.</li> </ul> <p>The model then calculates either the least number of new nodes or the minimal node rewards required to achieve these goals. A prototype implementation to run the model and create visuals is available.</p>"},{"location":"motion-proposals/125367%20Internet%20Computer%20topology%20-%20Optimization%20model.html#model-application-process","title":"Model application process","text":"<p>It is suggested to use the model framework for making informed decisions regarding future node onboarding as follows:</p> <p>Establishing a Target Topology</p> <p>The NNS agrees on a single target topology at any given time. Various decisions, such as whether to onboard additional nodes, can be derived from this agreed-upon topology. As the IC evolves, updated target topologies could be proposed to the NNS, ensuring continual alignment with the network\u2019s development and needs.</p> <p>Optimizing Node Allocation</p> <p>Utilizing the defined target topology, the model can determine the minimal number of nodes, or alternatively, the minimal amount of rewards required for achieving the target topology.</p> <p>Deciding on Node Candidates</p> <p>Utilizing the model, the following can be analyzed given a set of current nodes and node candidates:</p> <ul> <li>Effectiveness of Candidate Nodes: Determine if node candidates can directly reduce 1:1 the number of additional nodes needed.</li> <li>Node Relevance of Existing Nodes: Identify existing nodes that are not utilized to achieve the decentralization target, signaling potential candidates for offboarding.</li> </ul>"},{"location":"motion-proposals/125549%20Internet%20Computer%20topology%20-%20target%20IC%20topology.html","title":"Motion Proposal Target IC Topology","text":"<p>As part of the forum posts on IC topology (see\u00a0Forum post 1\u00a0and\u00a0Forum post 2) an optimization model designed to reach decentralization targets with a minimum node count as well as a IC target topology have been discussed within the IC community. This motion proposal is for the community to vote on the initial IC target topology. A separate\u00a0motion proposal\u00a0has been submitted and accepted for the community to vote on the optimization model.</p>"},{"location":"motion-proposals/125549%20Internet%20Computer%20topology%20-%20target%20IC%20topology.html#establishing-a-target-topology","title":"Establishing a Target Topology","text":"<p>Should this proposal be adopted, the target topology as presented below and motivated in\u00a0Forum post 2\u00a0shall be used by the IC community to decide on future node onboarding in the following way:</p> <ul> <li>Target Subnet structure: This involves determining the number and respective sizes of subnets, aligned with anticipated future demand.</li> <li>Decentralization Targets: Per subnet, decentralization targets should be set, utilizing either a Nakamoto coefficient or a subnet limit. The node topology matrix, described in the previous Forum post 2, assists in evaluating achievable targets.</li> </ul> <p>It is proposed that the NNS agrees on a single target topology at any given time. Various decisions, such as whether to onboard additional nodes, can then be derived from this agreed-upon topology, and potential node providers can get clarity. As the IC evolves, new target topologies can be proposed to the NNS, ensuring continual alignment with the network\u2019s development and needs.</p>"},{"location":"motion-proposals/125549%20Internet%20Computer%20topology%20-%20target%20IC%20topology.html#proposed-target-topology","title":"Proposed Target Topology","text":"<p>The following table specifies the type, number, and size of anticipated subnets. The column labeled \u201cSEV\u201d indicates whether the subnet is designated to run on generation 2 SEV machines, enhancing protection against malicious actors. This table serves as a suggestion for the Subnet Target Structure for the next 6-12 months or until a new target topology is adopted by the NNS. The number of subnets is based on current and anticipated demand. There are some special subnets that are dedicated to specific use cases, e.g., ECDSA signing, SNS, and NNS. The sizes of the subnets were chosen depending on the sensitivity of the services/dapps running on them, e.g,. the NNS subnet has the highest sensitivity and is thus proposed to be larger than app subnets.</p> <p>The following decentralization targets are proposed:</p> <ul> <li>The node provider, data center, and data center provider characteristics, will adhere to maximum decentralization (subnet limit = 1 where subnet limit is defined as the maximum number nodes with identical node provider, data center or data center providers in one specific subnet).</li> <li>The decentralization target for the country characteristic will be set to 3 for subnets with 28 or more nodes, and 2 for subnets consisting of 13 nodes. This ensures that both larger and smaller subnets have similar decentralization coefficients (Nakamoto coefficients) that are high enough for each type of subnet to have a high level of protection against malicious or colluding nodes. The country limit would permit the same country to appear up to twice in any 13 node subnet, and three times in a subnet of 28 or more nodes (country subnet limit = 2 or 3 where country subnet limit is defined as the maximum number of nodes from one specific country in one specific subnet).</li> </ul> Subnet Type # Subnets # Nodes in subnet Total SEV Subnet limit NP, DC, DC Provider* Subnet limit country NNS 1 43 43 no 1 3 SNS 1 34 34 no 1 3 Fiduciary 1 28 28 no 1 3 Internet Identity 1 28 28 yes 1 3 ECDSA signing 1 28 28 yes 1 3 ECDSA backup 1 28 28 yes 1 3 Bitcoin canister 1 13 13 no 1 2 European subnet 1 13 13 yes 1 2 Swiss subnet 1 13 13 yes 1 2 Application subnet 31 13 403 no 1 2 Reserve nodes Gen1 100 Reserve nodes Gen2 20 Total 751 <p>*Node Provider, Data Center, Data Center Provider</p>"},{"location":"motion-proposals/126094%20Resource%20Reservation%20Mechanism.html","title":"Background &amp; Problem statement","text":"<p>Canisters pay for storage dynamically every few minutes following the \"pay-as-you-go\" model. Such a fine-grained payment is convenient for developers, but at the same time it doesn\u2019t handle spikey usage patterns well.</p> <p>Consider a scenario where someone allocates the entire subnet storage for a few hours and pays only for those hours. The end result is that during those hours, the operation of other canisters on the same subnet might be disrupted as they might fail to allocate new storage. The problem here is that the cost of such a spikey usage is low due to the \"pay-as-you-go\" model.</p> <p>The goal of this proposal is to address this long-standing problem with a new resource reservation mechanism that is designed to discourage the spiky usage pattern by making it more expensive while at the same time keeping costs the same for long-term users.</p>"},{"location":"motion-proposals/126094%20Resource%20Reservation%20Mechanism.html#proposal","title":"Proposal","text":"<p>Recently the subnet storage capacity has been increased from\u00a0<code>450GiB</code>\u00a0to\u00a0<code>700GiB</code>. The newly added\u00a0<code>250GiB</code>\u00a0is subject to a new resource reservation mechanism that works as follows:</p> <ul> <li>As long as the subnet remains under the previous limit of\u00a0<code>450GiB</code>, the storage payment remains the same as before, following the \"pay-as-you-go\" model. This is the case for all subnets as of this writing.</li> <li>When the subnet grows above\u00a0<code>450GiB</code>, then the new reservation mechanism activates. Every time a canister allocates new storage bytes, the system sets aside some amount of cycles from the main balance of the canister. These reserved cycles will be used to cover future payments for the newly allocated bytes. The reserved cycles are not transferable and the amount of reserved cycles depends on how full the subnet is. For example, it may cover days, months, or even years of payments for the newly allocated bytes. It is important to note that the reservation mechanism applies only to the newly allocated bytes and does not apply to the storage already in use by the canister.</li> </ul> <p>Summary of the changes:</p> <ul> <li>A new field named\u00a0<code>reserved_cycles</code>\u00a0is added to the canister state.</li> <li>A new field name\u00a0<code>reserved_cycles_limit</code>\u00a0is added to canister settings.</li> <li>Storage allocation operations such as\u00a0<code>memory.grow</code>,\u00a0<code>stable_grow</code>,\u00a0<code>stable64_grow</code>, setting\u00a0<code>memory_allocation</code>\u00a0are adjusted to move cycles from the main balance to\u00a0<code>reserved_cycles</code>.</li> <li>The periodic charging for storage is adjusted to first burn cycles from\u00a0<code>reserved_cycles</code>\u00a0and only when that reaches 0, to start using the main balance.</li> <li>The freezing threshold computation is also updated to take\u00a0<code>reserved_cycles</code>\u00a0into account. This means that even if the main balance is below the freezing threshold, the canister may still be functional if it has enough\u00a0<code>reserved_cycles</code>.</li> </ul> <p>Detailed explanation of the changes is available in the linked\u00a0forum thread.</p> <p>Properties of the new reservation mechanism:</p> <ul> <li>It makes spikey usage of storage where a canister frequently allocates and deallocates storage more expensive. Note that currently the only way to deallocate storage is to uninstall, reinstall, or delete the canister, so it is not a common usage pattern.</li> <li>It has minimal impact on costs for canisters with long-term storage usage.</li> <li>It is backwards compatible in the sense that it activates only in the newly added\u00a0<code>250GiB</code>. Nothing changes for canisters if their subnet remains below the old limit of\u00a0<code>450GiB</code>.</li> <li>It seamlessly integrates with the allocation operations without changing their interfaces.</li> <li>Controllers can opt out by setting\u00a0<code>reserved_cycles_limit</code>\u00a0to zero. Such opted-out canisters would not be able to allocate from the newly added\u00a0<code>250GiB</code>, which means that these canisters will trap if they try to allocate storage when the subnet usage grows above\u00a0<code>450GiB</code>.</li> </ul>"},{"location":"motion-proposals/126094%20Resource%20Reservation%20Mechanism.html#voting","title":"Voting","text":"<p>Normally, changes in the protocol first go through the community discussion, then the NNS proposal, and then roll out to the mainnet. In this case, because of the possibility that this vulnerability of the protocol could be abused, DFINITY followed\u00a0the approach for security fixes, where the fix is deployed first and then discussed later.</p> <p>The new reservation mechanism has been implemented in the replica and is active in the newly added\u00a0<code>250GiB</code>.</p> <p>This vote is about whether to keep the new reservation mechanism or remove it. If the proposal is accepted, then the new reservation mechanism will remain in replica and will be added to the interface specification of the Internet Computer. If the proposal is rejected, then the new reservation mechanism will be removed and an alternative fix for vulnerability would need to be found and implemented.</p>"},{"location":"motion-proposals/127044%20Interim%20remuneration%20proposal%20-%20rewards%20for%20Gen-1%20node%20machines%20after%2048%20months.html","title":"127044 Interim remuneration proposal   rewards for Gen 1 node machines after 48 months","text":""},{"location":"motion-proposals/127044%20Interim%20remuneration%20proposal%20-%20rewards%20for%20Gen-1%20node%20machines%20after%2048%20months.html#problem-statement","title":"Problem statement","text":"<p>With the approval of the\u00a0IC target topology, a target of 750 node machines in the IC network is defined for the next half year/year. This target is defined keeping in mind that several of the Gen-1 node provider agreements will expire in the next two years.</p> <p>Although the IC target topology requires fewer node machines than the current number of node machines (750 node machines vs approximately 1300 node machines), the IC network still requires an extensive number of Gen-1 node machines for operating purposes.</p> <p>The long term objective is to have remuneration based on useful work for all node machines, which means node rewards are paid out based on the actual contribution to the IC, e.g. the number of blocks created, the size of the blocks created, how many times the node machine has been a block maker, in which subnet the node machine is running, etc; regardless of the type of node machine and when the node machine was bought. Since implementing this new approach to remuneration requires extensive discussion within the community as well as time to design and develop, an interim approach is required for the remuneration of Gen-1 node machines for which the node provider agreements will expire.</p> <p>Despite the introduction of Gen-2 node machines, the Gen-1 node machines are still very relevant for the IC network for several reasons:</p> <ul> <li>They provide for the necessary decentralization of the IC network.</li> <li>Not all subnets require SEV-SNP functionality (the additional security functionality introduced with Gen-2 node machines).</li> <li>Since the initial capital investments for the Gen-1 node machines have been amortized, Gen-1 node machines are economically very attractive to operate.</li> <li>They provide for a buffer to scale up the IC network should use of the network start to increase sharply.</li> </ul> <p>On the other hand, Gen-1 node machines in the IC network have several constraints:</p> <ul> <li>They cannot be deployed in every IC subnet since some subnets will require node machines with SEV-SNP support.</li> <li>As described in the forum posts on node diversification (see\u00a0node diversification part 1, and\u00a0node diversification part 2) - Gen-1 node machines are less decentralized and more concentrated at fewer node providers than Gen 2 node machines.</li> <li>There are too many Gen-1 node machines to fit the IC target topology.</li> </ul>"},{"location":"motion-proposals/127044%20Interim%20remuneration%20proposal%20-%20rewards%20for%20Gen-1%20node%20machines%20after%2048%20months.html#proposal","title":"Proposal","text":"<p>Taking into account both the benefits and constraints of Gen-2 node machines, the following interim remuneration scheme for Gen-1 node machines after 48 months is proposed:</p> <ul> <li>Rewards are optimized for 28 node machines\u00a0- if all Gen-1 node provider agreements have reached 48 months, it can be calculated that with a maximum of 28 nodes per Gen-1 Node provider, sufficient node machines remain in the IC network to meet the target topology of 750 node machines. However, it will still be possible for a Node Provider to continue to operate up to 42 node machines (similar as for Gen-2 Node Providers, and described in node diversification part 2), for example in anticipation of growth of the IC network and increase in ICP token price.</li> <li>Rewards for Gen-1 node machines are lower than at launch\u00a0- rewards for Gen-1 node machines are lower than the rewards set at launch because of several reasons: not all Gen-1 node machines add to decentralization, Gen-1 node machines cannot be deployed in every subnet, and the initial investment costs for buying the node machines have been amortized by the node provider.</li> <li>Rewards apply for a period of 12 months\u00a0- the interim remuneration proposal applies for a period of 12 months, after which the scheme will be reevaluated based on feedback and input from the community.</li> <li>Rewards for Gen-1 node machines follow a similar formula as the rewards scheme for Gen-2 node machines\u00a0- node rewards will follow the same formula as remuneration for Gen-2 node machines, which is Initial reward for first node machine x Multiplier x Reduction Coefficient.</li> </ul> <p>The Gen-1 node machine rewards are set at the values specified in the below table. To summarize the remuneration scheme, for a geography g, let</p> <p>mult(g)\u00a0be the geography multiplier</p> <p>value(g)\u00a0be the base value for a Gen-1 node in XDR</p> <p>r(np, g)\u00a0be the reduction coefficient</p> <p>Then the monthly reward for the n-th node of a Node Provider (np) in geography g are defined as follows:</p> <p>reward(g, n)\u00a0= value(g) * mult(g) * r(np, g) ^ (n-1)</p> <p>With a multiplier of 2.5 on the base value of the node, and a reduction coefficient of 0.97, this optimum of 28 node machines as described above can be achieved. The following table shows the geography-dependent values and the monthly reward for the first node onboarded. A few previously-separated geographic areas have been combined:</p> Geography Gen-2 value per node before multiplier for comparison Reduced value for non- SEV-SMP nodes Multiplier Monthly reward for 1<sup>st</sup> node Reduction coefficient r US - California 771 496 2.5 1247.5 0.97 US - other 647 465 2.5 1162.5 0.97 Canada 771 496 2.5 1247.5 0.97 Europe 771 496 2.5 1247.5 0.97 Japan and Singapore 844 568 2.5 1420 0.97 <p>The above formula and table can be used to calculate the accumulated profit for each additional node. When calculating the accumulated profit for Gen-1 node machines in the United States, the below graph results, which shows the total profit for all machines up to the n-th node machine. It shows that when 28 nodes (2 racks of node machines) are kept on the IC network, almost maximum profit is achieved (30 to 31 node machines being the optimal).</p>"},{"location":"motion-proposals/127668%20Scalable%20Messaging%20Model.html","title":"Scalable Messaging Model","text":""},{"location":"motion-proposals/127668%20Scalable%20Messaging%20Model.html#background","title":"Background","text":"<p>The IC\u2019s messaging model (as of January 2024) is conceptually an attractive proposition: remote procedure calls (RPCs) with reliable replies and automatic call bookkeeping through call contexts. Unpacking it a bit:</p> <ul> <li>Canisters interact via RPCs<ul> <li>A \u21fe B\u00a0request\u00a0(message); followed by B \u21fe A\u00a0response\u00a0(message).</li> <li>Every call being handled (Motoko\u00a0shared function; or Rust\u00a0update function) is backed by a call context.</li> </ul> </li> <li>Best-effort requests<ul> <li>With\u00a0ordering guarantees: if canister A makes two calls,\u00a0c1\u00a0and\u00a0c2\u00a0(in that order) to canister B, then\u00a0c2\u00a0will not be delivered before\u00a0c1.</li> </ul> </li> <li>Guaranteed responses<ul> <li>There is\u00a0globally exactly one response\u00a0(reply or reject) for every request.</li> <li>This response is\u00a0eventually delivered\u00a0to the caller.</li> </ul> </li> <li>Backpressure mechanism<ul> <li>Canister A may only have a limited number of outstanding calls to canister B.</li> </ul> </li> </ul>"},{"location":"motion-proposals/127668%20Scalable%20Messaging%20Model.html#problem-statement","title":"Problem Statement","text":"<p>The long term goal of messaging on the IC is to ensure that\u00a0canisters can rely on canister-to-canister messaging, irrespective of subnet load and with reasonable operational costs.</p> <p>This goal is impossible to achieve with the current messaging model; to the extent that\u00a0there were already discussions\u00a0about increasing prices for messaging on the IC. These discussions were paused to take a step back and see whether there are any other variables that could be tweaked to achieve the goal.</p> <p>The reasons for why messaging on the IC doesn\u2019t satisfy the long term goal are the following:</p> <ul> <li>Guaranteed response delivery implies unbounded response times. Concrete examples where this is a problem include calling into untrusted canisters, safe canister upgrades, and responsive applications in general. It also makes (true) fire-and-forget type of messages impossible. Later \u2013 once the IC supports more diverse subnet topologies \u2013 calling into dishonest subnets will also become a problem because of these guarantees.</li> <li>Relatively large upper bound on the size of requests/replies (2MB while the mean message size observed on mainnet is 1kB). In combination with guaranteed replies, this requires reserving orders of magnitude more memory than necessary in most practical cases, increasing costs both to the canister and to the system.</li> </ul>"},{"location":"motion-proposals/127668%20Scalable%20Messaging%20Model.html#proposed-solution","title":"Proposed Solution","text":"<p>The shortcomings above can be addressed by extending the current messaging model in two directions:\u00a0small messages with guaranteed responses, and\u00a0best-effort messages. The extensions will require explicit opt-in from canister developers so that backwards compatibility is maintained. Canisters that do not take any action will simply keep sending messages with the current semantics and guarantees.</p>"},{"location":"motion-proposals/127668%20Scalable%20Messaging%20Model.html#small-messages-with-guaranteed-responses","title":"Small Messages with Guaranteed Responses","text":"<p>Small messages with guaranteed replies have the same semantics as existing canister-to-canister messages, except for being limited to 1 kB payloads. Besides being significantly less resource-intensive, the size restriction opens the possibility of ensuring every canister a quota of messages and thus a much more predictable environment. The current thinking is that it should be possible to give every canister guaranteed quotas of 50kB for incoming and 50kB for outgoing small messages that can be in flight at the same time, plus use of an optimistically shared pool of 5GB. (We assume an upper bound of 100k canisters per subnet. More would only be reasonable if they are part of one or more canister groups where a quota is no longer so important. Note that the guarantee to be able to produce an outgoing request does not change anything to the fact that delivery of requests is best-effort.) Initially small messages\u2019 payloads will be limited to 1kB (50 incoming, 50 outgoing, 5M shared optimistically), but given demand this can be made more flexible later.</p> <p>Small guaranteed-response messages still have the issue of potentially unbounded response times, but this may be an acceptable tradeoff in certain situations.</p>"},{"location":"motion-proposals/127668%20Scalable%20Messaging%20Model.html#best-effort-messages","title":"Best-Effort Messages","text":"<p>For best-effort messages, both request and response delivery would be best-effort, which opens up the possibility for the system to ensure fairness even in high load scenarios via fair load shedding. Because the system may drop requests or responses under heavy load, memory reservations for responses are unnecessary. From a canister\u2019s perspective every request still gets exactly one response. But the system does not guarantee that it is a globally unique response (e.g. the callee may produce a reply while the caller sees a timeout reject).</p> <p>This means that canister developers who choose to rely on best-effort messages may have to handle the case where they can not infer from the reply whether the callee changed its state or not. In other words, best-effort messages allow developers to make a choice between bounded response times and the (potential) requirement to handle this additional case.</p> <p>Additionally, every call has a deadline, set explicitly by the caller; or implicitly by the system. And when a response does not materialize before the deadline (whether because the callee did not produce one; or because the response got dropped) the subnet always generates and delivers a timeout reject response.</p> <p>Similarly to small guaranteed-response messages, canisters would be guaranteed a quota of 50 concurrent best-effort calls, complemented by an optimistically shared pool of 5M calls.</p>"},{"location":"motion-proposals/127668%20Scalable%20Messaging%20Model.html#whats-in-it-for-me","title":"What\u2019s in It for Me?","text":"<p>A predictable environment in terms of messaging, responsiveness, scalability, fairness, upgradeable canisters, safe interactions with untrusted canisters and malicious subnets. Eventually, sensible retries.</p> <p>For more details see the accompanying\u00a0forum post.</p>"},{"location":"motion-proposals/127668%20Scalable%20Messaging%20Model.html#conclusion-next-steps","title":"Conclusion &amp; Next Steps","text":"<p>The new extensions to the messaging model will provide an environment canisters are able to rely on, and, hence, make it easier to implement reliable and/or consistent cross-canister applications.</p> <p>The scope of this proposal is the high-level direction to evolve the protocol outlined above. Assuming the proposal is accepted, we will keep working on the interface details and follow up as soon as they are worked out (link to the proposed changes to be posted on the forum). With that in place, the goal is to start working on a first iteration towards an MVP in the replica; and expose it in Motoko and the Rust CDK. Discussions on technical details will continue in the\u00a0forum discussion.</p> <p>Work beyond an MVP will be prioritized based on real-world community and system needs: besides completing the vision outlined in this post, we believe that there will be demand for fair load shedding; a\u00a0<code>sleep</code>\u00a0API; rejecting pending calls on stop; dropping the payloads of expired messages from blocks; bidding for message priority; etc.</p>"},{"location":"motion-proposals/128820%20Subnet%20Rental.html","title":"SRC Motion Proposal","text":""},{"location":"motion-proposals/128820%20Subnet%20Rental.html#background","title":"Background","text":"<p>Multiple community members have expressed interest in renting an entire ICP subnet. There seem to be sufficient use cases that justify the introduction of subnet rental on the Internet Computer. Subnet rental means that a whole subnet can be rented by a tenant (a person or company). A rented subnet gives the tenant a) exclusive resource access and b) some choice over the geographic distribution of nodes, which in turn allows more regulatory certainty. The IC ecosystem benefits because subnets are rented for a fee that assumes full utilization, which will significantly increase the cycles burn rate, contributing to deflation.</p> <p>The community, represented by the NNS, retains ownership of all IC resources, including subnets for rent. A rental agreement is therefore an agreement between a tenant and the NNS. This suggests the use of NNS proposals to create such an agreement. The rental process, in particular payments, should be handled by a smart contract, so that the solution is as autonomous as possible. This suggests a canister whose domain is subnet rental. This Subnet Rental Canister (SRC) is controlled by the NNS and installed on the NNS subnet, because it 1) handles a significant amount of cycles and 2) must be able to make privileged method calls on other NNS canisters, so the security standards of the SRC should at least match that of its dependencies (CMC, Registry canister, etc.).</p>"},{"location":"motion-proposals/128820%20Subnet%20Rental.html#details","title":"Details","text":"<p>Exclusive access to a subnet can be granted via whitelisting a user\u2019s principal. Only this principal will be able to install canisters on the rented subnet. A method for this purpose already exists on the cycles minting canister. Disabling cycles consumption for the rented subnet can be achieved with a new Registry flag and small changes to the execution environment. The lifecycle of a rental agreement between the tenant and the NNS, as well as the payment flow, is the responsibility of the new subnet rental canister.</p>"},{"location":"motion-proposals/128820%20Subnet%20Rental.html#setup-process","title":"Setup Process","text":"<p>In general, a subnet with a specific topology desired by the user may not yet exist. Therefore, the process described below includes the creation of a new subnet as part of the rental agreement setup.</p> <p>Here is a timeline of the setup process:</p> <ol> <li>The user who wishes to rent a subnet pays a deposit to an ICP ledger subaccount controlled by the SRC. The amount is defined in XDR, published on the SRC, and covers six months of node provider rewards, scaled by subnet size and including a surcharge. The XDR/ICP exchange rate used must be the exchange rate from the UTC-midnight before the proposal creation in step 2. In other words: No midnight should lie between the payment and the proposal creation.</li> <li>The user creates an NNS proposal of type \u201cRental Subnet Request\u201d, which contains their principal and either a topology description or an existing subnet id. The description may refer to the geographical features of the desired subnet and will be judged by the NNS community.<ol> <li>If the proposal is rejected, the deposit is refunded</li> <li>If the proposal is accepted, the topology description is considered an addendum to the NNS-agreed target topology, which should facilitate the necessary node provider and node onboarding proposals. 10% of the deposit is considered non-refundable and is converted to cycles immediately. Every 30 days, another 10% of the initial amount is converted to cycles and becomes non-refundable. The remaining ICP are refundable up until the subnet is created. A refund can be initiated via a method \u201cget_refund\u201d on the SRC which may only be called by the principal specified in the NNS proposal.</li> <li>The proposal fails if the SRC does not find the deposit or it does not suffice (based on the XDR/ICP exchange rate from the UTC-midnight before the proposal creation).</li> </ol> </li> <li>Over the next weeks, new node providers may be onboarded, new nodes created etc.</li> <li>When all necessary nodes exist, the user may create a subnet creation proposal. This existing proposal type is extended with an optional field \u201cparent_proposal_id\u201d. This field must contain the id of the initial proposal from step 2, so the SRC can detect the newly created subnet id and connect it to the rental agreement and the user\u2019s principal. The subnet is now ready to be rented, so the SRC whitelists the user\u2019s principal via a call to the CMC, converts the remaining ICP to cycles and starts burning cycles.</li> </ol>"},{"location":"motion-proposals/128820%20Subnet%20Rental.html#active-rental-phase","title":"Active Rental Phase","text":"<p>While a rental agreement is active, the SRC keeps track of the point in time until which the subnet is paid for, referred to as \u201ccovered_until\u201d in the following. If this point in time lies less than three months in the future, the SRC attempts to convert ICP to cycles to cover for the next month. If it succeeds, \u201ccovered_until\u201d is moved 30 days into the future. Otherwise, the SRC tries again the next day. If \u201ccovered_until\u201d lies in the past, the SRC suspends services on the rented subnet (see below).</p> <p>The cycles are burned at regular intervals during the rental period. The total cycles burn rate of the IC will increase significantly with every active rental agreement.</p>"},{"location":"motion-proposals/128820%20Subnet%20Rental.html#suspension","title":"Suspension","text":"<p>In case payments cease and funds run out, the SRC can autonomously suspend services on a rented subnet. Suspension is reversible and should not brick canisters that are developed robustly. It should, however, make running canisters unusable for the duration of the suspension, and prevent malicious canisters from doing any harmful work (including calling into other subnets).</p> <p>In case potential malicious canisters on the rented subnet interfere with other subnets, the suspension mechanism can be used by the community to mitigate the problem at any time via a regular NNS proposal.</p> <p>When a tenant no longer has use of a rented subnet, they stop paying, the SRC suspends the whole subnet and the community can decide how to reuse either the whole subnet or the nodes it comprises.</p>"},{"location":"motion-proposals/128820%20Subnet%20Rental.html#proposal","title":"Proposal","text":"<p>By adopting this proposal, the NNS agrees to adding the above described subnet rental capability to the ICP. This feature is realized with a new canister under NNS control, the subnet rental canister (SRC). A new proposal type \u201cSubnet Rental Request\u201d shall be added, which is executed on the SRC. The SRC processes and supervises the payment flow and can autonomously suspend the services on a rented subnet if payments cease.</p> <p>Forum discussions on this feature can be found\u00a0here\u00a0and\u00a0here.</p> <p>Please vote to accept or reject this motion proposal.</p>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html","title":"The ICRC-3 Block Log Standard","text":""},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#introduction","title":"Introduction","text":"<p><code>ICRC-3</code>\u00a0is a standard for accessing the block log of a Ledger on the\u00a0Internet Computer.</p> <p>The ICRC-3 Fungible Token Standard is the product of the collaborative effort of the\u00a0Ledger &amp; Tokenization working group.</p>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#what-does-the-standard-imply","title":"What does the standard imply?","text":"<p>ICRC-3 is a contract between organizations participating in the Ledger &amp; Tokenization working group and the community. If the standard is accepted, we agree to</p> <ol> <li>Provide a production-ready implementation of the standard.</li> <li>Support the standard in our existing and future products.</li> <li>Build tools to interact with standard-compliant implementations.</li> <li>Promote the standard.</li> <li>Design extensions to the standard to simplify and scale payment flows.</li> </ol> <p>Accepting the ICRC-3 standard does not imply that all other standards should be considered obsolete. Everyone is free to experiment with new designs, application interfaces, and products.</p> <p>The main goal of ICRC-3 is to provide a solid common ground for interoperability.</p>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#what-does-icrc-3-specify","title":"What does\u00a0<code>ICRC-3</code>\u00a0specify?","text":"<ol> <li>A way to fetch the archive nodes of a Ledger</li> <li>A generic format for sharing the block log without information loss. This includes the fields that a block must have</li> <li>A mechanism to verify the block log on the client side to allow downloading the block log via query calls</li> <li>A way for new standards to define new transactions types compatible with ICRC-3</li> </ol>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#archive-nodes","title":"Archive Nodes","text":"<p>The Ledger must expose an endpoint\u00a0<code>icrc3_get_archives</code>\u00a0listing all the canisters containing its blocks.</p>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#block-log","title":"Block Log","text":"<p>The block log is a list of blocks where each block contains the hash of its parent (<code>phash</code>). The parent of a block\u00a0<code>i</code>\u00a0is block\u00a0<code>i-1</code>\u00a0for\u00a0<code>i&gt;0</code>\u00a0and\u00a0<code>null</code>\u00a0for\u00a0<code>i=0</code>.</p> <pre><code>   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   |         Block i         |          |         Block i+1       |\n   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u25c4\u2500\u2500| phash = hash(Block i-1) |\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500| phash = hash(Block i)   |\n   | ...                     |          | ...                     |\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#value","title":"Value","text":"<p>The\u00a0candid\u00a0format supports sharing information even when the client and the server involved do not have the same schema (see the\u00a0Upgrading and subtyping\u00a0section of the candid spec). While this mechanism allows to evolve services and clients independently without breaking them, it also means that a client may not receive all the information that the server is sending, e.g. in case the client schema lacks some fields that the server schema has.</p> <p>This loss of information is not an option for\u00a0<code>ICRC-3</code>. The client must receive the same exact data the server sent in order to verify it. Verification is done by hashing the data and checking that the result is consistent with what has been certified by the server.</p> <p>For this reason,\u00a0<code>ICRC-3</code>\u00a0introduces the\u00a0<code>Value</code>\u00a0type which never changes:</p> <pre><code>type Value = variant {\n    Blob : blob;\n    Text : text;\n    Nat : nat;\n    Int : int;\n    Array : vec Value;\n    Map : vec record { text; Value };\n};\n</code></pre> <p>Servers must serve the block log as a list of\u00a0<code>Value</code>\u00a0where each\u00a0<code>Value</code>\u00a0represent a single block in the block log.</p>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#value-hash","title":"Value Hash","text":"<p><code>ICRC-3</code>\u00a0specifies a standard hash function over\u00a0<code>Value</code>.</p> <p>This hash function should be used by Ledgers to calculate the hash of the parent of a block and by clients to verify the downloaded block log.</p> <p>The hash function is the\u00a0representation-independent hashing of structured data\u00a0used by the IC:</p> <ul> <li>the hash of a\u00a0<code>Blob</code>\u00a0is the hash of the bytes themselves</li> <li>the hash of a\u00a0<code>Text</code>\u00a0is the hash of the bytes representing the text</li> <li>the hash of a\u00a0<code>Nat</code>\u00a0is the hash of the\u00a0<code>leb128</code>\u00a0encoding of the number</li> <li>the hash of an\u00a0<code>Int</code>\u00a0is the hash of the\u00a0<code>sleb128</code>\u00a0encoding of the number</li> <li>the hash of an\u00a0<code>Array</code>\u00a0is the hash of the concatenation of the hashes of all the elements of the array</li> <li>the hash of a\u00a0<code>Map</code>\u00a0is the hash of the concatenation of all the hashed items of the map sorted lexicographically. A hashed item is the tuple composed by the hash of the key and the hash of the value.</li> </ul> <p>Pseudocode for representation independent hashing of Value, together with test vectors to check compliance with the specification can be found\u00a0<code>here</code>.</p>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#blocks-verification","title":"Blocks Verification","text":"<p>The Ledger MUST certify the last block (tip) recorded. The Ledger MUST allow to download the certificate via the\u00a0<code>icrc3_get_tip_certificate</code>\u00a0endpoint. The certificate follows the\u00a0IC Specification for Certificates. The certificate is comprised of a tree containing the certified data and the signature. The tree MUST contain two labelled values (leafs):</p> <ol> <li><code>last_block_index</code>: the index of the last block in the chain. The values must be expressed as\u00a0<code>leb128</code></li> <li><code>last_block_hash</code>: the hash of the last block in the chain</li> </ol> <p>Clients SHOULD download the tip certificate first and then download the block backward starting from\u00a0<code>last_block_index</code>\u00a0and validate the blocks in the process.</p> <p>Validation of block\u00a0<code>i</code>\u00a0is done by checking the block hash against</p> <ol> <li>if\u00a0<code>i + 1 &lt; len(chain)</code>\u00a0then the parent hash\u00a0<code>phash</code>\u00a0of the block\u00a0<code>i+1</code></li> <li>otherwise the\u00a0<code>last_block_hash</code>\u00a0in the tip certificate.</li> </ol>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#generic-block-schema","title":"Generic Block Schema","text":"<p>An ICRC-3 compliant Block</p> <ol> <li>MUST be a\u00a0<code>Value</code>\u00a0of variant\u00a0<code>Map</code></li> <li>MUST contain a field\u00a0<code>phash: Blob</code>\u00a0which is the hash of its parent if it has a parent block</li> <li>SHOULD contain a field\u00a0<code>btype: String</code>\u00a0which uniquely describes the type of the Block. If this field is not set then the block type falls back to ICRC-1 and ICRC-2 for backward compatibility purposes</li> </ol>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#interaction-with-other-standards","title":"Interaction with other standards","text":"<p>Each standard that adheres to\u00a0<code>ICRC-3</code>\u00a0MUST define the list of block schemas that it introduces. Each block schema MUST:</p> <ol> <li>extend the\u00a0Generic Block Schema</li> <li>specify the expected value of\u00a0<code>btype</code>. This MUST be unique accross all the standards. An ICRC-x standard MUST use namespacing for its op identifiers using the following scheme of using the ICRC standard's number as prefix to the name followed by an operation name that must begin with a letter:</li> </ol> <pre><code>op = icrc_number op_name\nicrc_number = nonzero_digit *digit\nnonzero_digit = \"1\" / \"2\" / \"3\" / \"4\" / \"5\" / \"6\" / \"7\" / \"8\" / \"9\"\ndigit = \"0\" / nonzero_digit\nop_name = a-z *(a-z / digit / \"_\" / \"-\")\n</code></pre> <p>For instance,\u00a0<code>1xfer</code>\u00a0is the identifier of the ICRC-1 transfer operation.</p>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#supported-standards","title":"Supported Standards","text":"<p>An ICRC-3 compatible Ledger MUST expose an endpoint listing all the supported block types via the endpoint\u00a0<code>icrc3_supported_block_types</code>. The Ledger MUST return only blocks with\u00a0<code>btype</code>\u00a0set to one of the values returned by this endpoint.</p>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#icrc-1-and-icrc-2-block-schema","title":"ICRC-1\u00a0and\u00a0ICRC-2\u00a0Block Schema","text":"<p>ICRC-1 and ICRC-2 use the\u00a0<code>tx</code>\u00a0field to store input from the user and use the external block to store data set by the Ledger. For instance, the amount of a transaction is stored in the field\u00a0<code>tx.amt</code>\u00a0because it has been specified by the user, while the time when the block was added to the Ledger is stored in the field\u00a0<code>ts</code>\u00a0because it is set by the Ledger.</p> <p>A generic ICRC-1 or ICRC-2 Block:</p> <ol> <li>it MUST contain a field\u00a0<code>ts: Nat</code>\u00a0which is the timestamp of when the block was added to the Ledger</li> <li>if the operation requires a fee and if the\u00a0<code>tx</code>\u00a0field doesn't specify the fee then it MUST contain a field\u00a0<code>fee: Nat</code>\u00a0which specifies the fee payed to add this block to the Ledger</li> <li>its field\u00a0<code>tx</code><ol> <li>CAN contain a field\u00a0<code>op: String</code>\u00a0that uniquely defines the type of operation</li> <li>MUST contain a field\u00a0<code>amt: Nat</code>\u00a0that represents the amount</li> <li>MUST contain the\u00a0<code>fee: Nat</code>\u00a0field for operations that require a fee if the user specifies the fee in the request. If the user does not specify the fee in the request, then this field is not set and the top-level\u00a0<code>fee</code>\u00a0is set.</li> <li>CAN contain the\u00a0<code>memo: Blob</code>\u00a0field if specified by the user</li> <li>CAN contain the\u00a0<code>ts: Nat</code>\u00a0field if the user sets the\u00a0<code>created_at_time</code>\u00a0field in the request.</li> </ol> </li> </ol> <p>Operations that require paying a fee: Transfer, and Approve.</p> <p>The type of a generic ICRC-1 or ICRC-2 Block is defined by either the field\u00a0<code>btype</code>\u00a0or the field\u00a0<code>tx.op</code>. The first approach is preferred, the second one exists for backward compatibility. If both are specified then\u00a0<code>btype</code>\u00a0defines the type of the block regardless of\u00a0<code>tx.op</code>.</p> <p><code>icrc3_supported_block_types</code>\u00a0should always return all the\u00a0<code>btype</code>s supported by the Ledger even if the Ledger doesn't support the\u00a0<code>btype</code>\u00a0field yet. For example, if the Ledger supports mint blocks using the backward compatibility schema, i.e. without\u00a0<code>btype</code>, then the endpoint\u00a0<code>icrc3_supported_block_types</code>\u00a0will have to return\u00a0<code>\"1mint\"</code>\u00a0among the supported block types.</p>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#account-type","title":"Account Type","text":"<p>ICRC-1 Account is represented as an\u00a0<code>Array</code>\u00a0containing the\u00a0<code>owner</code>\u00a0bytes and optionally the subaccount bytes.</p>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#burn-block-schema","title":"Burn Block Schema","text":"<ol> <li>the\u00a0<code>btype</code>\u00a0field MUST be\u00a0<code>\"1burn\"</code>\u00a0or\u00a0<code>tx.op</code>\u00a0field MUST be\u00a0<code>\"burn\"</code></li> <li>it MUST contain a field\u00a0<code>tx.from: Account</code></li> </ol> <p>Example with\u00a0<code>btype</code>:</p> <pre><code>variant { Map = vec {\n    record { \"btype\"; \"variant\" { Text = \"1burn\" }};\n    record { \"phash\"; variant {\n        Blob = blob \"\\a1\\a9p\\f5\\17\\e5\\e2\\92\\87\\96(\\c8\\f1\\88iM\\0d(tN\\f4-~u\\19\\88\\83\\d8_\\b2\\01\\ec\"\n    }};\n    record { \"ts\"; variant { Nat = 1_701_108_969_851_098_255 : nat }};\n    record { \"tx\"; variant { Map = vec {\n        record { \"amt\"; variant { Nat = 1_228_990 : nat } };\n        record { \"from\"; variant { Array = vec {\n                variant { Blob = blob \"\\00\\00\\00\\00\\020\\00\\07\\01\\01\" };\n                variant { Blob = blob \"&amp;\\99\\c0H\\7f\\a4\\a5Q\\af\\c7\\f4;\\d9\\e9\\ca\\e5 \\e3\\94\\84\\b5c\\b6\\97/\\00\\e6\\a0\\e9\\d3p\\1a\" };\n        }}};\n        record { \"memo\"; variant { Blob = blob \"\\82\\00\\83x\\223K7Bg3LUkiXZ5hatPT1b9h3XxJ89DYSU2e\\19\\07\\d0\\00\"\n        }};\n    }}};\n}};\n</code></pre> <p>Example without\u00a0<code>btype</code>:</p> <pre><code>variant { Map = vec {\n    record { \"phash\"; variant {\n        Blob = blob \"\\a1\\a9p\\f5\\17\\e5\\e2\\92\\87\\96(\\c8\\f1\\88iM\\0d(tN\\f4-~u\\19\\88\\83\\d8_\\b2\\01\\ec\"\n    }};\n    record { \"ts\"; variant { Nat = 1_701_108_969_851_098_255 : nat }};\n    record { \"tx\"; variant { Map = vec {\n        record { \"op\"; variant { Text = \"burn\" } };\n        record { \"amt\"; variant { Nat = 1_228_990 : nat } };\n        record { \"from\"; variant { Array = vec {\n                variant { Blob = blob \"\\00\\00\\00\\00\\020\\00\\07\\01\\01\" };\n                variant { Blob = blob \"&amp;\\99\\c0H\\7f\\a4\\a5Q\\af\\c7\\f4;\\d9\\e9\\ca\\e5 \\e3\\94\\84\\b5c\\b6\\97/\\00\\e6\\a0\\e9\\d3p\\1a\" };\n        }}};\n        record { \"memo\"; variant { Blob = blob \"\\82\\00\\83x\\223K7Bg3LUkiXZ5hatPT1b9h3XxJ89DYSU2e\\19\\07\\d0\\00\"\n        }};\n    }}};\n}};\n</code></pre>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#mint-block-schema","title":"Mint Block Schema","text":"<ol> <li>the\u00a0<code>btype</code>\u00a0field MUST be\u00a0<code>\"1mint\"</code>\u00a0or the\u00a0<code>tx.op</code>\u00a0field MUST be\u00a0<code>\"mint\"</code></li> <li>it MUST contain a field\u00a0<code>tx.to: Account</code></li> </ol> <p>Example with\u00a0<code>btype</code>:</p> <pre><code>variant { Map = vec {\n    record { \"btype\"; \"variant\" { Text = \"1mint\" }};\n    record { \"ts\"; variant { Nat = 1_675_241_149_669_614_928 : nat } };\n    record { \"tx\"; variant { Map = vec {\n        record { \"amt\"; variant { Nat = 100_000 : nat } };\n        record { \"to\"; variant { Array = vec {\n                variant { Blob = blob \"Z\\d0\\ea\\e8;\\04*\\c2CY\\8b\\delN\\ea&gt;]\\ff\\12^. WGj0\\10\\e4\\02\" };\n        }}};\n    }}};\n}};\n</code></pre> <p>Example without\u00a0<code>btype</code>:</p> <pre><code>variant { Map = vec {\n    record { \"ts\"; variant { Nat = 1_675_241_149_669_614_928 : nat } };\n    record { \"tx\"; variant { Map = vec {\n        record { \"op\"; variant { Text = \"mint\" } };\n        record { \"amt\"; variant { Nat = 100_000 : nat } };\n        record { \"to\"; variant { Array = vec {\n                variant { Blob = blob \"Z\\d0\\ea\\e8;\\04*\\c2CY\\8b\\delN\\ea&gt;]\\ff\\12^. WGj0\\10\\e4\\02\" };\n        }}};\n    }}};\n}};\n</code></pre>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#transfer-and-transfer-from-block-schema","title":"Transfer and Transfer From Block Schema","text":"<ol> <li>the\u00a0<code>btype</code>\u00a0field MUST be<ol> <li><code>\"2xfer\"</code>\u00a0for\u00a0<code>icrc2_transfer_from</code>\u00a0blocks</li> <li><code>\"1xfer\"</code>\u00a0for\u00a0<code>icrc1_transfer</code>\u00a0blocks</li> </ol> </li> <li>if\u00a0<code>btype</code>\u00a0is not set then\u00a0<code>tx.op</code>\u00a0field MUST be\u00a0<code>\"xfer\"</code></li> <li>it MUST contain a field\u00a0<code>tx.from: Account</code></li> <li>it MUST contain a field\u00a0<code>tx.to: Account</code></li> <li>it CAN contain a field\u00a0<code>tx.spender: Account</code></li> </ol> <p>Example with\u00a0<code>btype</code>:</p> <pre><code>variant { Map = vec {\n    record { \"btype\"; \"variant\" { Text = \"1xfer\" }};\n    record { \"fee\"; variant { Nat = 10 : nat } };\n    record { \"phash\"; variant { Blob =\n        blob \"h,,\\97\\82\\ff.\\9cx&amp;l\\a2e\\e7KFVv\\d1\\89\\beJ\\c5\\c5\\ad,h\\5c&lt;\\ca\\ce\\be\"\n    }};\n    record { \"ts\"; variant { Nat = 1_701_109_006_692_276_133 : nat } };\n    record { \"tx\"; variant { Map = vec {\n        record { \"amt\"; variant { Nat = 609_618 : nat } };\n        record { \"from\"; variant { Array = vec {\n                variant { Blob = blob \"\\00\\00\\00\\00\\00\\f0\\13x\\01\\01\" };\n                variant { Blob = blob \"\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\" };\n        }}};\n        record { \"to\"; variant { Array = vec {\n            variant { Blob = blob \" \\ef\\1f\\83Zs\\0a?\\dc\\d5y\\e7\\ccS\\9f\\0b\\14a\\ac\\9f\\fb\\f0bf\\f3\\a9\\c7D\\02\" };\n            variant { Blob = blob \"\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\" };\n        }}};\n    }}};\n}};\n</code></pre> <p>Example without\u00a0<code>btype</code>:</p> <pre><code>variant { Map = vec {\n    record { \"fee\"; variant { Nat = 10 : nat } };\n    record { \"phash\"; variant { Blob =\n        blob \"h,,\\97\\82\\ff.\\9cx&amp;l\\a2e\\e7KFVv\\d1\\89\\beJ\\c5\\c5\\ad,h\\5c&lt;\\ca\\ce\\be\"\n    }};\n    record { \"ts\"; variant { Nat = 1_701_109_006_692_276_133 : nat } };\n    record { \"tx\"; variant { Map = vec {\n        record { \"op\"; variant { Text = \"xfer\" } };\n        record { \"amt\"; variant { Nat = 609_618 : nat } };\n        record { \"from\"; variant { Array = vec {\n                variant { Blob = blob \"\\00\\00\\00\\00\\00\\f0\\13x\\01\\01\" };\n                variant { Blob = blob \"\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\" };\n        }}};\n        record { \"to\"; variant { Array = vec {\n            variant { Blob = blob \" \\ef\\1f\\83Zs\\0a?\\dc\\d5y\\e7\\ccS\\9f\\0b\\14a\\ac\\9f\\fb\\f0bf\\f3\\a9\\c7D\\02\" };\n            variant { Blob = blob \"\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\" };\n        }}};\n    }}};\n}};\n</code></pre>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#approve-block-schema","title":"Approve Block Schema","text":"<ol> <li>the\u00a0<code>btype</code>\u00a0field MUST be\u00a0<code>\"2approve\"</code>\u00a0or\u00a0<code>tx.op</code>\u00a0field MUST be\u00a0<code>\"approve\"</code></li> <li>it MUST contain a field\u00a0<code>tx.from: Account</code></li> <li>it MUST contain a field\u00a0<code>tx.spender: Account</code></li> <li>it CAN contain a field\u00a0<code>tx.expected_allowance: Nat</code>\u00a0if set by the user</li> <li>it CAN contain a field\u00a0<code>tx.expires_at: Nat</code>\u00a0if set by the user</li> </ol> <p>Example with\u00a0<code>btype</code>:</p> <pre><code>variant { Map = vec {\n    record { \"btype\"; \"variant\" { Text = \"2approve\" }};\n    record { \"fee\"; variant { Nat = 10 : nat } };\n    record { \"phash\"; variant {\n        Blob = blob \";\\f7\\bet\\b6\\90\\b7\\ea2\\f4\\98\\a5\\b0\\60\\a5li3\\dcXN\\1f##2\\b5\\db\\de\\b1\\b3\\02\\f5\"\n    }};\n    record { \"ts\"; variant { Nat = 1_701_167_840_950_358_788 : nat } };\n    record { \"tx\"; variant { Map = vec {\n        record { \"amt\"; variant { Nat = 18_446_744_073_709_551_615 : nat } };\n        record { \"from\"; variant { Array = vec {\n                variant { Blob = blob \"\\16c\\e1\\91v\\eb\\e5)\\84:\\b2\\80\\13\\cc\\09\\02\\01\\a8\\03[X\\a5\\a0\\d3\\1f\\e4\\c3{\\02\" };\n        }}};\n        record { \"spender\"; variant { Array = vec {\n            variant { Blob = blob \"\\00\\00\\00\\00\\00\\e0\\1dI\\01\\01\" };\n        }}};\n    }}};\n}}};\n</code></pre> <p>Example without\u00a0<code>btype</code>:</p> <pre><code>variant { Map = vec {\n    record { \"fee\"; variant { Nat = 10 : nat } };\n    record { \"phash\"; variant {\n        Blob = blob \";\\f7\\bet\\b6\\90\\b7\\ea2\\f4\\98\\a5\\b0\\60\\a5li3\\dcXN\\1f##2\\b5\\db\\de\\b1\\b3\\02\\f5\"\n    }};\n    record { \"ts\"; variant { Nat = 1_701_167_840_950_358_788 : nat } };\n    record { \"tx\"; variant { Map = vec {\n        record { \"op\"; variant { Text = \"approve\" } };\n        record { \"amt\"; variant { Nat = 18_446_744_073_709_551_615 : nat } };\n        record { \"from\"; variant { Array = vec {\n                variant { Blob = blob \"\\16c\\e1\\91v\\eb\\e5)\\84:\\b2\\80\\13\\cc\\09\\02\\01\\a8\\03[X\\a5\\a0\\d3\\1f\\e4\\c3{\\02\" };\n        }}};\n        record { \"spender\"; variant { Array = vec {\n            variant { Blob = blob \"\\00\\00\\00\\00\\00\\e0\\1dI\\01\\01\" };\n        }}};\n    }}};\n}}};\n</code></pre>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#specification","title":"Specification","text":""},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#icrc3_get_blocks","title":"<code>icrc3_get_blocks</code>","text":"<pre><code>type Value = variant {\n    Blob : blob;\n    Text : text;\n    Nat : nat;\n    Int : int;\n    Array : vec Value;\n    Map : vec record { text; Value };\n};\n\ntype GetArchivesArgs = record {\n    // The last archive seen by the client.\n    // The Ledger will return archives coming\n    // after this one if set, otherwise it\n    // will return the first archives.\n    from : opt principal;\n};\n\ntype GetArchivesResult = vec record {\n    // The id of the archive\n    canister_id : principal;\n\n    // The first block in the archive\n    start : nat;\n\n    // The last block in the archive\n    end : nat;\n};\n\ntype GetBlocksArgs = vec record { start : nat; length : nat };\n\ntype GetBlocksResult = record {\n    // Total number of blocks in the block log\n    log_length : nat;\n\n    // Blocks found locally to the Ledger\n    blocks : vec record { id : nat; block: Value };\n\n    // List of callbacks to fetch the blocks that are not local\n    // to the Ledger, i.e. archived blocks\n    archived_blocks : vec record {\n        args : GetBlocksArgs;\n        callback : func (GetBlocksArgs) -&gt; (GetBlocksResult) query;\n    };\n};\n\nservice : {\n    icrc3_get_archives : (GetArchivesArgs) -&gt; (GetArchivesResult) query;\n    icrc3_get_blocks : (GetBlocksArgs) -&gt; (GetBlocksResult) query;\n};\n</code></pre>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#icrc3_get_tip_certificate","title":"<code>icrc3_get_tip_certificate</code>","text":"<pre><code>// See https://internetcomputer.org/docs/current/references/ic-interface-spec#certification\ntype DataCertificate = record {\n\n  // Signature of the root of the hash_tree\n  certificate : blob;\n\n  // CBOR encoded hash_tree\n  hash_tree : blob;\n};\n\nservice : {\n  icrc3_get_tip_certificate : () -&gt; (opt DataCertificate) query;\n};\n</code></pre>"},{"location":"motion-proposals/128824%20The%20ICRC-3%20Block%20Log%20Standard.html#icrc3_supported_block_types","title":"<code>icrc3_supported_block_types</code>","text":"<pre><code>service : {\n    icrc3_supported_block_types : () -&gt; (vec record { block_type : text; url : text }) query;\n};\n</code></pre>"},{"location":"motion-proposals/129625%20ICRC-10%20Supported%20Standards%20Generalization.html","title":"ICRC-10 Supported Standards Generalization","text":"ICRC Title Author Discussions Status Type Category Created 10 Supported Standards Generalization Austin Fatheree (@skilesare), Dieter Sommer (@dietersommer), Mario Pastorelli (@MarioDfinity) Issue 10 Draft Standards Track 2024-03-05 <p>ICRC-10 is a standard aimed at simplifying the discovery of the supported standards of canisters on the Internet Computer. By providing a unified method,\u00a0<code>icrc10_supported_standards</code>, canisters can easily expose the standards they implement, enhancing interoperability and easing integration efforts across the ecosystem.</p>"},{"location":"motion-proposals/129625%20ICRC-10%20Supported%20Standards%20Generalization.html#what-does-the-standard-imply","title":"What does the standard imply?","text":"<p>ICRC-10 is a contract between organizations participating in the Ledger &amp; Tokenization working group and the community. If the standard is accepted, we agree to</p> <ol> <li>Provide a production-ready implementation of the standard.</li> <li>Support the standard in our existing and future products.</li> <li>Build tools to interact with standard-compliant implementations.</li> <li>Promote the standard.</li> <li>Design extensions to the standard to simplify and scale payment flows.</li> </ol> <p>Accepting the ICRC-10 standard does not imply that all other standards should be considered obsolete. Everyone is free to experiment with new designs, application interfaces, and products.</p> <p>The main goal of ICRC-10 is to provide a solid common ground for interoperability.</p>"},{"location":"motion-proposals/129625%20ICRC-10%20Supported%20Standards%20Generalization.html#data","title":"Data","text":"<p>The\u00a0<code>icrc10_supported_standards</code>\u00a0method returns a list of standardized records, each corresponding to a standard supported by the canister. The response record format of this method is structured as follows:</p> <pre><code>type SupportedStandard = record { name : text; url : text; };\ntype SupportedStandardsResponse = vec SupportedStandard;\n</code></pre>"},{"location":"motion-proposals/129625%20ICRC-10%20Supported%20Standards%20Generalization.html#name","title":"name","text":"<p>For ICRC standards the text SHOULD be in the form\u00a0<code>ICRC-X</code>\u00a0where\u00a0<code>X</code>\u00a0is the official ICRC number assigned at\u00a0https://github.com/dfinity/ICRC/issues. New ICRC numbers can be procured by filing an issue at\u00a0https://github.com/dfinity/ICRC/issues/new. The resulting issue number MUST be used for numbering the new ICRC standard.</p> <p>For non-ICRC standards, the text SHOULD be a human-readable namespace with a small likelihood of collision. The Internet Computer shares a global namespace. Please be polite to other developers and make namespaces specific. See\u00a0https://forum.dfinity.org/t/prefix-all-the-methods-of-the-icrc-1-token-standard-with-icrc1/13865\u00a0for more discussion.</p> <p>Suggestions:</p> <ul> <li>com.search.searchable</li> <li>org.icdevs.donatable</li> <li>eth.ecrc20.addressable</li> </ul>"},{"location":"motion-proposals/129625%20ICRC-10%20Supported%20Standards%20Generalization.html#url","title":"url","text":"<p>The url SHOULD point to a stable URL that explains the referenced standard.</p>"},{"location":"motion-proposals/129625%20ICRC-10%20Supported%20Standards%20Generalization.html#methods","title":"Methods","text":""},{"location":"motion-proposals/129625%20ICRC-10%20Supported%20Standards%20Generalization.html#icrc10_supported_standards","title":"icrc10_supported_standards","text":"<p>Standard developers MUST implement the\u00a0<code>icrc10_supported_standards</code>\u00a0query endpoint.</p> <p>The method returns a list of standards supported by the canister.</p> <p>The method is used for discovering which APIs or protocols a given canister complies with, making it easier for developers and applications to interact with a broad range of services on the Internet Computer without prior detailed knowledge of each service's capabilities.</p> <pre><code>icrc10_supported_standards : () -&gt; (SupportedStandardsResponse) query;\n</code></pre> <p>The result MUST include entries for\u00a0<code>ICRC-1</code>\u00a0if the canister supports these standards. This mandate ensures that clients querying for supported standards can upgrade to ICRC-10 for discoverability.</p> <p>The result MUST include a self-reference for\u00a0<code>record{name=\"ICRC-10\"; url=\"https://github.com/dfinity/ICRCs/ICRC-10\"}</code>.</p> <p>Additionally, the result SHOULD include entries for other supported standards, offering a flexible and extensible way to advertise capabilities.</p>"},{"location":"motion-proposals/129625%20ICRC-10%20Supported%20Standards%20Generalization.html#rationale","title":"Rationale","text":"<p>The rationale behind the\u00a0<code>icrc10_supported_standards</code>\u00a0method over implementing similar endpoints within ICRC-1 individually is that it alleviates the need for ICRC standard writers to include custom specifications for a feature retrieval endpoint. This approach streamlines the integration process, broadens the compatibility across different protocols, and fosters a more interconnected and versatile ecosystem.</p> <p>In effect,\u00a0<code>icrc10_supported_standards</code>\u00a0serves as a universal discovery mechanism, enabling canisters to succinctly communicate their supported interfaces. This capability is particularly valuable in a decentralized environment like the Internet Computer, where discovering and leveraging the functionalities of various services can significantly enhance application development and user experiences.</p> <p>Through standardization of the\u00a0<code>icrc10_supported_standards</code>\u00a0method, ICRC-10 aims to simplify interoperability, reduce implementation complexity, and foster a more vibrant and accessible ecosystem of services on the Internet Computer.</p>"},{"location":"motion-proposals/129625%20ICRC-10%20Supported%20Standards%20Generalization.html#migration-path-for-ledgers-using-icrc-1","title":"Migration Path for Ledgers Using ICRC-1","text":"<p>Ledgers and other services that already exist, to the extent that they are capable, SHOULD implement ICRC-10 to increase interoperability on the Internet Computer.</p> <p>For maintaining interoperability with ICRC-1, existing ICRC-1 ledgers MUST maintain their implementation of\u00a0<code>icrc1_supported_standards</code>. The new\u00a0<code>icrc10_supported_standards</code>\u00a0method SHOULD return at least the same data, but MAY be augmented with additional ICRC features or supported standards.</p>"},{"location":"motion-proposals/129625%20ICRC-10%20Supported%20Standards%20Generalization.html#versioning","title":"Versioning","text":"<p>Generally, new versions and updates of existing standards SHOULD be assigned a new ICRC number as opposed to attempting to support versioning through this interface.</p>"},{"location":"motion-proposals/129625%20ICRC-10%20Supported%20Standards%20Generalization.html#future-work","title":"Future Work","text":"<p>A potential area for improvement is the establishment of a registry or directory service on the Internet Computer that indexes canisters by the standards they support, using the ICRC-10 mechanism. Such a service could dramatically improve discoverability and foster a more interconnected ecosystem of canisters.</p>"},{"location":"motion-proposals/129625%20ICRC-10%20Supported%20Standards%20Generalization.html#interface","title":"Interface","text":"<pre><code>type SupportedStandard = record { name : text; url : text; };\ntype SupportedStandardsResponse = vec SupportedStandard;\n\nservice : {\n  icrc10_supported_standards : () -&gt; (SupportedStandardsResponse) query;\n}\n</code></pre>"},{"location":"motion-proposals/129626%20ICRC-7%20Minimal%20Non-Fungible%20Token%20%28NFT%29%20Standard.html","title":"129626 ICRC 7 Minimal Non Fungible Token (NFT) Standard","text":"ICRC Title Author Discussions Status Type Category Created 7 Minimal Non-Fungible Token (NFT) Standard Ben Zhai (@benjizhai), Austin Fatheree (@skilesare), Dieter Sommer (@dietersommer), Thomas (@sea-snake), Moritz Fuller (@letmejustputthishere), Matthew Harmon https://github.com/dfinity/ICRC/issues/7 Draft Standards Track 2023-01-31"},{"location":"motion-proposals/129626%20ICRC-7%20Minimal%20Non-Fungible%20Token%20%28NFT%29%20Standard.html#icrc-7-minimal-non-fungible-token-nft-standard","title":"ICRC-7: Minimal Non-Fungible Token (NFT) Standard","text":"<p>ICRC-7 is the minimal standard for the implementation of Non-Fungible Tokens (NFTs) on the\u00a0Internet Computer.</p> <p>A token ledger implementation following this standard hosts an\u00a0NFT collection\u00a0(collection), which is a set of NFTs.</p> <p>ICRC-7 does not handle approval-related operations such as\u00a0<code>approve</code>\u00a0and\u00a0<code>transfer_from</code>\u00a0itself. Those operations are specified by ICRC-37 which extends ICRC-7 with approval semantics.</p>"},{"location":"motion-proposals/129626%20ICRC-7%20Minimal%20Non-Fungible%20Token%20%28NFT%29%20Standard.html#what-does-the-standard-imply","title":"What does the standard imply?","text":"<p>ICRC-7 is a contract between organizations participating in the Ledger &amp; Tokenization working group and the community. If the standard is accepted, we agree to</p> <ol> <li>Provide a production-ready implementation of the standard.</li> <li>Support the standard in our existing and future products.</li> <li>Build tools to interact with standard-compliant implementations.</li> <li>Promote the standard.</li> <li>Design extensions to the standard to simplify and scale payment flows.</li> </ol> <p>Accepting the ICRC-7 standard does not imply that all other standards should be considered obsolete. Everyone is free to experiment with new designs, application interfaces, and products.</p> <p>The main goal of ICRC-7 is to provide a solid common ground for interoperability.</p>"},{"location":"motion-proposals/129626%20ICRC-7%20Minimal%20Non-Fungible%20Token%20%28NFT%29%20Standard.html#specification","title":"Specification","text":"<p>The specification of ICRC-7 is too big to fit into a motion proposal. The full ICRC-7 specification in the version to be voted on can be found at\u00a0https://github.com/dfinity/ICRC/tree/424402830cbcc2def46112556d13ac5c5ab41c0c/ICRCs/ICRC-7.</p>"},{"location":"motion-proposals/129640%20ICRC-37%20Approval%20Support%20for%20the%20Minimal%20Non-Fungible%20Token%20%28NFT%29%20Standard.html","title":"129640 ICRC 37 Approval Support for the Minimal Non Fungible Token (NFT) Standard","text":"ICRC Title Author Discussions Status Type Category Created 37 Approval Functionality for the ICRC-7 Non-Fungible Token (NFT) Standard Ben Zhai (@benjizhai), Austin Fatheree (@skilesare), Dieter Sommer (@dietersommer), Thomas (@sea-snake), Moritz Fuller (@letmejustputthishere), Matthew Harmon https://github.com/dfinity/ICRC/issues/37 Draft Standards Track 2023-11-22"},{"location":"motion-proposals/129640%20ICRC-37%20Approval%20Support%20for%20the%20Minimal%20Non-Fungible%20Token%20%28NFT%29%20Standard.html#icrc-37-approval-support-for-the-minimal-non-fungible-token-nft-standard","title":"ICRC-37: Approval Support for the Minimal Non-Fungible Token (NFT) Standard","text":"<p>This document specifies approval support for the\u00a0ICRC-7 minimal NFT standard for the Internet Computer. It defines all the methods required for realizing approval semantics for an NFT token ledger, i.e., creating approvals, revoking approvals, querying approval information, and making transfers based on approvals. The scope of ICRC-37 has been part of ICRC-7 originally, however, the NFT Working Group has decided to split it out into a separate standard for the following reasons:</p> <ul> <li>ICRC-7 and ICRC-37 are much shorter and hence easier to navigate on their own due to their respective foci;</li> <li>Ledgers that do not want to implement approval and transfer from semantics do not need to provide dummy implementations of the corresponding methods that fail by default.</li> </ul> <p>This standard extends the ICRC-7 NFT standard and is intended to be implemented by token ledgers that implement ICRC-7. An ICRC-7 ledger may implement ICRC-37 in case it intends to offer approve and transfer from semantics. Principles put forth in ICRC-7 apply to ICRC-37 as well, e.g., the design of the update and query API.</p> <p>The keywords \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119.</p> <p>Canisters implementing the\u00a0<code>ICRC-37</code>\u00a0standard MUST implement all the functions in the\u00a0<code>ICRC-7</code>\u00a0interface.</p> <p>Canisters implementing the\u00a0<code>ICRC-37</code>\u00a0standard MUST include\u00a0<code>ICRC-37</code>\u00a0in the list returned by the\u00a0<code>icrc7_supported_standards</code>\u00a0method.</p>"},{"location":"motion-proposals/129640%20ICRC-37%20Approval%20Support%20for%20the%20Minimal%20Non-Fungible%20Token%20%28NFT%29%20Standard.html#what-does-the-standard-imply","title":"What does the standard imply?","text":"<p>ICRC-37 is a contract between organizations participating in the Ledger &amp; Tokenization working group and the community. If the standard is accepted, we agree to</p> <ol> <li>Provide a production-ready implementation of the standard.</li> <li>Support the standard in our existing and future products.</li> <li>Build tools to interact with standard-compliant implementations.</li> <li>Promote the standard.</li> <li>Design extensions to the standard to simplify and scale payment flows.</li> </ol> <p>Accepting the ICRC-37 standard does not imply that all other standards should be considered obsolete. Everyone is free to experiment with new designs, application interfaces, and products.</p> <p>The main goal of ICRC-37 is to provide a solid common ground for interoperability.</p>"},{"location":"motion-proposals/129640%20ICRC-37%20Approval%20Support%20for%20the%20Minimal%20Non-Fungible%20Token%20%28NFT%29%20Standard.html#specification","title":"Specification","text":"<p>The specification of ICRC-37 is too big to fit into a motion proposal. The full ICRC-37 specification in the version to be voted on can be found at\u00a0https://github.com/dfinity/ICRC/tree/424402830cbcc2def46112556d13ac5c5ab41c0c/ICRCs/ICRC-37.</p>"},{"location":"motion-proposals/129727%20Reevaluating%20Neuron%20Control%20Restrictions.html","title":"129727 Reevaluating Neuron Control Restrictions","text":""},{"location":"motion-proposals/129727%20Reevaluating%20Neuron%20Control%20Restrictions.html#tldr","title":"TL;DR","text":"<p>The Internet Computer Protocol prevents canisters from directly controlling neurons, a measure intended to block neuron sales and ensure long-term thinking in voting behavior. However, these restrictions can be circumvented by a canister using threshold ECDSA (tECDSA) and HTTP outcalls. This suggests a need to reconsider the restrictions and their effectiveness.</p> <p>This proposal recommends lifting restrictions on canister neuron control in the NNS and monitoring their materiality through newly developed metrics. A threshold is set to initiate mitigation measures if canister-controlled neurons exceed 10% of total voting power. Additional measures to disincentive neuron transfers will be considered and implemented if this threshold is reached, balancing security enhancements with user complexity and implementation effort.</p>"},{"location":"motion-proposals/129727%20Reevaluating%20Neuron%20Control%20Restrictions.html#background","title":"Background","text":"<p>Restrictions on Neuron Control</p> <p>In the ICP network, neurons are decision-making entities created through the staking of ICP tokens. These neurons participate in governance by voting on proposals. To promote long-term decision-making, users are incentivized to stake their tokens for several years. Additionally, control over neurons is deliberately restricted: Currently only so-called\u00a0self-authenticating principals\u00a0can be set as the controller of a neuron. A self-authenticating principal is an entity that utilizes its own cryptographic key pair (consisting of a private and a public key) to authenticate itself; the idea being that control over the underlying private key cannot be transferred without full trust. For example, a user relying on Internet Identity, Quill or a Ledger hardware wallet is of this kind. Canisters, which do not possess self-authenticating principals, are therefore excluded from directly controlling neurons.</p> <p>Reason for the Restrictions on Neuron Control</p> <p>The restriction is based on the requirement that neurons should not be sold - when canisters can control neurons directly, one can sell a neuron by selling the canister that controls it. It is considered important for neurons to be non-transferable/no-sellable because</p> <ul> <li>neurons should have an incentive to vote in the long-term interest of the Internet Computer and</li> <li>to avoid the possibility of attacks where an attacker acquires tokens only for a short amount of time, votes on a malicious proposal (e.g. transfer tokens) and then sells the neurons again.</li> </ul> <p>Circumventing Restrictions on Neuron Control</p> <p>Despite these safeguards, there are a few ways to bypass the restrictions on neuron control:</p> <ul> <li>Threshold ECDSA (tECDSA): As canisters are able to control tECDSA keys, which are a feature of the Internet Computer Protocol, they can also sign ingress messages to the Internet Computer and thereby act as the controller of a neuron (making calls via HTTP to appear as ingress).</li> <li>Canister signatures: A canister can control a neuron through canister signatures, again making calls via HTTP to appear as ingress.</li> </ul> <p>Furthermore, it is also possible to control a neuron via an Internet Identity (II) and\u00a0sell the II.</p>"},{"location":"motion-proposals/129727%20Reevaluating%20Neuron%20Control%20Restrictions.html#revisiting-neuron-control-restrictions","title":"Revisiting Neuron Control Restrictions","text":"<p>Given that the restriction for canisters to not control neurons can be circumvented relatively easily, it should be considered to drop that restriction. This was suggested by several ICP community members already. Lifting that restriction would bring the following benefits</p> <ul> <li>Facilitate NNS neurons that are SNS controlled: SNSs already chose to do this (e.g. OpenChat, GoldDAO), providing them a continuous income to cover cycles fees and involving SNSs in NNS governance. Allowing direct canister control of neurons would simplify the current more complicated workflow via tECDSA.</li> <li>Consistency with SNS: As opposed to the NNS, the SNS framework does not apply restrictions on neuron controllers.</li> <li>Facilitate organizational neuron ownership: An organization could control a neuron via a canister.</li> </ul> <p>A key point discussed in the\u00a0forum\u00a0is the issue of materiality: As long as only a small number of neurons are controlled by canisters or through II, this does not significantly impact long-term voting behavior of the NNS overall.</p>"},{"location":"motion-proposals/129727%20Reevaluating%20Neuron%20Control%20Restrictions.html#suggested-way-forward","title":"Suggested Way Forward","text":"<p>Acknowledging the potential for circumvention of the current mechanism, it is recommended to lift the restrictions on canister neuron control while monitoring the materiality of canister-controlled neurons, via the following steps:</p> <ul> <li>Lift restrictions on neuron control: Remove existing restrictions, allowing canisters to control NNS neurons.</li> <li>Implement new metrics: Develop and implement metrics in the NNS governance canister that track the total stake and voting power of canister-controlled neurons on a daily basis. This should include the proportion of canister-controlled voting power relative to the total voting power.</li> <li>Establish a materiality threshold: Set a threshold that triggers mitigation measures if canister-controlled neurons exceed 10% of the total voting power. The threshold might be adjusted at a later point in time. For example, the threshold might be increased (upon NNS approval) following a materiality analysis of canister-controlled neurons belonging to DAOs, which are not considered to be an issue.</li> <li>Delayed implementation of additional measures: Additional disincentives for neuron transfers will be implemented if the materiality threshold is surpassed. While further measures could enhance security, they would also increase complexity for users and require significant implementation effort. The specifics of these mitigation measures can be determined as the situation evolves and will be subject to a separate motion proposal. For instance, these disincentives would reduce the rewards for transferable neurons created after this proposal is passed; to avoid the reduction, non-transferability of neurons could be shown via a proof of knowledge of a cryptographic key controlling the neuron.</li> </ul>"},{"location":"motion-proposals/130832%20Public%20and%20private%20neurons%20%26%20neuron%20index.html","title":"130832 Public and private neurons & neuron index","text":""},{"location":"motion-proposals/130832%20Public%20and%20private%20neurons%20%26%20neuron%20index.html#tldr","title":"TL;DR","text":"<p>Introduce public and private neurons and a neuron index</p> <ul> <li>Public neurons provide more transparency to followers</li> <li>Private neurons keep ballots confidential</li> <li>More transparency by revealing all neurons\u2019 IDs and their stake</li> </ul>"},{"location":"motion-proposals/130832%20Public%20and%20private%20neurons%20%26%20neuron%20index.html#current-situation","title":"Current situation","text":"<p>For a neuron whose neuron ID is known, there is some information that everyone can get about the neuron, including:</p> <ul> <li>The neuron\u2019s stake and voting power</li> <li>The neuron\u2019s voting power bonus in the form of the dissolve delay and age (and the neuron\u2019s creation time)</li> <li>The neuron\u2019s ballots</li> <li>Whether the neuron is a member of the Neurons\u2019 Fund</li> </ul> <p>Not all neuron IDs of neurons in the NNS governance are known today.</p> <p>The reason for this is that whenever a new neuron is created, it gets a new random neuron ID. This neuron ID is only known to the user that created the neuron and to the NNS.</p> <p>This has the following consequences.</p> <ol> <li>If a\u00a0user would like to hide information about their neuron, they can keep their neuron ID secret.\u00a0This is an important feature to some users who would like to keep their ballots private, in order to vote freely and not fear consequences. For these neurons whose IDs are kept secret, no information is publicly known.</li> <li>It is\u00a0not possible today to build an index of all neurons. The community has adopted a proposal to build a neuron index in NNS governance (https://dashboard.internetcomputer.org/proposal/48491). Once implemented, this allows users to get a comprehensive overview of how much each neuron stakes, allowing for example to get more insights with respect to how much stake is locked for how long.</li> <li>If a user\u00a0would like to show details about their neuron to the public, there is no easy way to do so. This means for example, that followers of a known neuron cannot easily query the NNS to learn settings of the known neuron, such as who the known neuron follows on different topics.</li> </ol>"},{"location":"motion-proposals/130832%20Public%20and%20private%20neurons%20%26%20neuron%20index.html#proposed-new-design","title":"Proposed new design","text":"<p>This proposal suggests a new design that addresses all needs above. For all neurons, it provides more transparency with respect to the stake. At the same time it maintains the users\u2019 choice how much they would like to reveal with respect to their ballots where this is requested. Finally, for users who would like to reveal more information about their neuron, the new design provides an easy way to do so.</p>"},{"location":"motion-proposals/130832%20Public%20and%20private%20neurons%20%26%20neuron%20index.html#details-of-the-new-design","title":"Details of the new design","text":"<ul> <li>For public neurons, everyone can read all fields of the neuron, except for the ICP ledger account ID. The reason for this exception is that this is particularly sensitive and can be used for in-depth ledger tracing.</li> <li>For private neurons, everyone can read the following fields<ul> <li>Neuron ID</li> <li>The neuron\u2019s stake and voting power</li> <li>The neuron\u2019s voting power bonus in the form of the dissolve delay and age (and the neuron\u2019s creation time)</li> <li>The neuron\u2019s type indicating that the neuron is a seed or ECT neuron</li> </ul> </li> <li>A neuron index in NNS governance lists all private and public neurons.</li> <li>Each neuron can choose to be private or public and change this at any point in time. The known neurons are public while all other neurons are set to private by default.</li> </ul> <p>This design has the following consequences regarding the above mentioned user needs.</p> <ol> <li>If a user would like to keep their voting behavior confidential, they can set their neuron to be private.</li> <li>The neuron index provides more transparency by listing all neurons and important information about them, such as how much they stake.</li> <li>If a user would like to show details about their neurons to the public, they can set their neuron to be public. This allows any neuron, but especially known neurons, to provide additional transparency about their behavior to their followers.</li> </ol>"},{"location":"motion-proposals/130832%20Public%20and%20private%20neurons%20%26%20neuron%20index.html#resulting-properties","title":"Resulting properties","text":"<ul> <li>This design increases the transparency with respect to the neurons\u2019 stake and voting power.</li> <li>This design maintains the main verifiability properties of the voting process.<ul> <li>As is the case now, each user can verify that their vote was submitted (individual verifiability) and all users and outsiders can verify that the tallying process is correct by verifying the code (universal verifiability).</li> <li>With respect to ballot visibility, the new design can make some ballots visible which are not visible now (unknown neuron IDs that chose to be public) and vice versa (neurons who choose to be private).</li> </ul> </li> </ul>"},{"location":"motion-proposals/131010%20Reduce%20the%20minimum%20dissolve%20delay%20to%203%20months.html","title":"131010 Reduce the minimum dissolve delay to 3 months","text":""},{"location":"motion-proposals/131010%20Reduce%20the%20minimum%20dissolve%20delay%20to%203%20months.html#tldr-reduce-the-minimum-dissolve-delay-for-staking-on-the-nns-to-3-months","title":"TL;DR\u00a0Reduce the minimum dissolve delay for staking on the NNS to 3 months","text":"<p>Following @dominicwilliams post on\u00a0possible optimizations of the NNS tokenomics\u00a0and @bjoernek analysis on\u00a0possible optimizations of tokenomics, it seems clear that reducing the minimum dissolve delay from 6 months to 3 months would benefit the IC. The feedback from the forum was positive, hence this proposal submission.</p> <p>@bjoernek conclusion on this proposal was the following:</p> <p>Proposal 1: Reduced Dissolve Delays\u00a0The analysis in this post shows that he primary method to balance supply and demand within the ICP ecosystem remains the growth of the cycle burn rate. Adjustments to inflation are likely to have a lesser impact. Reacting to community concerns, Proposal 1 was adjusted with the inclusion of an opt-in mechanism. Assuming that 25% of neurons opt in, the proposed changes would result in a modest 3.8% reduction in voting rewards. Given this limited impact, pursuing this proposal further may not be worthwhile. It might be sensible to revisit the voting reward function at a later point in time. However, a specific element of the proposal\u2014reducing the minimum dissolve delay to three months\u2014could be considered separately. This adjustment aims to attract new stakers by lowering the barrier to entry and merits discussion in a dedicated forum thread.</p> <p>Another effect of that change is making it easier to go in and out of staking, which could benefit liquid staking protocols like @WaterNeuron.</p> <p>Following a quick analysis, the\u00a0APY for locking ICP for 3 months would be 7.66%.</p>"},{"location":"motion-proposals/131010%20Reduce%20the%20minimum%20dissolve%20delay%20to%203%20months.html#suggested-implementation","title":"Suggested Implementation","text":"<p>It seems that this change is pretty straightforward and would only require one change to the constant\u00a0here.</p> <p>All the frontends and other places where this constant is used would need to update this constant as well.</p>"},{"location":"motion-proposals/131786%20Voting%20grant%20application%20of%20Zenith%20Code%20LLC%20for%20topic%20IC%20OS%20Version%20Election.html","title":"131786 Voting grant application of Zenith Code LLC for topic IC OS Version Election","text":""},{"location":"motion-proposals/131786%20Voting%20grant%20application%20of%20Zenith%20Code%20LLC%20for%20topic%20IC%20OS%20Version%20Election.html#background","title":"Background","text":"<p>As part of an ongoing effort to to incentivise neurons to verify proposals in depth,\u00a0voting grants will be provided for candidates selected by the community. Overall, 2 candidates are selected for each of 4 proposal topics.\u00a0This is one of multiple motion proposals submitted today to select these candidates:</p> <ul> <li>one NNS motion proposal is submitted for each candidate and topic for which the candidate applied\u00a0</li> <li>for each proposal, the result is computed as the number of YES minus the number of no votes (#YES-#NO)</li> <li>for each topic, the 2 candidates with the highest results get the grants</li> </ul> <p>You can read more about the motivation and the process\u00a0here.</p>"},{"location":"motion-proposals/131786%20Voting%20grant%20application%20of%20Zenith%20Code%20LLC%20for%20topic%20IC%20OS%20Version%20Election.html#proposal-topic","title":"Proposal Topic","text":"<p>IC OS Version Election, Protocol Canister Management</p>"},{"location":"motion-proposals/131786%20Voting%20grant%20application%20of%20Zenith%20Code%20LLC%20for%20topic%20IC%20OS%20Version%20Election.html#is-this-a-team-grant-application","title":"Is this a team grant application?","text":"<p>yes</p>"},{"location":"motion-proposals/131786%20Voting%20grant%20application%20of%20Zenith%20Code%20LLC%20for%20topic%20IC%20OS%20Version%20Election.html#application-text","title":"Application Text","text":"<p>Hi @lara and Everyone,</p> <p>I am Manvick, founder of Zenith Code LLC, and my team are excited to apply for the \u201cIC OS Version Election\u201d grant. We have been actively involved in the IC community since mid 2022. We have in-depth knowledge of creating dapps on ICP including but not limited to writing smart contracts in Motoko and architecting/designing multi canister apps on ICP. We are also a node provider on the ICP network.</p> <p>Node Provider:\u00a0https://dashboard.internetcomputer.org/provider/pa5mu-yxsey-b4yrk-bodka-dhjnm-a3nx4-w2grw-3b766-ddr6e-nupu4-pqe</p>"},{"location":"motion-proposals/131786%20Voting%20grant%20application%20of%20Zenith%20Code%20LLC%20for%20topic%20IC%20OS%20Version%20Election.html#about-zenith-code-and-our-team","title":"About Zenith Code and Our Team","text":"<p>Team Members and relevant experience:</p> <ul> <li> <p>Manvick</p> <ul> <li>Software Engineer by background with over 8+ years of experience in large scale Distributed systems and applications with 5+ years of experience in blockchain technologies.</li> <li>Actively worked on Nutanix distributed software defined storage and I was responsible for designing and developing the service which provided deduplication, data resilience, and fault tolerance in the node cluster.</li> <li>Education: Master\u2019s in Computer Science from Texas State University, USA</li> <li>LinkedIn:\u00a0https://www.linkedin.com/in/manvick-paliwal-67441676/</li> <li> <p>Ipsita Parida</p> </li> <li> <p>7+ years of experience in designing and implementing distributed systems including cryptography while at RSA, and AI/ML based projects..</p> </li> <li>Education: Master\u2019s in System design engineering from University of Waterloo, Canada</li> <li>LinkedIn:\u00a0https://www.linkedin.com/in/ipsitaparida/</li> <li>Github:\u00a0https://github.com/ipsitaparida?tab=repositories</li> <li> <p>Harsh Patni (Ex Microsoft Software Engineer)</p> </li> <li> <p>8+ years of experience in software engineering</p> </li> <li>Education: Master\u2019s in Computer Science from Texas State University, USA</li> <li>LinkedIn:\u00a0https://www.linkedin.com/in/hpatni/</li> <li> <p>Yuvika Khardenavis</p> </li> <li> <p>Software Engineer, University of Waterloo graduate</p> </li> <li>4+ years of experience in Firmware and OS level design and development.</li> <li>LinkedIn:\u00a0https://www.linkedin.com/in/ykharden/</li> <li> <p>Vishal Kumar</p> </li> <li> <p>8+ years of experience leading software development teams in agile frameworks over various technology stacks.</p> </li> <li>Education: Master\u2019s in Computer Science from Texas State University, USA</li> <li>LinkedIn:\u00a0https://www.linkedin.com/in/vishalkumarmg/</li> <li> <p>Yamini K</p> </li> <li> <p>5+ years of hands-on experience in full stack software development in technologies like ReactJS, GraphQL.</p> </li> <li>LinkedIn:\u00a0https://www.linkedin.com/in/yaminichowdary/</li> </ul> </li> </ul> <p>Application Details</p> <p>Topic(s) we Apply For:</p> <ol> <li>IC OS Version Election</li> <li>Protocol Canister Management</li> </ol> <p>Team Grant: Yes</p> <p>Size of our Team: 6 members</p>"},{"location":"motion-proposals/131786%20Voting%20grant%20application%20of%20Zenith%20Code%20LLC%20for%20topic%20IC%20OS%20Version%20Election.html#relevant-experience-and-technical-knowledge","title":"Relevant Experience and Technical Knowledge","text":"<ul> <li>Operating nodes for several months on ICP (Zenith Code LLC).</li> <li>Relevant experience in installing IC OS and monitoring nodes.</li> <li>Strong understanding of the ICP infrastructure layer.</li> <li>As a development shop in the ecosystem, we have assisted various projects in setting up their proof of concept and MVP.</li> <li>Actively contributed to writing dapps in Motoko for in-house purposes.</li> <li>Over 7 years of experience in web3 and large-scale distributed systems.</li> <li>ICP enthusiast with a deep understanding of its core technology and architecture.</li> <li>Experience in designing large-scale distributed systems for storing user data across multiple nodes in RF-2 and RF-3 production clusters.</li> <li>Worked on the design and development of NDFS (Nutanix Distributed File Storage), similar to Google File System and blockchain data storage.</li> </ul>"},{"location":"motion-proposals/131786%20Voting%20grant%20application%20of%20Zenith%20Code%20LLC%20for%20topic%20IC%20OS%20Version%20Election.html#why-we-want-to-be-a-voting-neuron","title":"Why we Want to Be a Voting Neuron","text":"<ul> <li>By becoming a voting neuron we want to contribute towards decentralization of the ICP network.</li> <li>We believe we have the right skill set to evaluate the proposals and vote on them.</li> <li>We want to become a leading voting neuron with the capability of reviewing other governance proposals in the future.</li> </ul>"},{"location":"motion-proposals/131786%20Voting%20grant%20application%20of%20Zenith%20Code%20LLC%20for%20topic%20IC%20OS%20Version%20Election.html#why-we-need-the-grant","title":"Why We Need the Grant","text":"<p>We have been working full-time jobs and working part time on IC, and the grant will help us get one step closer to contributing full-time to the IC community. With Zenith Code, We plan to expand, add more nodes, develop large-scale distributed DAO products, and add significant value to the IC community.</p>"},{"location":"motion-proposals/131786%20Voting%20grant%20application%20of%20Zenith%20Code%20LLC%20for%20topic%20IC%20OS%20Version%20Election.html#long-term-interest-in-icp","title":"Long-Term Interest in ICP","text":"<p>We are fascinated by ICP technology and its long-term goals of creating a community cloud infrastructure. This vision, if realized, will add immense value globally and reduce dependence on major cloud players like AWS and GCP in the market. Zenith Code is also a stakeholder and long term node provider in the IC ecosystem.</p>"},{"location":"motion-proposals/131786%20Voting%20grant%20application%20of%20Zenith%20Code%20LLC%20for%20topic%20IC%20OS%20Version%20Election.html#interest-in-the-topics-applied-for","title":"Interest in the Topics Applied For","text":"<p>We are interested in applying for</p> <ol> <li>IC OS Version Election.</li> <li>Protocol Canister Management.</li> </ol> <p>We have expertise in large scale distributed systems, particularly distributed software-defined storage, cryptography, and web3.</p>"},{"location":"motion-proposals/131786%20Voting%20grant%20application%20of%20Zenith%20Code%20LLC%20for%20topic%20IC%20OS%20Version%20Election.html#long-term-commitment-as-a-voting-neuron","title":"Long-Term Commitment as a Voting Neuron","text":"<p>As an existing node provider in the IC ecosystem, we are committed to the long-term growth of the IC network. This commitment is driven by both personal interests and a broader vision of making the world a better place by contributing in the development of public cloud infrastructure.</p>"},{"location":"motion-proposals/131786%20Voting%20grant%20application%20of%20Zenith%20Code%20LLC%20for%20topic%20IC%20OS%20Version%20Election.html#voting-principles","title":"Voting Principles","text":"<p>Core Principle: Decentralization</p> <ul> <li>Definition and Relevance: The Nakamoto Coefficient measures the number of entities that must collude to control a system. A higher coefficient signifies greater decentralization and, consequently, a more secure and resilient network.</li> <li>Application to Voting: Applying this principle to voting within the IC ecosystem, we emphasize the need for a diverse and decentralized set of voters to prevent centralization and ensure fair representation of the community\u2019s interests.</li> </ul> <p>Ensuring Robust Decentralization</p> <ul> <li>Diverse Voting Participation: By adhering to decentralization principles, our voting approach aligns with the IC community and foundation. This means advocating for broad participation across various stakeholders to avoid power concentration.</li> </ul> <p>Evolving with Community Decisions</p> <ul> <li>Adaptive Voting Strategy: Our commitment to evolving voting strategies based on community discussions reflects a dynamic approach to maintaining and enhancing decentralization. We plan to formalize a process in which we will actively listen to community inputs and adapt our voting accordingly.</li> <li>Continuous Improvement: By aligning future voting with community conclusions on decentralization, our approach ensures that the network remains resilient to centralization risks, analogous to maintaining a high Nakamoto Coefficient.</li> </ul> <p>We are eager to contribute to the ICP community in a structured and impactful way. Thank you for considering our application. \ud83d\ude4f</p>"},{"location":"motion-proposals/132136%20Internet%20Computer%20topology%20-%20increase%20subnet%20size%20Internet%20Identity%20and%20Fiduciary%20subnets.html","title":"132136 Internet Computer topology   increase subnet size Internet Identity and Fiduciary subnets","text":""},{"location":"motion-proposals/132136%20Internet%20Computer%20topology%20-%20increase%20subnet%20size%20Internet%20Identity%20and%20Fiduciary%20subnets.html#motion-proposal-adjustment-of-ic-target-topology-to-increase-subnet-size-of-fiduciary-and-ii-subnets","title":"Motion Proposal Adjustment of IC Target Topology to Increase Subnet Size of Fiduciary and II subnets","text":"<p>The IC Target Topology sets targets for the number of Gen1 and Gen2 nodes per subnet and the decentralization coefficients (Nakamato coefficients) per subnet. It is a model used in the Internet Computer network to optimize the balance between node rewards and decentralization, and is not fixed and is voted in by the community as the target to be achieved within a certain timeframe. The latest IC Target Topology can be found here 5.</p> <p>It is proposed to update the IC target topology in order to increase the subnet size of the Fiduciary subnet and II subnet. The main reason for this is to increase the security of the IC since larger subnets require more node machines or node providers colluding with each other in order to take over these subnets. Given that the IC network currently has sufficient spare node machines, no additional node providers or node machines need to be onboarded. The intention is to gradually increase the size of these Fiduciary subnet and II subnet over time, and at a certain (to be determined) time set up specific production and backup subnets for storing the tECDSA signatures.</p> <p>In addition to increasing the subnet size of the II and Fiduciary subnets, it is proposed to update the country limit for the Swiss subnet to 13, which was erroneously set to 2 in the currently approved IC Target Topology table.</p> <p>Further discussion on this proposal can be found in\u00a0this forum post.</p>"},{"location":"motion-proposals/132136%20Internet%20Computer%20topology%20-%20increase%20subnet%20size%20Internet%20Identity%20and%20Fiduciary%20subnets.html#background","title":"Background","text":"<p>The uzr34 and pzp6e subnets are subnets with 28 active node machines. These subnets have more active node machines than an application subnet (that has 13 active node machines) since the uzr34 and pzp6e support important functionality for the IC:</p> <ul> <li>The uzr34 subnet runs the Internet Identity, Cycles Ledger, Exchange Rate canister, and XRC dashboard, and stores the backup tECDSA signing keys. The pzp6e subnet acts as a Fiduciary subnet and stores the tECDSA signing keys.</li> <li>With 28 nodes, | 28 / 3 | + 1 or 10 nodes need to be malicious in order to take control of these subnets. By adding more node machines to these subnets, more node machines/node providers need to collude to take over the subnet. Hence, adding more node machines to these subnets will add to the security of these subnets.</li> </ul> <p>It is proposed to improve the security of these subnets by adding more node machines. This will mean an update of the IC target topology that was previously approved 1 on 12<sup>th</sup> November 23. As there are currently more node machines in the IC network than required to meet the decentralization targets set in the target topology, no new node machines will need to be onboarded, and spare node machines can be used to increase the size of the uzr34 and pzp6e subnets.</p>"},{"location":"motion-proposals/132136%20Internet%20Computer%20topology%20-%20increase%20subnet%20size%20Internet%20Identity%20and%20Fiduciary%20subnets.html#roll-out-plan","title":"Roll-out plan","text":"<p>With the currently available nodes, the uzr34 and pzp64 subnets can be increased to 34 node machines and still meet decentralization targets.</p> <p>For the long term, the following roll-out plan is intended:</p> <ul> <li>Further increase the subnet size of uzr34 and pzp64 with 3 nodes every time when spare nodes are available. The reason for adding nodes in multiples of 3 is that each three nodes will result in one more additional node machine having to act maliciously to take over the subnet.</li> <li>As a final step in the roll-out plan, separate ECDSA signing and ECDSA backup subnets will be created, provided the load on the subnets requires to have separate subnets for ECDSA, and sufficient spare node machines are available to meet the decentralization coefficients.</li> </ul>"},{"location":"motion-proposals/132136%20Internet%20Computer%20topology%20-%20increase%20subnet%20size%20Internet%20Identity%20and%20Fiduciary%20subnets.html#proposed-target-topology","title":"Proposed Target Topology","text":"<p>The below table shows the proposed update of the target topology. The original target topology can be found in\u00a0this forum post.</p> Subnet Type # Subnets # Nodes in subnet Total SEV Subnet limit NP, DC, DC Provider* Subnet limit country NNS 1 43 43 no 1 3 SNS 1 34 34 no 1 3 Fiduciary 1 34 34 no 1 3 Internet Identity 1 34 34 yes 1 3 ECDSA signing 1 28 28 yes 1 3 ECDSA backup 1 28 28 yes 1 3 Bitcoin canister 1 13 13 no 1 2 European subnet 1 13 13 yes 1 2 Swiss subnet 1 13 13 yes 1 13 Application subnet 31 13 403 no 1 2 Reserve nodes Gen1 100 Reserve nodes Gen2 20 Total 763 <p>In addition to increasing the subnet size of the II and Fiduciary subnets, it is proposed to update the country limit for the Swiss Subnet to 13, which was erroneously set to 2 in the currently approved IC Target Topology table. Note that there currently is no Swiss subnet yet implemented.</p>"},{"location":"motion-proposals/132411%20Periodic%20confirmation%20of%20following.html","title":"132411 Periodic confirmation of following","text":""},{"location":"motion-proposals/132411%20Periodic%20confirmation%20of%20following.html#tldr","title":"TL;DR","text":"<ul> <li>This proposal replaces proposal 55651 with an updated and more detailed design.</li> <li>A neuron has to regularly take one of the following actions: directly vote, set following, confirm following.</li> <li>If a neuron fails to do one of these actions for 6 months, the neurons\u2019 voting power will be decreased until it reaches zero after 7 months of inactivity. At this point the neuron\u2019s following settings are reset.</li> </ul>"},{"location":"motion-proposals/132411%20Periodic%20confirmation%20of%20following.html#context","title":"Context","text":"<p>This proposal is the result of\u00a0this forum discussion.</p> <p>This proposal refines the design from proposal\u00a055651\u00a0to make it actionable. Adopting this proposal means that the proposal 55651 and the decision to adopt it are replaced with this proposal\u2019s content.</p>"},{"location":"motion-proposals/132411%20Periodic%20confirmation%20of%20following.html#design","title":"Design","text":"<p>The main idea of periodic confirmation is that in order to get rewards, governance participants have to remain active voters and regularly confirm their following settings. Neurons who set following once and then never interact with the NNS again get lower, adjusted voting rewards. Neurons who were created with default following and never made an active decision who to follow, have to do so in order to keep getting voting rewards.</p> <ol> <li> <p>In order to have voting power and get voting rewards, a neuron has to\u00a0regularly vote directly, set following, or confirm its current following settings.</p> </li> <li> <p>If a neuron does not take any of the above actions, the\u00a0neuron\u2019s voting power is adjusted. After\u00a06 months of no action, the neuron\u2019 voting power is linearly decreased for one month until it\u00a0reaches zero at the end of 7 months\u00a0without any action. After these 7 months, the neuron\u2019s following settings are fully reset.</p> </li> </ol> <p>To track its activity, governance remembers for each neuron the\u00a0last time when it took any of the above actions.</p>"},{"location":"motion-proposals/132411%20Periodic%20confirmation%20of%20following.html#relevant-actions","title":"Relevant actions","text":"<p>In addition to confirmation of following, the design includes set following and direct voting. Setting following also reflects an explicit choice by the neuron who to follow. Direct voting is included because a neuron that always votes directly and does not rely on following is a very active governance participant and should not have adjusted voting power or adjusted voting rewards.</p>"},{"location":"motion-proposals/132411%20Periodic%20confirmation%20of%20following.html#voting-power-adjustment","title":"Voting power adjustment","text":"<p>\u201cSleeper\u201d neurons, who don\u2019t take any of the above actions for more than 7 months, should not be automatically participating in voting and getting voting rewards. This design suggests to realize this by adjusting their voting power and resetting their followees.</p>"},{"location":"motion-proposals/132411%20Periodic%20confirmation%20of%20following.html#voting-power-adjustment_1","title":"Voting power adjustment","text":"<p>This design proposes that</p> <ul> <li>For each proposal and neuron, in the\u00a0ballot\u00a0consider the\u00a0adjusted voting power. That is, record less voting power for neurons that have not taken any of the above actions for the last 6 months.</li> <li>For each proposal, distinguish<ul> <li>the\u00a0total (potential) voting power, which is the sum of all neurons\u2019 voting power without the adjustment</li> <li>the\u00a0total adjusted voting power, which is the sum of all neurons\u2019 adjusted voting power that can contribute to the decision</li> </ul> </li> <li>For each proposal,<ul> <li>consider the total adjusted voting power for deciding proposals.</li> <li>consider the total (potential) voting power when computing the rewards. This is similar to the current design in that the rewards take into account the voting power if all neurons participated.</li> </ul> </li> </ul>"},{"location":"motion-proposals/132411%20Periodic%20confirmation%20of%20following.html#effect-of-the-voting-power-adjustment","title":"Effect of the voting power adjustment","text":"<p>Adjusting the voting power in this way has the following consequences:</p> <ul> <li>Sleeper-neurons are not considered in the decision making process. This means that proposals can still be decided quickly if the majority of the regularly active voters agree quickly.</li> <li>If a neuron has been sleeping for more than 7 months, then the voting power recorded for the neuron in any open proposal is zero. This has the advantage that neurons who have been inactive for a long time cannot simply show up and swing a proposal in an unexpected way.</li> </ul>"},{"location":"motion-proposals/132411%20Periodic%20confirmation%20of%20following.html#who-can-perform-the-relevant-actions","title":"Who can perform the relevant actions?","text":"<p>Already now, direct voting and setting of following can be done by a neuron\u2019s controller or its hotkeys. The same permission should apply for confirmation of following.</p> <p>This means that a neuron can take any of the relevant actions (which will also reset the timer) with its controller or hotkey, which is in line with the original proposal and ensures that the feature can be implemented in a user-friendly way.</p>"},{"location":"motion-proposals/132411%20Periodic%20confirmation%20of%20following.html#alternatives-considered","title":"Alternatives considered","text":"<ol> <li> <p>Reset of following without voting power adjustment.</p> <p>Compared to the current design, this has the disadvantage that sleepers would still be counted towards the total decision voting power. As a consequence, if there are many sleepers, it would be hard or impossible to reach quick decisions for urgent proposals such as fixes to security bugs.</p> </li> <li> <p>A one-off reset, possibly also with batches of neurons being reset at a time.</p> <p>This has similar disadvantages wrt quick decisions as above. Moreover, it does not achieve the goal that this is a periodic task for participants.</p> </li> <li> <p>Adjustment of the voting rewards, but not the voting power.</p> <p>This would have similar disadvantages as explained for the reset of following above. In addition, this would mean that sleeper neurons would still contribute to the decisions with their full voting power, which does not seem to be in line with the original proposal\u2019s intention.</p> </li> <li> <p>Alternative time periods.</p> <p>Alternatives to the 6 and 7 months were discussed. The period before the adjustment should be short enough to have some effect but long enough so that it is realistic for most neurons to perform one of the actions without being a major hurdle. In the discussion it was argued that a biannual confirmation is easy to remember and meets these arguments.</p> </li> </ol>"},{"location":"motion-proposals/132411%20Periodic%20confirmation%20of%20following.html#future-reward-changes","title":"Future reward changes","text":"<p>This design does not change how rewards work and keeps the status quo. The motion does not exclude changing rewards going forward. Any other changes in how rewards work should be discussed, agreed upon, and implemented separately.</p>"},{"location":"motion-proposals/132553%20Update%20Interim%20Gen-1%20Node%20Provider%20Remuneration%20After%2048%20months.html","title":"132553 Update Interim Gen 1 Node Provider Remuneration After 48 months","text":"<p>Following the adoption of\u00a0proposal #127044, Node Providers (NPs) conducted simulations that revealed significant concerns about the economic viability of operating nodes in top-tier data centers. The new remuneration model could potentially make it difficult to sustain these nodes and might even lead to server removal from the network. These concerns were shared with DFINITY during the IC Lab for NPs, and since then, a group of NPs has been working on an updated proposal. We have incorporated feedback from the community and DFINITY, and are now ready to present the revised proposal for broader input.</p> <p>Why Proposal #127044 Needs to Be Updated</p> <p>The current post-48-month remuneration model for Gen-1 NPs (proposal #127044) will lead to downsizing or a complete exit of these providers. This is not in the community's best interest for several reasons:</p> <ol> <li> <p>Readiness for Future Growth: The Internet Computer (IC) is expected to experience significant growth in the future. Reducing the number of Gen-1 nodes now would limit the network's capacity to handle sudden spikes in demand, potentially causing a degraded user experience. Gen-1 NPs have been integral to the IC's infrastructure since Genesis and possess the experience and resources needed to scale rapidly. Maintaining these nodes in top-tier data centers is crucial to the IC\u2019s preparedness for future expansion.</p> </li> <li> <p>Maintaining Positive Sentiment: A reduction in the number of active nodes, even by a small margin (14%), could be perceived as a failure by investors and developers. This could lead to a loss of confidence, causing a negative spiral in ICP price and harming the overall perception of the IC network. Keeping all Gen-1 nodes active will help maintain a positive outlook among stakeholders.</p> </li> <li> <p>Cost-Effectiveness of Gen-1 Nodes: With their initial purchase/setup costs already paid off, Gen-1 nodes are cheaper to operate than Gen-2 nodes. Reducing these nodes would require onboarding new NPs to meet future demand, which would be more expensive and increase inflation. Retaining Gen-1 nodes avoids these additional expenses and ensures efficient use of existing resources.</p> </li> </ol> <p>Proposed Updates</p> <p>After thorough discussions, we propose an approach that maintains Gen-1 NP rewards and network decentralization without reducing the total node count. We ask the community to vote to accept this proposal. Voting to accept would mean:</p> <ol> <li> <p>Smaller Reduction in Rewards for Gen-1 Nodes:\u00a0The rewards for Gen-1 nodes operating beyond the initial 4-year period would be reduced by 33% from current levels, rather than the 75% reduction proposed in\u00a0proposal #127044\u00a0(see remuneration table below).</p> </li> <li> <p>New Reward Scheme in Place for 2 Years:\u00a0This revised reward scheme would be in place for 2 years, providing Gen-1 NPs with the stability needed to make necessary commitments to their service providers, such as data centers, ISPs, and other necessary costs.</p> </li> <li> <p>42-Node Cap:\u00a0NPs will be limited to operating a maximum of 42 nodes, in line with the target topology.</p> </li> <li> <p>Handling Excess Nodes:\u00a0Any excess nodes from the 42-node cap can remain on the IC. While reassignment to a \u201cNew\u201d NP is mandatory, these nodes may continue in the same or a different data center and/or country. A \u201cNew\u201d NP can also be an existing NP with fewer than 42 nodes. No NP will receive rewards for more than 42 nodes, and the new NP will receive the same 33% reduced remuneration for the acquired nodes. New NPs not yet listed in the NNS will be accepted through the self-declaration process and respective proposals.</p> </li> <li> <p>No Rejection Due to Topology Restrictions:\u00a0NPs taking on these excess nodes will be allowed to onboard them without rejection due to target topology restrictions, as this is not an addition of new nodes but a reassignment that maintains the total node count and enhances network decentralization and utilization.</p> </li> </ol> TYPE-1 Reward per node per month (in XDR) US - California 1072 US - other 1004 Canada 1088 Slovenia 1152 Switzerland 1136 EU - other 1061 Singapore 1234 Japan 1188 <p>Conclusion</p> <p>Avoiding the downsizing of Gen-1 NPs, as imposed in\u00a0proposal #127044,\u00a0is crucial for the community's long-term success. Downsizing could undermine the IC's readiness for growth, damage investor sentiment, and increase costs due to the need to onboard new NPs. Given the economic advantages of Gen-1 nodes over Gen-2 nodes, sustaining their operation is vital to avoid network capacity loss and inflation costs associated with new node provider onboarding.</p> <p>We urge the community to vote in favor of this proposal to ensure the continued strength and readiness of the IC network. By doing so, we can maintain a positive outlook among investors and developers, efficiently serve high-demand regions, and prepare for future growth without incurring unnecessary costs.</p> <p>This proposal was crafted and submitted with\u00a0proposals.network\u00a0\u2013 where ideas meet the ICP governance.</p>"},{"location":"motion-proposals/133108%20Ability%20to%20Remove%20a%20Neuron%20from%20the%20Known%20Neurons%20Registry%20%26%20Modify%20Its%20Description.html","title":"133108 Ability to Remove a Neuron from the Known Neurons Registry & Modify Its Description","text":"<p>This proposal recommends two key improvements to the known neurons management in the Network Nervous System (NNS):</p> <p>1.\u00a0Enable the modification of a known neuron\u2019s description\u00a0without requiring a name change. This would allow neuron owners to keep their descriptions current as their expertise, voting topics, or goals evolve over time.</p> <p>2.\u00a0Allow for the complete removal of a known neuron from the registry\u00a0through an NNS proposal. This would provide flexibility for users who no longer wish to maintain their neuron in the known registry, offering greater control over their privacy and participation.</p>"},{"location":"motion-proposals/133108%20Ability%20to%20Remove%20a%20Neuron%20from%20the%20Known%20Neurons%20Registry%20%26%20Modify%20Its%20Description.html#detailed-rationale","title":"Detailed Rationale","text":"<p>Recent efforts to implement the changes outlined in proposal\u00a0130832\u00a0soon will made it possible for all neurons - particularly known neurons - to provide greater transparency regarding their voting behavior and governance decisions. As part of this process, users are given the choice of how much information they wish to disclose regarding their neurons\u2019 stake and votes. However, many neurons were added to the known neuron registry before these transparency measures were introduced, and there is now a need to address two key issues:</p> <p>1.\u00a0Inability to update neuron description without a name change: Currently, once a neuron is registered as a known neuron, its description cannot be modified without also changing the name. This presents a challenge for those who may shift their focus or expertise over time and wish to communicate these changes to their followers. Updating neuron descriptions should be a straightforward process that doesn't necessitate a name change, which is currently the only workaround (e.g., modifying the capitalization of the name).</p> <p>2.\u00a0Inability to remove a neuron from the known neuron registry: There are legitimate reasons why a user may wish to withdraw their neuron from the known neuron registry, whether due to privacy concerns or changes in how the neuron is used. Presently, this functionality does not exist. The implementation of proposal 130832 introduces new transparency rules that may cause privacy concerns for those neurons that were added to the registry before these changes. Enabling a removal mechanism via an NNS proposal would provide the flexibility needed to address this issue.</p>"},{"location":"motion-proposals/133108%20Ability%20to%20Remove%20a%20Neuron%20from%20the%20Known%20Neurons%20Registry%20%26%20Modify%20Its%20Description.html#benefits","title":"Benefits","text":"<ol> <li> <p>Enhances the user experience by making it easier to update neuron descriptions and manage their public profile.</p> </li> <li> <p>Protects user privacy by providing a way to remove known neurons from the registry, if desired.</p> </li> <li> <p>Increases flexibility for neuron controllers to adapt their known neurons as their focus, expertise, or goals evolve.</p> </li> </ol> <p>I encourage the community to review and share feedback, particularly from those involved in the ongoing development of neuron management and transparency features.</p>"},{"location":"motion-proposals/133175%20Add%20Public%20and%20Restricted%20canister_status%20Visibility.html","title":"Proposal to add Public and Restricted canister_status Visibility","text":"<p>This proposal is submitted by the\u00a0CycleOps team. CycleOps is proactive, automated no code canister management for the Internet Computer.</p> <p>Additional Credits: Thanks to Fulco Taen, the original proposer of a\u00a0public canister_status\u00a0in October 2022, and thanks to Dominic Woerner for helping push this proposal forward and to Dimitris Sarlis for revewing this proposal.</p>"},{"location":"motion-proposals/133175%20Add%20Public%20and%20Restricted%20canister_status%20Visibility.html#summary","title":"Summary","text":"<p>There is a function on the Internet Computer management canister called\u00a0canister_status\u00a0which returns detailed metric information about the canister, including but not limited to its controllers, wasm hash, cycle balance, and memory utilization. Currently this can only be called by the controllers (owners) of a canister.</p> <p>This goal of this proposal is to allow the controllers of a canister the voluntary option to make the canister_status API callable by third parties without requiring those parties be a controller of a canister, and to similarly provide the ability for those owners to revoke a canister\u2019s public\u00a0<code>canister_status</code>\u00a0endpoint to make this data private again at any time.</p>"},{"location":"motion-proposals/133175%20Add%20Public%20and%20Restricted%20canister_status%20Visibility.html#real-world-uses-cases","title":"Real World Uses Cases","text":"<ul> <li>Public metrics for public canisters\u00a0- SNS and NNS canisters can publicly expose metrics without needing to provide a public API.</li> <li>Protocol verified Metrics\u00a0- user created canisters such as NFID vaults, OpenChat user canisters, or even Bob miner canisters can expose metrics that are verified by the protocol instead of via a 3<sup>rd</sup> party API.</li> <li>Easier canister monitoring\u00a0- Provide an alternative option for developers who wish to set up canister monitoring services without requiring them to integrate with a monitoring blackhole.</li> </ul>"},{"location":"motion-proposals/133175%20Add%20Public%20and%20Restricted%20canister_status%20Visibility.html#background","title":"Background","text":"<p>In late 2022, an Internet Computer developer\u00a0proposed several options for making a canister\u2019s status public. Many of the responding developers in the thread favored option D, \u201cMake canister_status something that the controller can choose to expose or not with a flag which is set to private by default\u201d.</p> <p>In March 2024, a DFINITY team member opened up a\u00a0poll of different options for making a canister\u2019s status public, with the majority of respondents voting for the option to \u201cAllow canister_status to be made public with the understanding this could expose secrets and could be extended to make all code &amp; state of the canister public, i.e, public\u00a0<code>canister_status</code>\u00a0== public canister.\u201d</p> <p>All respondents favored adding a route for making canister status public, with none believing that the status quo is sufficient.</p> <p>This proposal suggests a flexible path forward for publicly and selectively exposing canister metrics to third parties.</p>"},{"location":"motion-proposals/133175%20Add%20Public%20and%20Restricted%20canister_status%20Visibility.html#proposed-mechanism-for-adding-canister_status-visibility","title":"Proposed Mechanism for Adding canister_status Visibility","text":"<p>Given that the Internet Computer protocol already provides canister status metric information but restricts access of it to the controller(s) of a canister, we propose that a\u00a0<code>status_visibility</code>\u00a0variant property should be added to the\u00a0<code>canister_settings</code>\u00a0returned by\u00a0<code>canister_status</code>.</p> <p>We propose that\u00a0<code>status_visibility</code>\u00a0have a\u00a0<code>public</code>\u00a0,\u00a0<code>controllers</code>, and third\u00a0<code>allowed_viewers</code>\u00a0option that would bridge the gap between fully private and fully public canisters by allowing a short list (limit 5 principals) that are able to retrieve the canister status for that canister, without requiring that the status be publicly available, or only available to controllers.</p> <p>Specifying the\u00a0<code>allowed_viewers</code>\u00a0variant would restrict\u00a0<code>canister_status</code>\u00a0access to both the list of\u00a0<code>allowed_viewers</code>\u00a0and any controllers of the canister.</p>"},{"location":"motion-proposals/133175%20Add%20Public%20and%20Restricted%20canister_status%20Visibility.html#proposed-interface-change","title":"Proposed Interface Change","text":"<pre><code>type status_visibility = variant {\n  controllers;\n  allowed_viewers : opt vec principal;\n  public;\n};\n\ntype definite_canister_settings = record {\n  controllers : vec principal;\n  compute_allocation : nat;\n  memory_allocation : nat;\n  freezing_threshold : nat;\n  reserved_cycles_limit : nat;\n  log_visibility : opt log_visibility;\n  status_visibility : opt status_visibility; // new field (similar to log_visibility)\n};\n</code></pre>"},{"location":"motion-proposals/133175%20Add%20Public%20and%20Restricted%20canister_status%20Visibility.html#what-is-asked-of-the-community","title":"What is asked of the community","text":"<p>Review the full developer forum proposal, ask questions, and vote to accept or reject.</p> <p>Forum proposal (live since June 2024):\u00a0https://forum.dfinity.org/t/nns-proposal-add-public-and-restricted-canister-status-visibility/31814</p>"},{"location":"motion-proposals/133388%20Increase%20cycle%20costs%20to%20offset%20network%20overutilization.html","title":"Solving ICP's latency problem with a much needed cost increase","text":"<p>This proposal is simple. If you support raising costs to decrease network congestion, vote yes. If you want to keep subsidized ICP compute costs in place and wait another 2-6 weeks for applications to maybe be reliable to use again, vote no.</p>"},{"location":"motion-proposals/133388%20Increase%20cycle%20costs%20to%20offset%20network%20overutilization.html#the-current-situation","title":"The Current Situation","text":"<p>All of ICP has been overloaded for over two weeks, with latencies making applications unreliable. The European subnet that many, including myself have assets on has been essentially unusable for 6 weeks.</p> <p>People are excited about increased cycle burn, but developers are paying a bigger price as requests to their applications are unusable and frequently time out, forcing some to migrate everything or start over from scratch.</p> <ul> <li>A promising on-chain AI app cannot run</li> <li>A DAO has been forced to completely rebuild from the ground up - x.com/beadle1989/status/1843524802566312040)</li> <li>People's funds are getting stuck &amp; transactions are failing on the top DEX on ICP</li> <li>People are trying to demo their ICP apps and ICP is not available</li> </ul> <p>Developers were promised \\(5 a year for a simple application to be reliable. Users were promised web speed on-chain applications. Now DFINITY is recommending that developers pay\u00a0[\\)35 a month](https://forum.dfinity.org/t/subnets-with-heavy-compute-load-what-can-you-do-now-next-steps/35762), or $420 a year just for a 1% chance that our canister will execute in a single round, meaning that a user could be waiting up to 40 seconds for a canister paying for that 1% compute to execute on a busy subnet.</p> <p>From $5 to $420 is a 8300% cost increase for semi-reliable on chain compute. This is unacceptable.</p>"},{"location":"motion-proposals/133388%20Increase%20cycle%20costs%20to%20offset%20network%20overutilization.html#a-solution-bump-up-compute","title":"A Solution - Bump up compute","text":"<p>Since genesis, compute costs have been heavily subsidized to promote developer growth.\u00a0The $5/GB/yr estimates were based on optimistic future estimates of hardware costs &amp; protocol capabilities, but the reality of the situation is that ICP is currently at full utilization and we are not deflationary.\u00a0If the network is at capacity, ICP should burn close to as much as it mints.</p> <p>Building on ICP is cheap. When faced with the alternative of paying an 83X cost increase for compute allocation, most developers would gladly pay 2-3x more as long as they get reliable compute in return. This can be rolled back in the future as the protocol improves its scalability and hardware and protocol costs come down.</p> <p>Applications on ICP subnets have been unusable for 2-6 weeks. It's too cheap for people to clog up the network. Please raise the prices.</p> <ul> <li>With love, a concerned ICPunk.</li> </ul>"},{"location":"motion-proposals/133462%20Reduce%20ckBTC%20Minimum%20Retrieval%20Amount.html","title":"133462 Reduce ckBTC Minimum Retrieval Amount","text":"<p>The ckBTC minimum retrieval variable is currently set to 0.001 BTC as shown by the function \u2018retrieve_btc_min_amount\u2019and. This determines the minimum ckBTC amount that can be burned and, correspondingly, the minimum native BTC amount that can be withdrawn from the minter.  </p> <p>However, at current BTC prices this puts the minimum amount of native BTC one can withdraw from the IC between \\(60-\\)70 USD, despite there being no minimum deposit amount. We find this minimum amount to be high and contend this creates a barrier to ckBTC onboarding and adoption from both native BTC holders and IC Dapps.  </p> <p>We propose that this amount be lowered to 0.0005 BTC (\\(30-\\)35 USD at current value) in order to facilitate the onboarding to ckBTC by guaranteeing a lower offboarding requirement and improving the ckBTC user experience.\u00a0</p> <p>Whilst we recognise the potential risk this lower minimum could lead to a potentially dropped BTC withdrawal in the case of a small ckBTC unminting coupled with a large BTC fee rise,\u00a0 we contend this risk is minor and is outweighed by the benefits of an improved ckBTC user experience and lower barrier to usage.\u00a0</p> <p>We have put forward this proposal as a motion proposal in order to gauge community sentiment as to this move. Should this motion proposal pass, we will put forward a subsequent proposal including the code changes required.  </p> <p>Please find more information and community discussion surrounding this topic on the forum here: https://forum.dfinity.org/t/for-the-icp-community-how-to-get-the-number-of-bitcoin-confirmations-from-the-ckbtc-minter/35973</p>"},{"location":"motion-proposals/133841%20Update%20Target%20Topology%20-%20adding%2020%20application%20subnets.html","title":"133841 Update Target Topology   adding 20 application subnets","text":""},{"location":"motion-proposals/133841%20Update%20Target%20Topology%20-%20adding%2020%20application%20subnets.html#motion-proposal-adjustment-of-ic-target-topology-to-add-20-application-subnets","title":"Motion Proposal - Adjustment of IC Target Topology to add 20 application subnets","text":"<p>The IC Target Topology sets targets for the number of Gen1 and Gen2 nodes per subnet and the decentralization coefficients (Nakamato coefficients) per subnet. It is a model used in the Internet Computer network to optimize the balance between node rewards and decentralization, and is not fixed and is voted in by the community as the target to be achieved within a certain timeframe. The latest approved IC Target Topology can be found in\u00a0this forum post\u00a0and was approved in\u00a0this proposal.</p>"},{"location":"motion-proposals/133841%20Update%20Target%20Topology%20-%20adding%2020%20application%20subnets.html#background","title":"Background","text":"<p>Given the rapidly increasing load on the IC, we would like to propose adding additional application subnets to provide developers with greater flexibility and choice when selecting a deployment environment. This expansion will better position the IC network to accommodate continued growth in the coming months.</p> <p>The following three step approach is taken to further expand the availability of accessibility of subnets for developers:</p> <ul> <li>Step 1: Expand the list of public subnets. This step has already been completed</li> <li>Step 2: Open subsets of the currently verified subnets to the community. Of the existing subnets, several so-called verified subnets are not open for deployment because these contain existing legacy canisters, some of which depend on a special replica configuration. These verified subnets - of which there are 11 - will be gradually opened.</li> <li>Step 3: Extend the target topology by 20 additional application subnets and gradually submit proposals to create these subnets as capacity is needed.</li> </ul> <p>Further discussion on this approach can be found in\u00a0this forum post.</p> <p>This motion proposal covers step 3 of the above approach, which is updating the target topology by adding application subnets to the IC Target Topology. It is proposed to add an additional 20 application subnets of each 13 node machines, based on the currently available node machine on the IC network.</p>"},{"location":"motion-proposals/133841%20Update%20Target%20Topology%20-%20adding%2020%20application%20subnets.html#proposed-target-topology","title":"Proposed Target Topology","text":"<p>The below table shows the proposed update of the target topology.</p> Subnet Type # Subnets # Nodes in subnet Total SEV Subnet limit NP, DC, DC Provider* Subnet limit country NNS 1 43 43 no 1 3 SNS 1 34 34 no 1 3 Fiduciary 1 34 34 no 1 3 Internet Identity 1 34 34 yes 1 3 ECDSA signing 1 28 28 yes 1 3 ECDSA backup 1 28 28 yes 1 3 Bitcoin canister 1 13 13 no 1 2 European subnet 1 13 13 yes 1 2 Swiss subnet 1 13 13 yes 1 13 Application subnet 51 13 663 no 1 2 Reserve nodes Gen1 100 Reserve nodes Gen2 20 Total 1023 <p>The latest approved IC Target Topology can be found in\u00a0this forum post\u00a0and was approved in\u00a0this proposal.</p>"},{"location":"motion-proposals/134031%20API%20Boundary%20Node%20Incident%20Handling.html","title":"API Boundary Node Incident Handling","text":"<p>The DFINITY Foundation proposes adding functionality to configure temporary rate limits at the API boundary nodes to protect vulnerable components during incidents. This feature is especially important for supporting\u00a0the new boundary node architecture, in particular the API boundary nodes. Rate limits would be only activated as part of incident response under the\u00a0\u201cSecurity Patch Policy and Procedure,\u201d\u00a0which the NNS adopted in March 2022.</p> <p>This proposal was previously discussed on the forum on Oct 21, 2024:\u00a0https://forum.dfinity.org/t/incident-handling-with-the-new-boundary-node-architecture/36390</p>"},{"location":"motion-proposals/134031%20API%20Boundary%20Node%20Incident%20Handling.html#background","title":"Background","text":"<p>In certain incidents, it may be necessary to temporarily block or throttle requests to safeguard vulnerable components, which can range from specific canister methods to entire subnets. These rate limits are temporary patches and are removed once a permanent fix is implemented. Given that the new API boundary nodes are fully NNS-controlled, there must also be an NNS-controlled mechanism to set such rate limits.</p>"},{"location":"motion-proposals/134031%20API%20Boundary%20Node%20Incident%20Handling.html#proposal","title":"Proposal","text":"<p>We propose the creation of a canister to store rate-limiting rules and enforce them across all API boundary nodes. This canister will be controlled by the NNS, including the authorization of specific principals to configure rate-limiting rules. During an incident, active rate-limiting rules will remain private to prevent the exposure of vulnerabilities. Once the incident is resolved, these rules will be made public as part of the post-mortem.</p> <p>If this proposal is approved, the DFINITY Foundation will initiate all required steps to set up the canister and incorporate this functionality into the incident handling process. Additional proposals will follow to authorize the canister installation and configuration.</p>"},{"location":"motion-proposals/135253%20Incentivizing%20Node%20Relocation%20to%20Increase%20IC%20Decentralization.html","title":"Motion proposal: Incentivizing Node Relocation to Increase IC Decentralization","text":""},{"location":"motion-proposals/135253%20Incentivizing%20Node%20Relocation%20to%20Increase%20IC%20Decentralization.html#summary","title":"Summary","text":"<p>As the Internet Computer (IC) continues efforts to decrease rewards for Gen-1 nodes (as outlined in Proposal #132553) and promote decentralization among Gen-1 Node Providers (NPs), it is important to consider the reward adjustments for NPs willing to relocate nodes to new countries that are currently unrepresented on the IC. This motion proposes a moderate increase in rewards for NPs who incur additional costs to relocate their nodes, thereby enhancing the decentralization of the IC.</p>"},{"location":"motion-proposals/135253%20Incentivizing%20Node%20Relocation%20to%20Increase%20IC%20Decentralization.html#background-and-rationale","title":"Background and Rationale","text":"<p>When Gen-1 and Gen-2 reward structures were initially defined, it was recognized that data center costs vary widely across regions. The current IC topology primarily consists of nodes in countries with stable and cost-effective data centers. However, adding new countries to the network enhances decentralization and mitigates systemic risks.</p> <p>Relocating nodes to an unrepresented country imposes additional financial burdens on NPs, including:</p> <ol> <li>Higher data center costs \u2013 Most economical hosting locations are already occupied.</li> <li>Logistical and import costs \u2013 Transporting and setting up nodes in a new country entails significant expenses.</li> </ol> <p>To encourage and compensate for these efforts, this proposal suggests introducing Gen-1.1 rewards, providing a 10% increase over the highest current Gen-1 reward value, specifically for nodes moved to new, unrepresented countries.</p>"},{"location":"motion-proposals/135253%20Incentivizing%20Node%20Relocation%20to%20Increase%20IC%20Decentralization.html#proposed-reward-structure","title":"Proposed Reward Structure","text":"<p>We propose a Gen-1.1 reward value that is 10% higher than the highest Gen-1 reward value, with the following conditions:</p> <ul> <li>Limited Timeframe: This incentive applies only to relocations occurring as part of the Gen-1 to Gen-1.1 transition (Q4 2024 \u2013 Q1 2025).</li> <li>Reduction from Original Gen-1 Rewards: While this represents an increase from the latest Gen-1 rates, it remains 33% lower than the original Gen-1 remuneration.</li> <li>Eligibility Criteria:<ul> <li>The new country must not currently have any Gen-1 or Gen-2 nodes as of end of December 2024.</li> <li>The new country must not be an EU member state.</li> </ul> </li> <li>Reward Calculation:<ul> <li>The highest existing Gen-1 reward value is 1,234 XDR.</li> <li>The proposed Gen-1.1 reward would be 1,357 XDR for nodes relocated to qualifying new countries.</li> </ul> </li> </ul>"},{"location":"motion-proposals/135253%20Incentivizing%20Node%20Relocation%20to%20Increase%20IC%20Decentralization.html#expected-benefits","title":"Expected Benefits","text":"<ul> <li>Increased Decentralization \u2013 Distributes nodes more evenly across different jurisdictions.</li> <li>Improved Network Resilience \u2013 Reduces dependency on specific regions.</li> <li>Fair Compensation for NPs \u2013 Encourages voluntary relocations by covering associated costs.</li> </ul>"},{"location":"motion-proposals/135253%20Incentivizing%20Node%20Relocation%20to%20Increase%20IC%20Decentralization.html#conclusion","title":"Conclusion","text":"<p>By implementing a moderate reward increase, this proposal supports decentralization while ensuring that node providers are fairly compensated for expanding the IC\u2019s geographical diversity. We encourage the community to vote in favor of this motion to strengthen the IC\u2019s network and governance structure.</p>"},{"location":"motion-proposals/135700%20Increasing%20the%20number%20of%20DFINITY-owned%20nodes%20on%20the%20IC.html","title":"Motion Proposal \u2013 Increase DFINITY-Owned Nodes to 70 and Allow NNS Subnet Exception","text":"<p>This motion proposal requests that DFINITY be permitted to expand its operating node count from 42 to 70. In addition, it seeks an exception for the NNS subnet, allowing it to include 3 DFINITY nodes that are exempt from the standard topology restrictions (i.e., the limits of one node per data center owner, per data center, and per node provider). These modifications are essential to ensure sufficient spare capacity for maintenance, support ongoing redeployments, and facilitate the planned expansion of application subnets.</p>"},{"location":"motion-proposals/135700%20Increasing%20the%20number%20of%20DFINITY-owned%20nodes%20on%20the%20IC.html#background","title":"Background","text":"<ul> <li>Current Configuration: DFINITY currently has 42 nodes on the IC Mainnet, which comprises of 37 subnets. The current recovery process expects one DFINITY node per subnet in general, and exceptionally three DFINITY nodes for the NNS subnet for enhanced recovery resilience. This arrangement results in 39 nodes actively deployed in subnets, leaving only 3 nodes as spares.</li> <li>Operational Challenge: Among the 42 nodes, one node in Stockholm is degraded, and several nodes in Z\u00fcrich are being redeployed via the HSM-less process (a procedure that temporarily removes nodes from an active subnet). Consequently, the effective spare capacity is reduced to 2 healthy nodes.</li> <li>Future Expansion: As per motion proposal 133841, plans to add up to 20 application subnets necessitate additional DFINITY nodes for each additional application subnet.</li> </ul>"},{"location":"motion-proposals/135700%20Increasing%20the%20number%20of%20DFINITY-owned%20nodes%20on%20the%20IC.html#proposal","title":"Proposal","text":"<ol> <li>Increase Total Node Count:     Approve the expansion of DFINITY-owned nodes from 42 to 70 by adding an additional 28 nodes. To maintain fairness among node providers (who are limited to 42 nodes), the additional 28 nodes will not be eligible for rewards.</li> <li>NNS Topology Exception:     Approve a modification to the adopted Target Topology that formally permits DFINITY to operate 3 nodes in the NNS subnet outside the regular topology restrictions. This exception will provide the necessary flexibility to maintain recovery resilience and manage operational challenges without extending recovery times.</li> </ol> <p>Proposed Target Topology Table (Adjusted):</p> Subnet Type # Subnets # Nodes in Subnet Total Nodes SEV Subnet Limit (NP, DC, DC Provider) Subnet Limit (Country) NNS 1 43 43 no 1* (with exception for DFINITY nodes) 3 SNS 1 34 34 no 1 3 Fiduciary 1 34 34 no 1 3 Internet Identity 1 34 34 yes 1 3 ECDSA Signing 1 28 28 yes 1 3 ECDSA Backup 1 28 28 yes 1 3 Bitcoin Canister 1 13 13 no 1 2 European Subnet 1 13 13 yes 1 2 Swiss Subnet 1 13 13 yes 1 13 Application Subnet 51 13 663 no 1 2 Reserve Nodes Gen1 \u2013 \u2013 100 \u2013 \u2013 \u2013 Reserve Nodes Gen2 \u2013 \u2013 20 \u2013 \u2013 \u2013 Total 1023 <p>*Note: The NNS subnet includes 3 nodes operated by DFINITY that are exempt from the standard \u201c1 node per DC/NP\u201d rule.</p> <p>Benefits and Risks:</p> <ul> <li>Benefits:<ul> <li>Operational Resilience: Expanding the node count provides additional spare capacity, ensuring that maintenance and redeployment can occur without significantly delaying subnet recovery.</li> <li>Future-Proofing: The increased capacity supports planned expansion, including the addition of up to 20 application subnets.</li> <li>Clarification of Topology Rules: Formalizing the NNS subnet exception will resolve ongoing community questions and create a clear reference point for subnet membership change NNS proposals.</li> </ul> </li> <li>Risks:<ul> <li>Deviation from Standard Topology: Allowing an exception in the NNS subnet introduces a moderate departure from the established topology rules. However, given the operational challenges, this deviation is justified by the need for improved resilience.</li> </ul> </li> </ul> <p>Conclusion: For the reasons outlined above, we urge the community to support this motion proposal to increase the total number of DFINITY-owned nodes to 70 and to allow the NNS subnet an exceptional status with 3 nodes not subject to standard topology restrictions. These changes are crucial to maintain network resilience and accommodate future expansion.</p>"},{"location":"motion-proposals/135700%20Increasing%20the%20number%20of%20DFINITY-owned%20nodes%20on%20the%20IC.html#community-discussion","title":"Community Discussion","text":"<p>Link to forum discussion: Increasing DFINITY Node Count and NNS Topology Exception</p>"},{"location":"qualification/running-qualification.html","title":"How to run qualification","text":"<p>There are three ways to run the test and a couple of environment variables that a user should be aware of.</p> Suggested way of running <p>Since this is a long test and bazel tends to be pretty heavy on the system it is's a good idea to run the test in a devenv. To do that you should follow the IDX guide on how to create a devenv. Once the devenv is deployed you can: <pre><code>eval `ssh-agent -s`\nssh-add ~/.ssh/ssh_key_that_has_access_to_k8s_repo\nssh devenv\ncd /to/root/of/ic/repo\n./ci/container/container-run.sh\n...\n</code></pre></p>"},{"location":"qualification/running-qualification.html#running-using-ict","title":"Running using <code>ict</code>","text":"<p><code>ict</code> is a go-lang tool that was developed to help developers run tests with less friction in having to write long test names. To use <code>ict</code> one can spin up a new shell and: <pre><code>cd /to/root/of/ic/repo\n./ci/container/container-run.sh\nict test guest_os_qualification -- --test_timeout=7200 --keep_going\n</code></pre></p>"},{"location":"qualification/running-qualification.html#running-using-bazel-test","title":"Running using <code>bazel test</code>","text":"<p>Spin up a new shell and: <pre><code>cd /to/root/of/ic/repo\n./ci/container/container-run.sh\nbazel test //rs/tests/dre:guest_os_qualification --config=systest --cache_test_results=no --test_env=IC_DASHBOARDS_DIR=/path/to/k8s_repo/bases/apps/ic-dashboards --sandbox_add_mount_pair=/path/to/k8s_repo/bases/apps/ic-dashboards --test_timeout=7200 --keep_going\n</code></pre></p>"},{"location":"qualification/running-qualification.html#environment-variables","title":"Environment variables","text":"<ul> <li><code>OLD_VERSION</code>: specifies the starting version for a testnet. If it's not specified it will default to the version specified in <code>tests/mainnet_revision.json</code>.</li> </ul>"},{"location":"qualification/running-qualification.html#how-does-it-work","title":"How does it work","text":"<p>Qualification test consits of multiple steps. If we have versions <code>A</code> and <code>B</code> where <code>A</code> is already deployed to the network and <code>B</code> is a version that is being qualified the steps would look like the following:</p> <ol> <li>Ensure that version <code>A</code> is on all subnets</li> <li>Ensure that version <code>A</code> is on all unassigned nodes</li> <li> <p>Upgrading phase:</p> <ol> <li>Deploy version <code>B</code> to application subnets</li> <li>Deploy version <code>B</code> to system subnets</li> <li>Deploy version <code>B</code> to unassigned nodes</li> </ol> </li> <li> <p>Testing phase:</p> <ol> <li>Run performance tests</li> <li>Run xnet tests</li> </ol> </li> <li> <p>Downgrade phase:</p> <ol> <li>Deploy version <code>A</code> to application subnets</li> <li>Deploy version <code>A</code> to system subnets</li> <li>Deploy version <code>A</code> to unassigned nodes</li> </ol> </li> <li> <p>Testing phase:</p> <ol> <li>Run performance tests</li> <li>Run xnet tests</li> </ol> </li> </ol>"},{"location":"release-controller/index.html","title":"Release controller","text":"<p>Automates the process of proposing new releases for IC HostOS and GuestOS.</p>"},{"location":"release-controller/index.html#usage","title":"Usage","text":"<ol> <li>Register new release / version in release-index.yaml <pre><code>releases:\n  - rc_name: rc--2024-02-21_23-01\n    versions:\n      # It is customary but not mandatory to add a link to the\n      # Qualification pipeline:\n      # https://github.com/dfinity/ic/actions/runs/14491317106\n      - name: base\n        version: 2e921c9adfc71f3edc96a9eb5d85fc742e7d8a9f\n</code></pre></li> <li>Relevant teams are notified with a link to a Google document for them to review the release notes.  In parallel, placeholder post is created in the forum to prepare for publication of the release notes.</li> <li>Once the Google document is reviewed (all teams crossed out), PR will be created with release notes.</li> <li>Once that PR is merged, the proposal will be placed and the placeholder forum post is updated with the final release notes.</li> <li>Once trusted neurons have voted to adopt the proposal, the adopted release can be rolled out (beyond the scope of release controller).</li> </ol>"},{"location":"release-controller/index.html#release-index-reference","title":"Release index reference","text":"<p>Releases are composed of a list of dictionaries, each having (1) an <code>rc_name</code> corresponding to the RC branch to be released, and (2) a list of versions each containing a <code>name</code> (at least one of which is typically named <code>base</code> and corresponds to the first listed version) and a <code>version</code> containing the commit ID desired to be tagged and released; two additional version fields <code>changelog_base</code> and <code>security_fix</code> are documented below.</p> <p>Out of each version within a release, a release branch named <code>{rc_name}-{version.name}</code> will be constructed to create a specific release for GuestOS and (in the case of the <code>base</code> or first version of all releases) HostOS.  There is currently no way to force a feature / non-base version of a release to turn into a proposed HostOS release.</p> <p>Only the two most recent releases will be paid attention to by the release controller.</p> <p>The release notes (changelog) for each release version is generated automatically, starting from a prior version which is typically determined automatically.  In the case of any base version of a release, the prior base release is considered the baseline for the release notes; in the case of a non-base / feature version, the base version the same release is considered the baseline.</p> <p>You can override this behavior; a version can have an additional <code>changelog_base</code> dictionary with (optional) keys <code>GuestOS</code> and/or <code>HostOS</code>, whose values must be the name of another release (<code>rc_name</code>) listed in the index, as well as the name of one of its versions (typically <code>base</code>).  This dictionary allows you to override which release/version combo is used as the baseline for (the start of) the release notes that will be generated for this OS and version combination.  Here is an example:</p> <pre><code>releases:\n  - rc_name: rc--2025-05-23_03-21\n    versions:\n      - name: base\n        version: 16825c5cbff83a51983d849b60c9d26b3268bbb6\n        changelog_base:\n          # Base the changelog for GuestOS at this version onto the May 1st base release.\n          # Due to absence of HostOS key, use the normal HostOS baseline detection mechanism\n          # for its changelog.\n          GuestOS:\n            rc_name: rc--2025-05-01_03-23\n            name: base\n  - rc_name: rc--2025-05-15_03-20\n    versions:\n      - name: base\n        version: 59ad18a77fbeaf3ebbba863972ff20f7ab588d7a\n  - rc_name: rc--2025-05-01_03-23\n    versions:\n      - name: base\n        version: f195ba756bc3bf170a2888699e5e74101fdac6ba\n</code></pre> <p>Finally, changelog generation can be suppressed entirely by adding <code>security_fix: true</code> to a version.  This creates an abridged release notes containing no changes at all, and indicating to the users that the code plus the changes will be available at a later date.  Use this flag when a release must be performed from the private security-fixes-only repository, as otherwise the changelog code will not work.</p> <pre><code>releases:\n  - rc_name: rc--2025-05-23_03-21\n    versions:\n      - name: base\n        version: 16825c5cbff83a51983d849b60c9d26b3268bbb6\n        security_fix: true\n</code></pre> <p>You are allowed to periodically clean up old releases from the release index, so long as the commit IDs they list are not in the blessed versions for HostOS or GuestOS, and none of the remaining releases refer to them via the <code>changelog_base</code> field.  As a rule of thumb, it will not cause problems to delete releases six months or older.</p>"},{"location":"release-controller/index.html#recreating-notes","title":"Recreating notes","text":"<p>Sometimes you'd want to recreate notes, either because a bug occured on the first generation, or you just want to have updated version of the notes submitted.</p>"},{"location":"release-controller/index.html#recreate-google-doc","title":"Recreate Google Doc","text":"<p>To recreate Google Doc, remove the document from Google Drive directory or rename it such that it doesn't include any release details.</p>"},{"location":"release-controller/index.html#recreate-github-pr-with-release-notes","title":"Recreate GitHub PR with release notes","text":"<p>To recreate GitHub PR, close the outstanding PR and make sure to delete the branch of the PR.</p>"},{"location":"release-controller/index.html#in-production","title":"In production","text":"<p>Several credentials are necessary.  For the reconciler:</p> <ol> <li>The Google Drive credentials.  These are stored in the DRE Team vault and named FIT on-call schedule sync and release controller Google Drive credential.  Saved to a file, the path to this file must be set in environment variable <code>GDOCS_CREDENTIALS_PATH</code>.</li> <li><code>PROPOSER_NEURON_ID</code> should be set to the neuron ID of the proposer, and the proposer key material should be saved to a file (asn the DRE team for information), whose path should be added to environment variable <code>PROPOSER_KEY_FILE</code>.</li> <li>The <code>GITHUB_TOKEN</code> environment variable must be set to the token that has access to the IC and DRE repositories.  This secret resides in the DRE Team vault under the name Release Controller GitHub API Key.  In the reconciler, this token is used to push tags.</li> </ol> <p>The commit annotator only needs the <code>GITHUB_TOKEN</code> credentials.  This token is used to push notes.</p>"},{"location":"release-controller/index.html#contributing","title":"Contributing","text":"<p>The project is split into two parts - commit annotator and reconciler.</p> <p>If you want fix a bug, or add a feature, please consider writing a test.</p>"},{"location":"release-controller/index.html#commit-annotator","title":"commit annotator","text":"<p>This simple service checks out each commit on <code>master</code> and <code>rc-*</code> branches of IC repo and runs target-determinator to identify whether GuestOS build changed as a result of changes in that commit. This information is then pushed to git notes and later used by reconciler.</p> <p>The annotator has a bonus mode to manually annotate failing commits.  See below for more info.</p>"},{"location":"release-controller/index.html#reconciler","title":"reconciler","text":"<p>Reconciler is responsible for:</p> <ol> <li> <p>generating a draft of the release notes   Done by release_notes.py. See Generate release notes locally in this document for manual release notes generation.</p> </li> <li> <p>making a forum post with the release notes draft   The draft release notes generated in step (1) are published as a post to a Discourse thread (which will be created if necessary).</p> </li> <li> <p>Google Docs publish   Done by google_docs.py. The draft of the release notes is published to Google Docs for internal engineering review.</p> </li> <li> <p>creating a GitHub PR to publish notes   Done by publish_notes.py once the notes are ready for review according to the content of the respective Google Doc. It's not recommended to run this manually. Instead, if you have an issue, try to create a unit test to resolve the issue. You can download the Google Doc you're having problems with to use it in your test. See tests that use <code>release-controller/test_data/b0ade55f7e8999e2842fe3f49df163ba224b71a2.docx</code>.</p> </li> <li> <p>placing the proposal for electing a version   Done by dre_cli.py / reconciler.py. There should be a logs for the command that was run if you want to debug any issues with it.</p> </li> <li> <p>forum post update   Done by forum.py. You can run the program manually to debug issues. Change the <code>main()</code> function to your needs.</p> </li> </ol> <p>It's important to note that forum logic depends on finding alredy created blog posts by querying posts from authenticated user (@DRETeam). For those reasons, it won't be able to find manually created posts by other users.</p>"},{"location":"release-controller/index.html#resolving-issues","title":"Resolving issues","text":""},{"location":"release-controller/index.html#diagnostics","title":"Diagnostics","text":"<p>The release controller has its own dashboard. Use the dashboard to supervise the progress of the components that comprise the release controller.</p>"},{"location":"release-controller/index.html#google-docs-generation-was-wrong-for-particular-commit","title":"Google Docs generation was wrong for particular commit","text":"<p>This could happen if release-index.yaml was wrongly configured or if there's a major bug that needs to be adressed before regenerating the notes again.</p>"},{"location":"release-controller/index.html#resolution","title":"Resolution","text":"<ol> <li>To cause the reconciler to regenerate release notes: move the document outside of the Google Drive folder. Renaming it to something meaningless (e.g. to-delete-12-09-24) should also do the trick.</li> <li>To fix the code in the commit annotator: manually generate the release notes (see below) on your computer and make changes to the queries or the code until the notes look as expected.</li> </ol>"},{"location":"release-controller/index.html#release-notes-are-not-yet-ready-for-a-long-time","title":"Release notes are not yet ready for a long time","text":"<p>This is caused by one or more missing GuestOS / HostOS annotations.  Release controller is stuck generating release notes because it's missing a note for some commit.</p>"},{"location":"release-controller/index.html#diagnostics_1","title":"Diagnostics","text":"<p>Verify that the annotator has completed and is not stuck on any branch (use the dashboard listed above).</p> <p>If <code>target-determinator</code> is crashing as the annotator executes it, here is how you find the failing commit causing the crash:</p> <ol> <li>Click on the Custom query square on the title of the Combined annotater and reconciler logs pane on the dashboard.</li> <li>Search for <code>annotate_object</code> -- the most recent occurrence will have the failing commit ID.</li> </ol>"},{"location":"release-controller/index.html#resolution_1","title":"Resolution","text":"<p>If the problem is that the annotator keeps crashing because a commit is not buildable, you can manually annotate that commit and that will cause the annotator to skip it.  There is an example below on how to manually annotate a failing commit.</p> <p>You may have to annotate all commits not annotated prior to the failing commit as well (although that should not be necessary because the annotator generally annotates from oldest to newest commit, so all older commits should already be annotated).</p> <p>[!TIP] If someone messed up and labeled commits in between, commit annotator might report that it labeled everything when it did not, and reconciler may never be ready with the release notes. Run the below commands on IC repo to find gaps where there are commits without labels.</p> <pre><code>git fetch origin 'refs/notes/*:refs/notes/*' -f --prune\ngit log --format='%H' --no-merges $BASE_COMMIT..$RELEASE_COMMIT | xargs -L1 -I_commit bash -c \"echo -n '_commit '; git notes --ref guestos-changed show _commit | cat\"\n# substitute guestos-changed with hostos-changed to detect gaps in HostOS annotations.\n# substitute guestos-changed with guestos-targets to see targets and target-determinator output for that commit's annotation work..\n</code></pre>"},{"location":"release-controller/index.html#missing-proposal","title":"Missing proposal","text":"<p>Proposal placement most likely failed.</p>"},{"location":"release-controller/index.html#evaluation","title":"Evaluation","text":"<p>release-controller should have a warning message something like this:</p> <pre><code>\"version 99ab7f03700ba6cf832eb18ffd55228f56ae927a: earlier proposal submission attempted but most likely failed\"\n</code></pre> <p>Make sure also that few minutes have passed and that public dashboard still doesn't list the proposal.  Sometimes it takes a minute or two.</p> <p>If the proposal was indeed submitted, you don't have to do anything -- the reconciler will notice and continue normally.</p>"},{"location":"release-controller/index.html#resolution_2","title":"Resolution","text":"<p>[!WARNING] Should resolve by itself in newer versions</p> <ol> <li>Top up the release-controller neuron if needed</li> <li>Execute into the pod   <pre><code>kubectl -n release-controller exec -it deployment/release-controller -- bash\n</code></pre></li> <li>Delete the state   <pre><code>rm /state/&lt;full_commit_hash&gt;\n</code></pre></li> </ol>"},{"location":"release-controller/index.html#development","title":"Development","text":"<p>Please see the parent folder's <code>README.md</code> for virtual environment setup. Follow the whole Contributing section to the letter.</p>"},{"location":"release-controller/index.html#running-the-reconciler-in-dry-run-mode","title":"Running the reconciler in dry-run mode","text":"<pre><code>bazel run //release-controller:release-controller -- --dry-run --verbose\n</code></pre> <p>No credentials of any kind are required by this mode.  By default everything the reconciler does in this mode has no outward effect.</p> <p>All the operations it executes are volatile as well.</p> <p>If you want the release notes this mock mode stores to be persisted in a folder so they are not regenerated on every run:</p> <pre><code>export RECONCILER_DRY_RUN_RELEASE_NOTES_STORAGE=/tmp/dryrun/relnotes\nbazel run //release-controller:release-controller \\\n  --action_env=RECONCILER_DRY_RUN_RELEASE_NOTES_STORAGE \\\n  -- --dry-run --verbose\n</code></pre> <p>If you want the mock forum interactions to be remembered between runs:</p> <pre><code>export RECONCILER_DRY_RUN_FORUM_STORAGE=/tmp/dryrun/forum\nbazel run //release-controller:release-controller \\\n  --action_env=RECONCILER_DRY_RUN_FORUM_STORAGE \\\n  -- --dry-run --verbose\n</code></pre> <p>Typing errors preventing you from running it, because you are editing code and testing your changes?  Add <code>--output_groups=-mypy</code> right after <code>bazel run</code>.</p> <p>The optional argument <code>--skip-preloading-state</code> makes it so that the reconciler will not preload its list of known proposals by version from the governance canister.  It is useful (in conjunction with an empty reconciler state folder) to make the reconciler do all the work of submitting proposals again.  It should only be used alongside <code>--dry-run</code>, to avoid submitting proposals twice.</p>"},{"location":"release-controller/index.html#running-the-reconciler-in-the-container-it-ships","title":"Running the reconciler in the container it ships","text":"<p>You can load the reconciler into your local podman or docker system:</p> <pre><code>bazel run //release-controller:oci_image_load\n</code></pre> <p>This will spit out a SHA256 sum, which is the name of the container image just built and imported into your containerization system.  Run it as follows:</p> <pre><code>SHASUM=...\npodman run --rm -it --entrypoint=/release-controller/release-controller $SHASUM\n</code></pre> <p>Or, in short:</p> <pre><code>mkdir -p -m 0777 /tmp/git\npodman run --rm -it \\\n  -v /tmp/git:/root/.cache \\\n  --entrypoint /release-controller/release-controller \\\n  $(bazel run --verbose_failures //release-controller:oci_image_load | tail -1 | cut -d : -f 3)\n</code></pre>"},{"location":"release-controller/index.html#running-the-annotator-locally-in-dry-run-mode","title":"Running the annotator locally in \"dry-run mode\"","text":"<p>The annotator can be run in a mostly stateless mode, for one single loop, with the following options:</p> <pre><code>bazel run //release-controller:commit-annotator \\\n  -- \\\n  --no-push-annotations \\\n  --loop-every=0 \\\n  --no-fetch-annotations \\ # don't clobber locally created annotations \n  --verbose\n</code></pre> <p>The annotator can also be run as a podman container, with a similar technique as above.  However, the annotator requires <code>--user $UID</code> because Bazel will not run as root (UID 0).</p> <p>Please consult <code>--help</code> for additional options.</p>"},{"location":"release-controller/index.html#manually-annotate-a-troublesome-commit","title":"Manually annotate a troublesome commit","text":"<pre><code>export GITHUB_TOKEN=&lt;any Github token with push access to the IC repo&gt;\nCOMMIT_TO_ANNOTATE=9da8cc52d3d576410174bb28d629862f05a635e0\nAFFECTS_OS=yes\nWHICH_OS=HostOS # or GuestOS, or leave out --os-kind for all OSes\nbazel run //release-controller:commit-annotator \\\n  -- \\\n  manually-annotate \\\n  $COMMIT_TO_ANNOTATE $AFFECTS_OS --os-kind $WHICH_OS\n</code></pre>"},{"location":"release-controller/index.html#generate-release-notes-locally","title":"Generate release notes locally","text":"<p>Release notes can be generated locally using several approaches:</p>"},{"location":"release-controller/index.html#method-1-using-bazel-with-specific-rc-names","title":"Method 1: Using Bazel with specific RC names","text":"<pre><code>PREV_RC=rc--2025-03-27_03-14-base\nPREV_COMMIT=3ae3649a2366aaca83404b692fc58e4c6e604a25\nCURR_RC=rc--2025-04-03_03-15\nCURR_COMMIT=68fc31a141b25f842f078c600168d8211339f422\nbazel run //release-controller:release-notes -- $PREV_RC $PREV_COMMIT $CURR_RC $CURR_COMMIT --verbose --commit-annotator=local\n</code></pre>"},{"location":"release-controller/index.html#method-2-using-bazel-with-generic-names-and-local-commit-annotator","title":"Method 2: Using Bazel with generic names and local commit annotator","text":"<pre><code>bazel run //release-controller:release-notes -- prev bf0d4d1b8cb6c0c19a5afa1454ada014847aa5c6 curr 07c01746ee3fa7700eb0eb781a7c26a53f989b1a --commit-annotator=local\n</code></pre>"},{"location":"release-controller/index.html#method-3-using-rye-python-environment","title":"Method 3: Using Rye (Python environment)","text":"<p>First, ensure your Python environment is set up:</p> <pre><code>rye sync\n</code></pre> <p>Then run the release notes script directly:</p> <pre><code>rye run python3 release-controller/release_notes.py prev bf0d4d1b8cb6c0c19a5afa1454ada014847aa5c6 curr 07c01746ee3fa7700eb0eb781a7c26a53f989b1a --commit-annotator=local\n</code></pre>"},{"location":"release-controller/index.html#commit-annotator-options","title":"Commit Annotator Options","text":"<p>The form of the command above requires you to run a commit annotator in parallel.  If you want to use the internal commit annotator that does not need a commit annotator running in parallel, add option <code>--commit-annotator-url local</code> instead.  If you want to recalculate the commit annotations instead of using cached ones, you can use option <code>--commit-annotator-url recreate</code>.  This last option is useful when testing the effects of changes made to the commit annotator code or Bazel query formulas the annotator uses.</p> <p>A great tip / trick to diagnose exactly what the release notes and commit annotation processes would do is to pick a commit from the IC repo, figure out which its parent commit is, then run:</p> <pre><code>PREV_RC=prev\nPREV_COMMIT=1354f31c9cd4fb6b4a65ab64eb9ac4a0a4d16839 # parent commit\nCURR_RC=curr\nCURR_COMMIT=f8131bfbc2d339716a9cff06e04de49a68e5a80b # commit\nbazel run //release-controller:release-notes -- \\\n   $PREV_RC $PREV_COMMIT $CURR_RC $CURR_COMMIT \\\n   --commit-annotator-url recreate \\\n  --os-kind=GuestOS \\\n  --verbose\nbazel run //release-controller:release-notes -- \\\n   $PREV_RC $PREV_COMMIT $CURR_RC $CURR_COMMIT \\\n   --commit-annotator-url recreate \\\n  --os-kind=HostOS \\\n  --verbose\n</code></pre> <p>That run tells you what the annotation process would do for that single commit in question.</p> <p>Please consult <code>--help</code> for additional options.</p>"},{"location":"release-controller/index.html#tests","title":"Tests","text":""},{"location":"release-controller/index.html#unit-tests","title":"Unit tests","text":"<pre><code>bazel test //release-controller/...\n</code></pre> <p>The above runs all tests and typechecks tested files.</p> <p>With a <code>.venv</code> setup by <code>rye</code>, you can also run (with varying levels of success):</p> <pre><code>export PYTHONPATH=$PWD/release-controller/\n.venv/bin/python3 release-controller/tests/runner.py\n</code></pre> <p>If you want to run a specific test file, specify its path as an argument to the above command line.</p>"},{"location":"release-controller/index.html#typing-correctness","title":"Typing correctness","text":"<p>Building it all tests MyPy types:</p> <pre><code>bazel build //release-controller/...\n</code></pre>"},{"location":"release-controller/index.html#maintenance","title":"Maintenance","text":"<p>The container image currently used by release controller components is an Ubuntu 24.04 image built by Bazel.  Refer to BUILD.bazel and images/BUILD.bazel for instructions on how to maintain and update the images.</p>"},{"location":"rs/cli/index.html","title":"Release CLI","text":"<p>Release CLI is used to enable faster and easier IC network operations for the Release Team.</p> <p>Features include:</p> <ul> <li>HSM auto-detection</li> <li>Neuron auto-detection</li> <li>Node replacement</li> <li>All ic-admin get &amp; propose commands</li> </ul>"},{"location":"rs/cli/index.html#mac-os-users-with-m1-chip","title":"Mac OS users with M1 chip","text":"<p>Before installing you need to install OpenSSL@3 for intel processor:</p> <pre><code>/usr/sbin/softwareupdate --install-rosetta\n\n# Install homebrew for intel apps\ncd ~/Downloads\nmkdir homebrew\ncurl -L https://github.com/Homebrew/brew/tarball/master | tar xz --strip 1 -C homebrew\nsudo mv homebrew /usr/local/homebrew\nalias axbrew='arch -x86_64 /usr/local/homebrew/bin/brew'\n\n# Install openssl@3\naxbrew install openssl@3\nsudo ln -s /usr/local/homebrew/Cellar/openssl@3/3.0.8 /usr/local/opt/openssl@3\n</code></pre>"},{"location":"rs/cli/index.html#install","title":"Install","text":"<pre><code>cargo install --git https://github.com/dfinity/dre.git dre\n</code></pre>"},{"location":"rs/cli/index.html#usage","title":"Usage","text":"<pre><code>dre --help\n</code></pre>"},{"location":"rs/cli/index.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"trusted-neurons-alerts/index.html","title":"Trusted Neurons Alerts","text":""},{"location":"trusted-neurons-alerts/index.html#overview","title":"Overview","text":"<p>The Trusted Neurons Alerts App is designed to receive alert notifications of <code>#SoonToExpireProposals</code> from an Alertmanager instance and push corresponding messages to a specified Slack channel.</p> <p>The user can then press on <code>Vote</code> button to place his/her vote on the proposal or on <code>Silence</code> button to add a silence in Alertmanager for all the alerts on the proposal.</p>"},{"location":"trusted-neurons-alerts/index.html#workflow","title":"Workflow","text":"<ul> <li>Receive Alert: The app receives an HTTP POST request from Alertmanager containing alert details.</li> <li>Parse Alert: The alert data is parsed to extract relevant information.</li> <li>Push to Slack: The extracted information is formatted into a message and pushed to a specified Slack channel.</li> </ul>"},{"location":"trusted-neurons-alerts/index.html#api-spec","title":"API spec","text":""},{"location":"trusted-neurons-alerts/index.html#post-alert","title":"<code>POST</code> /alert","text":"<p>The incoming requests from Alertmanager contain JSON payloads structured as follows:</p>"},{"location":"trusted-neurons-alerts/index.html#alert-payload-example","title":"Alert Payload Example","text":"<pre><code>{\n    \"alerts\": [\n        {\n            \"labels\": {\n                \"alertname\": \"ALERT_NAME\",\n                \"proposal_id\": \"123456\",\n                \"proposal_topic\": \"PROPOSAL_TOPIC\",\n                \"proposal_type\": \"PROPOSAL_TYPE\",\n                ...\n            },\n            ...\n        }\n    ],\n    ...\n}\n</code></pre>"},{"location":"trusted-neurons-alerts/index.html#environment-variables","title":"Environment Variables","text":""},{"location":"trusted-neurons-alerts/index.html#alertmanager_url","title":"ALERTMANAGER_URL","text":"<ul> <li>Description: URL for the Alertmanager instance, used to push silences.</li> <li>Usage: When the user wants to silence an alert, this URL is used to communicate with Alertmanager.</li> <li>Example: https://alertmanager.example.com</li> </ul>"},{"location":"trusted-neurons-alerts/index.html#slack_bot_token","title":"SLACK_BOT_TOKEN","text":"<ul> <li>Description: Bot token for the Slack app, used to authenticate API requests to Slack.</li> <li>Usage: This token is used by the app to send messages to Slack channels. Find it in Slack App config page</li> </ul>"},{"location":"trusted-neurons-alerts/index.html#slack_app_token","title":"SLACK_APP_TOKEN","text":"<ul> <li>Description: App token for the Slack app, used to configure the app and interact with the Slack API.</li> <li>Usage: This token is used by the app to send messages to Slack channels. Find it in Slack App config page</li> </ul>"},{"location":"trusted-neurons-alerts/index.html#slack_app_channel","title":"SLACK_APP_CHANNEL","text":"<ul> <li>Description: The Slack channel where alerts will be posted.</li> <li>Usage: Defines the target channel in the Slack workspace for the alert messages.</li> <li>Example: #alerts</li> </ul>"},{"location":"trusted-neurons-alerts/index.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>Alert Not Received: Alerts received by the App are logged in stdout. Verify that Alertmanager is correctly configured to send alerts to the App's endpoint and that the endpoint is reachable.</p> </li> <li> <p>Message Not Sent to Slack: Check the Slack tokens <code>SLACK_APP_TOKEN</code> and <code>SLACK_APP_CHANNEL</code> and ensure the channel exists. Check logs of the App.</p> </li> <li> <p>Silencing Not Working: Ensure the <code>ALERTMANAGER_URL</code> is correctly set and reachable.</p> </li> </ul>"},{"location":"trustworthy-metrics/architecture.html","title":"Trustworthy Node Metrics: Architectural Overview and Design","text":""},{"location":"trustworthy-metrics/architecture.html#introduction","title":"Introduction","text":"<p>This document offers a deeper look at the architectural design of the Trustworthy Node Metrics feature on the Internet Computer (IC). It is tailored for IC stakeholders and technical professionals, providing a detailed understanding of both the functional and structural aspects.</p> <p>For a higher-level document please take a look here.</p>"},{"location":"trustworthy-metrics/architecture.html#objectives","title":"Objectives","text":"<p>The primary goal is to provide clear visibility into the useful work carried out by nodes on the IC. This transparency is a foundational step towards a future feature that will enable the adjustment of node remuneration based on their operational performance and reliability.</p>"},{"location":"trustworthy-metrics/architecture.html#high-level-architectural-changes","title":"High-Level Architectural Changes","text":"<p>On a high level, planned architectural changes are illustrated in the following figure:</p> <p></p>"},{"location":"trustworthy-metrics/architecture.html#integration-with-existing-architecture","title":"Integration with Existing Architecture","text":"<p>The feature is designed to integrate seamlessly with the existing IC infrastructure. The changes primarily involve the Consensus Layer, Message Routing Layer, and the addition of new components to handle metric aggregation and retrieval.</p>"},{"location":"trustworthy-metrics/architecture.html#expose-already-existing-information-to-end-users","title":"Expose already existing information to end users","text":"<p>Consensus Layer to expose block maker information, which will be collected and aggregated by the Message Routing Layer, and stored in the replicated state with success and failure metrics of nodes. - The metrics for the number of successfully proposed blocks and failures thereof are accumulated in the replicated state for node IDs.\u00a0 - The state of this accumulation is saved as a snapshot including the last batch just before midnight in a queue of snapshots (in chronologically ascending order). - Snapshots in the queue are immutable, i.e. the current state is not included. - There is no guarantee that the snapshots represent whole days or that all days are included since the subnet could have been offline at some point.\u00a0</p>"},{"location":"trustworthy-metrics/architecture.html#data-accessibility","title":"Data Accessibility","text":"<p>The inclusion of new components to ensure that the metrics are easily accessible for analysis and decision-making processes. This involves the management canister playing a crucial role in fetching and providing these metrics to stakeholders.</p> <ul> <li> <p>A metrics-fetching function is added to the management canister</p> </li> <li> <p>There is support for querying since particular date</p> </li> <li> <p>The function will return data from the replicated state.</p> </li> <li> <p>See https://github.com/dfinity/interface-spec/pull/215 for more details.</p> </li> </ul> <p>The DRE team provided open source tooling that fetches the metrics from the management canister(s) of all subnets and allows the community members to inspect the metrics in details.</p> <ul> <li> <p>The metrics retrieved from the IC can be stored in a local file (JSON format), and then further analyzed</p> </li> <li> <p>The metrics will be retrieved from all subnets in parallel, whenever possible, to reduce the amount of time needed to fetch them, taking into account the possible increase of the number of subnets in the future.</p> </li> <li> <p>See trustworthy-metrics</p> </li> </ul>"},{"location":"trustworthy-metrics/architecture.html#detailed-architectural-diagrams-and-data-flow","title":"Detailed Architectural Diagrams and Data Flow","text":"<p>The high-level and in-depth technical diagrams provide a visual representation of the data flow within the IC architecture with to the implementation of the Trustworthy Node Metrics feature.</p> <pre><code>graph TD\n    subgraph \"IC Community\"\n        F[\"Community Members\\n[verify metrics]\"] --&gt;T[DRE Tooling]\n        F --&gt; H[Data Analysis Tools]\n        H --&gt; J[JSON data with metrics]\n        J --&gt; I[Community Insight and Verification of Node Performance]\n    end\n    subgraph Sx[\"IC subnet X\"]\n        T &lt;----&gt; W\n        W[Wallet Canister]\n        W &lt;-..-&gt;|fetch data| D1\n        A1[Consensus Layer] --&gt;|Exposes block maker information| B1[\"Message Routing (MR) Layer\"]\n        B1 --&gt;|Aggregates and writes| C1(((Replicated State)))\n        D1[Management Canister] --&gt;|Trustworthy metrics API| C1\n    end\n    subgraph Sy[\"IC subnet Y\"]\n        W &lt;-.....-&gt;|\"Fetch data with\\ncross-subnet (XNet) calls\"| D2\n        A2[Consensus Layer] --&gt;|Exposes block maker information| B2[\"Message Routing (MR) Layer\"]\n        B2 --&gt;|Aggregates and writes| C2(((Replicated State)))\n        D2[Management Canister] --&gt;|Trustworthy metrics API| C2\n    end\n    subgraph Sz[\"IC subnet Z\"]\n        W &lt;-.....-&gt;|\"Fetch data with\\ncross-subnet (XNet) calls\"| D3\n        A3[Consensus Layer] --&gt;|Exposes block maker information| B3[\"Message Routing (MR) Layer\"]\n        B3 --&gt;|Aggregates and writes| C3(((Replicated State)))\n        D3[Management Canister] --&gt;|Trustworthy metrics API| C3\n    end</code></pre> <p>Link for online editing with preview</p>"},{"location":"trustworthy-metrics/architecture.html#changes-in-the-public-specification","title":"Changes in the Public Specification","text":"<p>Addition of the <code>node_metrics</code> Interface: This involves updating the existing public spec to include a new <code>node_metrics</code> interface that will provide detailed metrics about node performance.</p> <p>This new interface is marked as experimental, which means that end users should not count on it being permanently being present without changes.</p>"},{"location":"trustworthy-metrics/architecture.html#security-and-reliability-considerations","title":"Security and Reliability Considerations","text":"<p>The feature requires the use of a wallet canister, in order to prevent abuse. Each request for fetching metrics will be charged for, which makes it harder for malicious users to conduct DOS attacks using this interface.</p> <p>All data is retrieved through <code>update</code> calls, in order to prevent a potentially malicious node from providing false data.</p>"},{"location":"trustworthy-metrics/architecture.html#conclusion","title":"Conclusion","text":"<p>The Trustworthy Node Metrics feature enables the next milestone in the transparency and operational efficiency of the IC. By providing clear insights into node performance, it lays the groundwork for decentralized data-driven decision making, and for future enhancements in node remuneration processes.</p>"},{"location":"trustworthy-metrics/trustworthy-metrics.html","title":"Get trustworthy metrics from the IC Mainnet","text":""},{"location":"trustworthy-metrics/trustworthy-metrics.html#introduction","title":"Introduction","text":"<p>Trustworthy Node Metrics provide greater visibility into node performance, in a trustworthy manner. Trustworthy here means that the metrics are generated and served by the IC itself, without an intermediary, and without a possibility that any single node can fake their health status.</p> <p>Medium term objective is to use these metrics to adjust node rewards based on the contributions of individual nodes to the core IC protocol. For this purpose, we currently expose the metric on how many block rounds a particular node was, or was not, contributing to the protocol. In each round, the block maker node is selected based on the random beacon. In order for the node to \"make a block\", it needs to be up to date, so must have sufficient network connectivity, and it must be fast enough, so the compute and storage resources must be sufficient for it. This makes the block maker a good metric for the node contributions to the protocol.</p> <p>The information that all nodes in the subnet have about who is the block maker, and who failed to be the block maker, was already stored in the consensus. We know exposed it to the public through the subnet's replicated state and the management canister. Since this information comes from the consensus, it can be considered to be trustworthy.</p> <p>Note that IC is split into subnets, and each subnet has its own consensus, replicated state, and management canister. We developed and open sourced tooling that fetches trustworthy metrics from all subnets, joins it together, and provides it to the IC community for analysis and inspection.</p> <p>This entire process is shown in the following diagram:</p> <pre><code>%%{init: {'theme':'forest'}}%%\ngraph TD\n    subgraph \"Subnet 1\"\n        S1[\"Consensus\"] --&gt;|Produces Trustworthy Data| M1[\"Management Canister 1\"]\n    end\n    subgraph \"Subnet 2\"\n        S2[\"Consensus\"] --&gt;|Produces Trustworthy Data| M2[\"Management Canister 2\"]\n    end\n    subgraph \"Subnet 3\"\n        S3[\"Consensus\"] --&gt;|Produces Trustworthy Data| M3[\"Management Canister 3\"]\n    end\n    M1 --&gt; DRE[\"DRE tool (open source)\"]\n    M2 --&gt; DRE\n    M3 --&gt; DRE\n    DRE --&gt; User\n    User --&gt; |Analyze &amp; Process Data| F[\"Trustworthy Node Metrics\"]\n\n    style S1 fill:#f9f,stroke:#333,stroke-width:2px\n    style S2 fill:#f9f,stroke:#333,stroke-width:2px\n    style S3 fill:#f9f,stroke:#333,stroke-width:2px\n    style DRE fill:#ff9,stroke:#333,stroke-width:2px\n    style F fill:#9ff,stroke:#333,stroke-width:2px</code></pre>"},{"location":"trustworthy-metrics/trustworthy-metrics.html#prerequisites","title":"Prerequisites","text":"<p>To be able to fetch trustworthy metrics, a couple of things are currently needed. While we are looking for ways to simplify the process, for security reasons at the moment one still needs to use wallet canister and fetch metrics with update calls, and these update calls go through the consensus as well and need to be paid for. Hence the requirement for the wallet canister.</p> Click here to learn how to create a wallet canister, if you don't have one already <ol> <li> <p>You need a dfx principal. If needed you can create a new one with</p> <pre><code># You can use the one from your HSM but there are some caveats to that that will be addressed later\ndfx identity new &lt;identity-name&gt;\n</code></pre> <p>or follow instructions from the IC SDK Docs</p> </li> <li> <p>You can list available dfx identities with <code>dfx identity list</code> and then need to select that identity and get its principal.</p> <pre><code>dfx identity use &lt;dfx-identity-name&gt;\ndfx identity get-principal\n</code></pre> </li> <li> <p>Check the current balance for the principal</p> <pre><code>dfx ledger --network ic balance\n</code></pre> <p>If you have less than 2 Trillion Cycles (TC) worth of ICP, based on the current ICP value, you can top up the ICP balance by sending funds to the principal, e.g., from https://ic0.app/wallet/.</p> <p>1 TC corresponds to 1 XDR at the time of conversion. XDR is the currency symbol of the IMF SDR, a basket of five fiat currencies, corresponding to 1.33 U.S. dollar at the time of writing. Canister creation itself will cost 1 TC, and you will need some cycles more to execute commands.</p> </li> <li> <p>Create the wallet canister, after that you will get the wallet canister id in the output.</p> <pre><code>dfx ledger --network ic create-canister --amount 0.5 &lt;principal-from-step-2&gt;\n</code></pre> <p>You may need to adjust the amount of ICPs if needed, based on the current ICP value. More info can be found in the IC SDK Docs.</p> </li> <li> <p>Deploy the wallet canister code</p> <pre><code>dfx identity --network ic deploy-wallet &lt;wallet-canister-id-from-step-4&gt;\n</code></pre> </li> </ol>"},{"location":"trustworthy-metrics/trustworthy-metrics.html#using-the-cli","title":"Using the cli","text":"<p>You can obtain the DRE tool by following the instructions from getting started</p> <p>To test out the command you can run the following command</p> <pre><code>dre &lt;auth-params&gt; node-metrics --trustworthy --wallet &lt;wallet-canister-id&gt; &lt;start-at-timestamp&gt; [&lt;subnet-id&gt;...]\n</code></pre> Explanation of the arguments <ol> <li><code>auth-params</code> - depending on which identity you used to deploy the wallet canister you have two options:</li> <li><code>wallet-canister-id</code> - id of the created wallet canister created in the step 4 above, or obtained by <code>bash     dfx identity --network ic get-wallet</code></li> <li><code>start-at-timestamp</code> - used for filtering the output. To get all metrics, provide 0</li> <li><code>subnet-id</code> - subnets to query, if empty will provide metrics for all subnets</li> </ol>"},{"location":"trustworthy-metrics/trustworthy-metrics.html#authentication","title":"Authentication","text":"<p>Both authentication with a private key, and with an HSM are supported. Authentication with a private key is recommended, since it allows for more parallelism.</p> Click here to see how to export a private key with <code>dfx</code> <ol> <li> <p>export identity as a <code>.pem</code> file:</p> <pre><code>dfx identity export &lt;identity-name&gt; &gt; identity.pem\n</code></pre> </li> <li> <p>replace <code>&lt;key-params&gt;</code> in the command with something like: <code>--private-key-pem identity.pem</code></p> </li> </ol> Click here to see how to authenticate with an HSM <p>Replace <code>&lt;key-params&gt;</code> with: <code>--hsm-slot 0 --hsm-key-id 0 --hsm-pin &lt;pin&gt;</code>. Note that the HSM operations are slower than the key file due to hardware limits, so getting metrics with an HSM will be a bit slower.</p> Click here to see how to add multiple controllers to the wallet canister <p>There are many reasons why this can be useful. For instance, allowing more team members to use the same wallet canister, or adding a private-key based controller in addition to an HSM, etc.</p> <ol> <li> <p>Get the principal of new identity</p> <pre><code>dfx identity use &lt;identity-name&gt; &amp;&amp; dfx identity get-principal\n</code></pre> </li> <li> <p>Add the identity as the controller of canister</p> <pre><code>dfx identity use &lt;identity-name-used-for-creating-canister&gt;\ndfx wallet --network ic add-controller &lt;principal&gt;\n</code></pre> </li> </ol> <p>And that's it! From now on, you can use the second identity while running the tool.</p>"},{"location":"trustworthy-metrics/trustworthy-metrics.html#example-use","title":"Example use","text":"<p>Here are some real-world examples of how metrics can be retrieved:</p> <pre><code>dre --private-key-pem identity.pem node-metrics --trustworthy --wallet nanx4-baaaa-aaaap-qb4sq-cai 0 &gt; data.json\n</code></pre> <p>Or with an HSM: <pre><code>dre --hsm-slot 0 --hsm-key-id 0 --hsm-pin \"&lt;pin&gt;\" node-metrics --trustworthy --wallet nanx4-baaaa-aaaap-qb4sq-cai 0 &gt; data.json\n</code></pre></p> <p>You can check some examples of the analytics possible with the IC Mainnet data in the following Jupyter Notebook</p> <p>If you don't have Jupyter notebooks locally, you can use Google Colaboratory or  to run it online for free.</p>"},{"location":"trustworthy-metrics/untrusted-metrics.html","title":"Get untrusted metrics from Node Metrics canister","text":""},{"location":"trustworthy-metrics/untrusted-metrics.html#introduction","title":"Introduction","text":"<p>Untrusted Node Metrics retrieval offers an alternative approach to accessing node performance data, relying on a canister that collects these metrics instead of quering the management canister of each subnet directly.</p> <p>This method allows users to fetch node metrics dating back to May 18, 2024, providing an historical view compared to the trustworthy method, which only offers data from the past month.</p> <p>The key drawback of quering untrusted metrics is that it introduces an intermediary, the canister responsible for data aggregation, which should NOT be considered trustworthy.</p> <p>Despite these concerns, the extended temporal coverage can be valuable for certain analytical purposes. Additionally, querying the node metrics canister is cheaper because it allows for a query call instead of an update call and does not require a wallet canister.</p> <p>This entire process is shown in the following diagram:</p> <pre><code>%%{init: {'theme':'forest'}}%%\ngraph TD\n    subgraph \"Subnet 1\"\n        S1[\"Consensus\"] --&gt;|Produces Trustworthy Data| M1[\"Management Canister 1\"] --&gt; M4[\"Node Metrics Canister\"]\n    end\n    subgraph \"Subnet 2\"\n        S2[\"Consensus\"] --&gt;|Produces Trustworthy Data| M2[\"Management Canister 2\"]\n    end\n    subgraph \"Subnet 3\"\n        S3[\"Consensus\"] --&gt;|Produces Trustworthy Data| M3[\"Management Canister 3\"]\n    end\n    M2 --&gt; M4\n    M3 --&gt; M4\n    M4 --&gt; DRE[\"DRE tool (open source)\"]\n    DRE --&gt; User\n    User --&gt; |Analyze &amp; Process Data| F[\"Node Metrics\"]\n\n\n    style S1 fill:#f9f,stroke:#333,stroke-width:2px\n    style S2 fill:#f9f,stroke:#333,stroke-width:2px\n    style S3 fill:#f9f,stroke:#333,stroke-width:2px\n    style DRE fill:#ff9,stroke:#333,stroke-width:2px\n    style F fill:#9ff,stroke:#333,stroke-width:2px</code></pre>"},{"location":"trustworthy-metrics/untrusted-metrics.html#using-the-cli","title":"Using the cli","text":"<p>You can obtain the DRE tool by following the instructions from getting started</p> <p>To test out the command you can run the following command</p> <pre><code>dre node-metrics &lt;start-at-timestamp&gt; [&lt;subnet-id&gt;...]\n</code></pre> Explanation of the arguments <ol> <li><code>start-at-timestamp</code> - used for filtering the output. To get all metrics, provide 0</li> <li><code>subnet-id</code> - subnets to query, if empty will provide metrics for all subnets</li> </ol>"},{"location":"trustworthy-metrics/untrusted-metrics.html#example-use","title":"Example use","text":"<p>Here are some real-world examples of how metrics can be retrieved:</p> <pre><code>dre node-metrics 0 &gt; data.json\n</code></pre>"}]}